{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\uddea Transformer Builder - Advanced Testing Lab\n",
        "\n",
        "Welcome! This notebook provides comprehensive testing and training capabilities for your custom transformer architecture.\n",
        "\n",
        "**What's included:**\n",
        "- \u2705 **Tier 1:** Critical validation (shape, gradients, numerical stability)\n",
        "- \ud83d\udd2c **Tier 2:** Advanced analysis (attention patterns, robustness, profiling)\n",
        "- \ud83d\ude80 **Tier 3:** Training utilities (fine-tuning, hyperparameter sweeps, benchmarks)\n",
        "\n",
        "**Quick Start:**\n",
        "1. Click \"Run all\" (Runtime \u2192 Run all)\n",
        "2. Review Tier 1 results (should complete in ~1 minute)\n",
        "3. Explore Tier 2/3 sections as needed\n",
        "\n",
        "**Source:** Generated from [Transformer Builder](https://transformer-builder.com)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Install Dependencies\n",
        "\n",
        "This may take 30-60 seconds on first run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install core dependencies\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q transformers datasets evaluate accelerate\n",
        "!pip install -q scipy matplotlib seaborn pandas tqdm\n",
        "!pip install -q torchinfo  # For model summaries\n",
        "\n",
        "print(\"\u2713 Core dependencies installed\")\n",
        "\n",
        "# Install Tier 2 dependencies (optional)\n",
        "!pip install -q captum  # For attribution analysis\n",
        "print(\"\u2713 Tier 2 dependencies installed\")\n",
        "\n",
        "# Install Tier 3 dependencies (optional)\n",
        "!pip install -q optuna  # For hyperparameter optimization\n",
        "print(\"\u2713 Tier 3 dependencies installed\")\n",
        "\n",
        "print(\"\\n\u2705 All dependencies ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Custom Model from URL\n",
        "\n",
        "This cell extracts your model code from the URL fragment (passed from Transformer Builder)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Load Custom Model from URL (robust)\nimport os\nimport re\nimport json\nimport urllib.request\nimport urllib.error\nfrom google.colab import output\n\n#@title Load Custom Model from Gist (fallback form)\ngist_id_form = \"\"  #@param {type:\"string\"}\nmodel_name_form = \"CustomTransformer\"  #@param {type:\"string\"}\n\ndef _try_eval_js_for_params():\n    js = r\"\"\"\n(() => {\n  try {\n    // Try top frame (can throw cross-origin), then fall back to referrer/baseURI.\n    let raw = null;\n    try { raw = (window.parent && window.parent.location && window.parent.location.href) || null; } catch (e) {}\n    if (!raw || typeof raw !== 'string' || raw === 'about:blank') {\n      raw = document.referrer || document.baseURI || '';\n    }\n    const url = new URL(raw, window.location.origin);\n    const sp = new URLSearchParams(url.search || '');\n    return {\n      ok: true,\n      gist_id: sp.get('gist_id'),\n      name: sp.get('name'),\n      href: url.href\n    };\n  } catch (err) {\n    return { ok: false, error: String(err) };\n  }\n})();\n\"\"\"\n    try:\n        return output.eval_js(js)\n    except Exception as e:\n        return {\"ok\": False, \"error\": f\"JS eval failed: {type(e).__name__}: {e}\"}\n\ndef _validate_gist_id(gid: str):\n    # Allow typical gist IDs (hex or alnum). Relax if needed.\n    return bool(re.fullmatch(r\"[A-Za-z0-9]+\", gid or \"\"))\n\ndef _fetch_gist(gist_id: str) -> dict:\n    url = f\"https://api.github.com/gists/{gist_id}\"\n    req = urllib.request.Request(\n        url,\n        headers={\n            \"Accept\": \"application/vnd.github+json\",\n            \"User-Agent\": \"transformer-builder-colab\"\n        },\n    )\n    try:\n        with urllib.request.urlopen(req, timeout=20) as resp:\n            return json.loads(resp.read().decode(\"utf-8\"))\n    except urllib.error.HTTPError as e:\n        detail = f\"HTTP {e.code}\"\n        try:\n            body = e.read().decode(\"utf-8\")\n            if \"rate limit\" in body.lower():\n                detail += \" (GitHub API rate limit; try later or authenticate)\"\n        except Exception:\n            pass\n        raise RuntimeError(f\"GitHub API error for gist {gist_id}: {detail}\") from e\n    except Exception as e:\n        raise RuntimeError(f\"Network error fetching gist {gist_id}: {e}\") from e\n\ndef _write(path: str, text: str):\n    with open(path, \"w\") as f:\n        f.write(text)\n\n# 1) Primary: JS from page URL (parent/referrer)\njs_result = _try_eval_js_for_params()\n\ngist_id = None\nmodel_name = \"CustomTransformer\"\n\nif isinstance(js_result, dict) and js_result.get(\"ok\"):\n    gist_id = (js_result.get(\"gist_id\") or \"\").strip() or None\n    model_name = (js_result.get(\"name\") or model_name).strip() or model_name\nelif isinstance(js_result, dict) and not js_result.get(\"ok\"):\n    print(f\"\u26a0\ufe0f Could not read URL params via JS: {js_result.get('error')}\")\n\n# 2) Override with environment variable if present\nenv_gid = (os.environ.get(\"GIST_ID\") or \"\").strip()\nif env_gid:\n    gist_id = env_gid\n\n# 3) Fallback: user inputs (Colab form)\nif not gist_id and gist_id_form.strip():\n    gist_id = gist_id_form.strip()\nif model_name_form.strip():\n    model_name = model_name_form.strip() or model_name\n\n# 4) Validate gist_id\nif gist_id and not _validate_gist_id(gist_id):\n    print(f\"\u26a0\ufe0f Invalid gist_id format: {gist_id!r}. Ignoring it.\")\n    gist_id = None\n\nparams = {\"name\": model_name}\n\n# 5) Load from gist or fall back to example\nif gist_id:\n    print(f\"\ud83d\udce5 Loading model from GitHub Gist: {gist_id} (name={model_name})\")\n    try:\n        gist_data = _fetch_gist(gist_id)\n        files = gist_data.get(\"files\") or {}\n        if \"model.py\" not in files or \"config.json\" not in files:\n            raise RuntimeError(\"Gist missing required files: model.py and/or config.json\")\n\n        model_code = files[\"model.py\"].get(\"content\", \"\")\n        config_json = files[\"config.json\"].get(\"content\", \"\")\n        if not model_code or not config_json:\n            raise RuntimeError(\"Empty content in model.py or config.json\")\n\n        _write(\"custom_transformer.py\", model_code)\n        _write(\"config.json\", config_json)\n\n        print(\"\u2705 Model code loaded successfully\")\n        print(f\"\u2705 Gist URL: {gist_data.get('html_url', 'N/A')}\")\n        print(f\"\u2705 Code size: {len(model_code):,} bytes\")\n        print(f\"\u2705 Config size: {len(config_json):,} bytes\")\n    except Exception as e:\n        print(f\"\u274c Failed to load model from Gist: {e}\")\n        print(\"\u26a0\ufe0f Falling back to example model...\")\n        gist_id = None\n\nif not gist_id:\n    print(\"\u26a0\ufe0f No valid gist_id found. Loading example model for demonstration...\")\n    example_code = \"\"\"import torch\nimport torch.nn as nn\n\nclass ExampleTransformer(nn.Module):\n    def __init__(self, vocab_size=50257, d_model=512, nhead=8, num_layers=6):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.output_projection = nn.Linear(d_model, vocab_size)\n    \n    def forward(self, input_ids):\n        x = self.embedding(input_ids)\n        x = self.transformer(x)\n        return self.output_projection(x)\n\"\"\"\n    _write(\"custom_transformer.py\", example_code)\n    _write(\"config.json\", json.dumps({\n        \"vocab_size\": 50257,\n        \"d_model\": 512,\n        \"nhead\": 8,\n        \"num_layers\": 6,\n    }))\n    params[\"name\"] = \"ExampleTransformer\"\n    print(\"\u2705 Example model loaded\")"
    },
    {
      "cell_type": "markdown",
      "source": "## \ud83d\udcc4 View Loaded Model Code\n\nThis cell displays the Python code that was loaded from your Transformer Builder export. You can review the architecture before running tests.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Display the loaded model code for transparency\nprint(\"=\" * 80)\nprint(\"\ud83d\udcc4 LOADED MODEL CODE (custom_transformer.py)\")\nprint(\"=\" * 80)\nprint()\n\nwith open('custom_transformer.py', 'r') as f:\n    model_code_display = f.read()\n\n# Use syntax highlighting\nfrom IPython.display import Code\ndisplay(Code(model_code_display, language='python'))\n\nprint()\nprint(\"=\" * 80)\nprint(\"\ud83d\udccb MODEL CONFIGURATION (config.json)\")\nprint(\"=\" * 80)\nprint()\n\nwith open('config.json', 'r') as f:\n    config_display = json.load(f)\n\n# Pretty print JSON\nprint(json.dumps(config_display, indent=2))\nprint()\nprint(\"\u2705 You can now proceed to run the model instantiation and tests below!\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dynamic Dependency Detection\n",
        "\n",
        "Automatically detect and install any custom dependencies your model needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Parse imports from generated code\n",
        "with open('custom_transformer.py', 'r') as f:\n",
        "    source_code = f.read()\n",
        "    tree = ast.parse(source_code)\n",
        "\n",
        "# Extract all imports\n",
        "imports = set()\n",
        "for node in ast.walk(tree):\n",
        "    if isinstance(node, ast.Import):\n",
        "        for alias in node.names:\n",
        "            imports.add(alias.name.split('.')[0])\n",
        "    elif isinstance(node, ast.ImportFrom):\n",
        "        if node.module:\n",
        "            imports.add(node.module.split('.')[0])\n",
        "\n",
        "print(f\"Detected imports: {', '.join(sorted(imports))}\")\n",
        "\n",
        "# Standard library modules (don't need pip install)\n",
        "stdlib_modules = {\n",
        "    'abc', 'collections', 'dataclasses', 'functools', 'json', 'math',\n",
        "    'typing', 'warnings', 'os', 'sys', 're', 'time', 'copy'\n",
        "}\n",
        "\n",
        "# Already installed\n",
        "installed_modules = {\n",
        "    'torch', 'transformers', 'numpy', 'scipy', 'matplotlib',\n",
        "    'pandas', 'seaborn', 'tqdm', 'torchinfo', 'captum', 'optuna'\n",
        "}\n",
        "\n",
        "# Find missing packages\n",
        "missing = imports - stdlib_modules - installed_modules\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\nInstalling additional dependencies: {', '.join(missing)}\")\n",
        "    for package in missing:\n",
        "        try:\n",
        "            subprocess.check_call(\n",
        "                [sys.executable, '-m', 'pip', 'install', '-q', package],\n",
        "                stdout=subprocess.DEVNULL,\n",
        "                stderr=subprocess.DEVNULL\n",
        "            )\n",
        "            print(f\"  \u2705 Installed {package}\")\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(f\"  \u26a0\ufe0f Failed to install {package} (may not be a pip package)\")\n",
        "else:\n",
        "    print(\"\\n\u2705 All dependencies already installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import and Instantiate Model\n",
        "\n",
        "Load your custom transformer and prepare for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "\n",
        "# Import the custom model\n",
        "exec(open('custom_transformer.py').read())\n",
        "\n",
        "# Load config\n",
        "with open('config.json') as f:\n",
        "    config_dict = json.load(f)\n",
        "\n",
        "# Find the model class\n",
        "model_class = None\n",
        "for name, obj in list(globals().items()):\n",
        "    if isinstance(obj, type) and issubclass(obj, nn.Module) and obj is not nn.Module:\n",
        "        if name == params['name']:\n",
        "            model_class = obj\n",
        "            break\n",
        "\n",
        "if model_class is None:\n",
        "    # Fallback: find any nn.Module subclass\n",
        "    for name, obj in list(globals().items()):\n",
        "        if isinstance(obj, type) and issubclass(obj, nn.Module) and obj is not nn.Module:\n",
        "            model_class = obj\n",
        "            print(f\"\u26a0\ufe0f Using {name} (expected {params['name']})\")\n",
        "            break\n",
        "\n",
        "if model_class:\n",
        "    # Instantiate model\n",
        "    try:\n",
        "        model = model_class(**config_dict)\n",
        "        model.eval()\n",
        "        \n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        \n",
        "        print(f\"\u2705 Model instantiated: {model_class.__name__}\")\n",
        "        print(f\"\u2705 Total parameters: {total_params:,}\")\n",
        "        print(f\"\u2705 Trainable parameters: {trainable_params:,}\")\n",
        "        \n",
        "        # Move to GPU if available\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model = model.to(device)\n",
        "        print(f\"\u2705 Device: {device}\")\n",
        "        \n",
        "        # Display model summary\n",
        "        print(\"\\n--- Model Summary ---\")\n",
        "        try:\n",
        "            # Create dummy input based on config\n",
        "            vocab_size = config_dict.get('vocab_size', 50257)\n",
        "            dummy_input = torch.randint(0, vocab_size, (1, 32)).to(device)\n",
        "            summary(model, input_data=dummy_input, depth=3)\n",
        "        except Exception as e:\n",
        "            print(f\"\u26a0\ufe0f Could not generate summary: {e}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Failed to instantiate model: {e}\")\n",
        "        raise\n",
        "else:\n",
        "    raise RuntimeError(f\"Could not find model class '{params['name']}' in generated code\")\n",
        "\n",
        "# Create config object for test functions\n",
        "class ModelConfig:\n",
        "    def __init__(self, **kwargs):\n",
        "        for key, value in kwargs.items():\n",
        "            setattr(self, key, value)\n",
        "\n",
        "config = ModelConfig(**config_dict)\n",
        "print(\"\\n\u2705 Ready for testing!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n# \ud83d\udd0d Tier 1: Critical Validation\n\nThese tests verify your model is mathematically sound and ready for training.\n\n**Estimated time:** ~1 minute\n\n**What's tested:**\n- \u2705 Shape validation across edge cases\n- \u2705 Gradient flow (detect vanishing/exploding gradients)\n- \u2705 Numerical stability (NaN/Inf detection)\n- \u2705 Parameter initialization quality\n- \u2705 Memory footprint scaling\n- \u2705 Inference speed benchmarks",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Import test utilities\n!wget -q https://raw.githubusercontent.com/matt-hans/transformer-builder-colab-templates/main/utils/test_functions.py\nfrom test_functions import (\n    test_shape_robustness,\n    test_gradient_flow,\n    test_output_stability,\n    test_parameter_initialization,\n    test_memory_footprint,\n    test_inference_speed\n)\n\nprint(\"\u2705 Test functions loaded\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print(\"=\" * 80)\nprint(\"TIER 1: CRITICAL VALIDATION\")\nprint(\"=\" * 80)\nprint()\n\n# Test 1: Shape Robustness\nprint(\"Test 1/6: Shape Validation\")\nprint(\"-\" * 80)\nshape_results = test_shape_robustness(model, config)\ndisplay(shape_results)\nprint()\n\n# Test 2: Gradient Flow\nprint(\"Test 2/6: Gradient Flow Analysis\")\nprint(\"-\" * 80)\ngrad_results = test_gradient_flow(model, config)\ndisplay(grad_results)\nprint()\n\n# Test 3: Output Stability\nprint(\"Test 3/6: Numerical Stability\")\nprint(\"-\" * 80)\nstability_stats = test_output_stability(model, config, n_samples=100)\nprint()\n\n# Test 4: Parameter Initialization\nprint(\"Test 4/6: Parameter Initialization\")\nprint(\"-\" * 80)\nparam_results = test_parameter_initialization(model)\ndisplay(param_results)\nprint()\n\n# Test 5: Memory Footprint\nprint(\"Test 5/6: Memory Footprint Analysis\")\nprint(\"-\" * 80)\nmemory_results = test_memory_footprint(model, config)\ndisplay(memory_results)\nprint()\n\n# Test 6: Inference Speed\nprint(\"Test 6/6: Inference Speed Benchmark\")\nprint(\"-\" * 80)\nspeed_stats = test_inference_speed(model, config, n_trials=50)\nprint()\n\nprint(\"=\" * 80)\nprint(\"\u2705 TIER 1 VALIDATION COMPLETE\")\nprint(\"=\" * 80)\nprint()\nprint(\"All critical tests passed! Your model is ready for advanced analysis.\")\nprint()\nprint(\"Next steps:\")\nprint(\"\u2022 Scroll down for Tier 2 (Advanced Analysis)\")\nprint(\"\u2022 Or jump to Tier 3 (Training & Fine-Tuning)\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}