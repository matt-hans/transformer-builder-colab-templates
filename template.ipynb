{
 "cells": [
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {"id": "error-handling-setup"},
  "outputs": [],
  "source": [
    "# ==============================================================================\n",
    "# ERROR HANDLING SETUP â€” Full Tracebacks and Formatting\n",
    "# ==============================================================================\n",
    "import sys, traceback\n",
    "sys.tracebacklimit = 50  # show up to 50 frames\n",
    "\n",
    "def format_exception(e: Exception, context_lines: int = 5) -> str:\n",
    "    \"\"\"Format exception with full traceback.\n",
    "\n",
    "    Args:\n",
    "        e: Exception instance\n",
    "        context_lines: Unused placeholder for future code context.\n",
    "    Returns: String with full traceback.\n",
    "    \"\"\"\n",
    "    tb_lines = traceback.format_exception(type(e), e, e.__traceback__)\n",
    "    return ''.join(tb_lines)\n",
    "\n",
    "# Install IPython custom exception handler to avoid truncation\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    ip = get_ipython()\n",
    "    if ip is not None:\n",
    "        def _custom_exc(shell, etype, evalue, tb, tb_offset=None):\n",
    "            print('âŒ Exception occurred')\n",
    "            print('=' * 60)\n",
    "            print(''.join(traceback.format_exception(etype, evalue, tb)))\n",
    "            print('=' * 60)\n",
    "            return True  # suppress default truncated handler\n",
    "        ip.set_custom_exc((Exception,), _custom_exc)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ==============================================================================\n",
    "# NETWORK RETRY MONKEY-PATCH â€” urllib.urlopen with retries (GitHub/HF)\n",
    "# ==============================================================================\n",
    "try:\n",
    "    import urllib.request as _ur, urllib.error as _ue, time as _t, random as _r\n",
    "    _orig_urlopen = _ur.urlopen\n",
    "    def _retrying_urlopen(req, timeout=20, max_retries=5, backoff=1.0):\n",
    "        attempt = 0\n",
    "        while True:\n",
    "            try:\n",
    "                return _orig_urlopen(req, timeout=timeout)\n",
    "            except _ue.HTTPError as e:\n",
    "                code = getattr(e, 'code', None)\n",
    "                if code == 404:\n",
    "                    raise\n",
    "                if code in (429, 500, 502, 503, 504, 403):\n",
    "                    attempt += 1\n",
    "                    if attempt > max_retries:\n",
    "                        raise\n",
    "                    ra = getattr(e, 'headers', {{}}).get('Retry-After') if hasattr(e, 'headers') else None\n",
    "                    try:\n",
    "                        ra_val = float(ra) if ra is not None else None\n",
    "                    except Exception:\n",
    "                        ra_val = None\n",
    "                    sleep_for = ra_val if ra_val is not None else backoff * (2 ** (attempt - 1))\n",
    "                    sleep_for += _r.random() * 0.25 * sleep_for\n",
    "                    print(f\"â³ Network retry {attempt}/{max_retries} in {sleep_for:.1f}s (HTTP {code})\")\n",
    "                    _t.sleep(min(sleep_for, 30.0))\n",
    "                    continue\n",
    "                raise\n",
    "            except Exception:\n",
    "                attempt += 1\n",
    "                if attempt > max_retries:\n",
    "                    raise\n",
    "                sleep_for = backoff * (2 ** (attempt - 1))\n",
    "                sleep_for += _r.random() * 0.25 * sleep_for\n",
    "                print(f\"â³ Network retry {attempt}/{max_retries} in {sleep_for:.1f}s\")\n",
    "                _t.sleep(min(sleep_for, 30.0))\n",
    "    def urlopen_with_retry(req, timeout=20):\n",
    "        return _retrying_urlopen(req, timeout=timeout)\n",
    "    _ur.urlopen = urlopen_with_retry\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "troubleshooting"},
   "source": "---\n\n### ðŸ› ï¸ Troubleshooting\n\n- This notebook shows full Python tracebacks (up to 50 frames).\n- When an error occurs, you'll see the complete stack to the root cause.\n- If a model load fails, check ImportError messages and missing packages.\n\nTip: You can also call \`print(format_exception(e))\` inside your own try/except blocks to display a full traceback."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# \ud83e\uddea Transformer Builder - Advanced Testing Lab\n\n**Welcome! This notebook tests your custom transformer architecture.**\n\n---\n\n## \ud83d\ude80 **Quick Start (3 Steps)**\n\n### **STEP 1:** Paste Your Gist ID\n\u2193 Scroll down to Cell 3 and paste the Gist ID you received from Transformer Builder\n\n### **STEP 2:** Run All Cells  \nClick **Runtime \u2192 Run all** (or run cells one-by-one)\n\n### **STEP 3:** Review Test Results\nYour model will be validated through 3 testing tiers\n\n---\n\n## \ud83d\udccb **What's Included:**\n\n- \u2705 **Tier 1:** Critical validation (shape, gradients, numerical stability)\n- \ud83d\udd2c **Tier 2:** Advanced analysis (attention patterns, robustness, profiling)\n- \ud83d\ude80 **Tier 3:** Training utilities (fine-tuning, hyperparameter sweeps, benchmarks)\n\n---\n\n## \u26a0\ufe0f **First Time Setup:**\n\nIf this is your first time OR you're continuing from a previous session:\n\n1. **Runtime** \u2192 **Restart runtime** (takes 5 seconds)\n2. **Edit** \u2192 **Clear all outputs** (optional, cleans up UI)\n3. **Scroll down to Cell 3** \u2192 Paste your Gist ID\n4. **Runtime** \u2192 **Run all**\n\nThis ensures a clean environment and prevents dependency conflicts.\n\n---\n\n**Source:** Generated from [Transformer Builder](https://transformer-builder.com)"
  },
  {
   "cell_type": "markdown",
   "source": "# \ud83e\uddea Transformer Builder - Advanced Testing Lab\n\nWelcome! This notebook provides comprehensive testing and training capabilities for your custom transformer architecture.\n\n**What's included:**\n- \u2705 **Tier 1:** Critical validation (shape, gradients, numerical stability)\n- \ud83d\udd2c **Tier 2:** Advanced analysis (attention patterns, robustness, profiling)\n- \ud83d\ude80 **Tier 3:** Training utilities (fine-tuning, hyperparameter sweeps, benchmarks)\n\n**Quick Start:**\n1. Click \"Run all\" (Runtime \u2192 Run all)\n2. Review Tier 1 results (should complete in ~1 minute)\n3. Explore Tier 2/3 sections as needed\n\n**Source:** Generated from [Transformer Builder](https://transformer-builder.com)\n\n---",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## \ud83d\udccb **STEP 1: Paste Your Gist ID**\n\nWhen you exported from **Transformer Builder**, you received a **Gist ID**.\n\n**Paste it in the cell below and run it.**\n\nIf you don't have a Gist ID yet, go back to Transformer Builder and click **\"Export to Colab\"**."
  },
  {
   "cell_type": "code",
   "source": "# ==============================================================================\n# GIST ID INPUT - Paste the ID from Transformer Builder\n# ==============================================================================\n\n#@title \ud83d\udce5 **Paste Your Gist ID Here**\nGIST_ID = \"\"  #@param {type:\"string\"}\n\n#@markdown ---\n#@markdown **Where to find your Gist ID:**\n#@markdown 1. Go to Transformer Builder\n#@markdown 2. Click \"Export to Colab\"\n#@markdown 3. Copy the Gist ID from the modal\n#@markdown 4. Paste it above and run this cell\n\nif not GIST_ID or not GIST_ID.strip():\n    print(\"=\" * 70)\n    print(\"\u26a0\ufe0f  NO GIST ID PROVIDED\")\n    print(\"=\" * 70)\n    print()\n    print(\"Please paste your Gist ID in the field above and re-run this cell.\")\n    print()\n    print(\"If you don't have a Gist ID:\")\n    print(\"  1. Go to Transformer Builder\")\n    print(\"  2. Click 'Export to Colab'\")\n    print(\"  3. Copy the Gist ID from the modal\")\n    print(\"  4. Come back here and paste it\")\n    print()\n    raise ValueError(\"Gist ID is required to load your custom model\")\nelse:\n    # Validate format\n    import re\n    if not re.fullmatch(r\"[A-Za-z0-9]+\", GIST_ID.strip()):\n        print(\"=\" * 70)\n        print(\"\u26a0\ufe0f  INVALID GIST ID FORMAT\")\n        print(\"=\" * 70)\n        print()\n        print(f\"The Gist ID you entered: {GIST_ID!r}\")\n        print()\n        print(\"Gist IDs should be alphanumeric (e.g., 'abc123def456')\")\n        print(\"Please check and re-enter.\")\n        print()\n        raise ValueError(\"Invalid Gist ID format\")\n    \n    # Store for later use\n    GIST_ID = GIST_ID.strip()\n    \n    print(\"=\" * 70)\n    print(\"\u2705 GIST ID SAVED\")\n    print(\"=\" * 70)\n    print()\n    print(f\"Gist ID: {GIST_ID}\")\n    print()\n    print(\"You can now proceed to run the cells below to:\")\n    print(\"  1. Install dependencies\")\n    print(\"  2. Load your custom model\")\n    print(\"  3. Run tests\")\n    print()\n    print(\"\ud83d\udca1 Tip: Click 'Runtime \u2192 Run all' to execute everything automatically\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================",
    "# DEPENDENCY VERIFICATION - v3.4.0 (Zero Installation Strategy)",
    "# ==============================================================================",
    "",
    "print(\"=\" * 70)",
    "print(\"\ud83d\udce6 DEPENDENCY VERIFICATION\")",
    "print(\"=\" * 70)",
    "print()",
    "print(\"Strategy: Use Colab pre-installed packages (no pip install)\")",
    "print(\"This prevents NumPy corruption caused by package reinstallation.\")",
    "print()",
    "",
    "# ==============================================================================",
    "# VERIFY CORE DEPENDENCIES (All pre-installed in Google Colab 2025)",
    "# ==============================================================================",
    "",
    "required = {",
    "    'torch': '2.6+',",
    "    'numpy': '2.3+',",
    "    'pandas': '1.5+',",
    "    'matplotlib': '3.7+',",
    "    'seaborn': '0.12+',",
    "}",
    "",
    "print(\"Checking pre-installed packages...\")",
    "print()",
    "",
    "all_good = True",
    "for package, min_version in required.items():",
    "    try:",
    "        module = __import__(package)",
    "        version = getattr(module, '__version__', 'unknown')",
    "        print(f\"  \u2705 {package:15s} {version:10s} (required: {min_version})\")",
    "    except ImportError:",
    "        print(f\"  \u274c {package:15s} NOT FOUND (should be pre-installed!)\")",
    "        all_good = False",
    "",
    "print()",
    "",
    "# ==============================================================================",
    "# NUMPY INTEGRITY CHECK",
    "# ==============================================================================",
    "",
    "print(\"Checking NumPy integrity...\")",
    "try:",
    "    from numpy._core.umath import _center",
    "    print(\"  \u2705 NumPy C extensions intact\")",
    "except ImportError as e:",
    "    print(\"  \u274c NumPy corrupted!\")",
    "    print()",
    "    print(\"=\" * 70)",
    "    print(\"ERROR: NumPy corruption detected\")",
    "    print(\"=\" * 70)",
    "    print()",
    "    print(\"This usually happens if you:\")",
    "    print(\"  1. Ran this notebook before without restarting runtime\")",
    "    print(\"  2. Manually installed packages that corrupted NumPy\")",
    "    print()",
    "    print(\"FIX: Runtime \u2192 Restart runtime, then run all cells again\")",
    "    print()",
    "    raise ImportError(\"NumPy corrupted - please restart runtime\") from e",
    "",
    "print()",
    "",
    "if not all_good:",
    "    print(\"=\" * 70)",
    "    print(\"ERROR: Missing required packages\")",
    "    print(\"=\" * 70)",
    "    print()",
    "    print(\"This shouldn't happen in Google Colab.\")",
    "    print(\"Are you running this notebook in a different environment?\")",
    "    print()",
    "    raise RuntimeError(\"Required packages not found\")",
    "",
    "print(\"=\" * 70)",
    "print(\"\u2705 ALL DEPENDENCIES VERIFIED\")",
    "print(\"=\" * 70)",
    "print()",
    "print(\"\u2705 No installation needed - using Colab pre-installed packages\")",
    "print(\"\u2705 NumPy corruption risk: ELIMINATED\")",
    "print()",
    "print(\"Note: Advanced features (Tier 2/3) will install packages on-demand\")",
    "print()",
    ""
   ]
  },
  {
   "cell_type": "code",
   "source": "# ==============================================================================\n# DOWNLOAD UTILS PACKAGE\n# ==============================================================================\n\nprint(\"\ud83d\udce6 Downloading test utilities package...\")\n\n# Remove old utils directory if exists\n!rm -rf utils/\n\n# Download complete utils package from GitHub\n!git clone --depth 1 --branch main https://github.com/matt-hans/transformer-builder-colab-templates.git temp_repo 2>/dev/null\n\n# Copy utils directory\n!cp -r temp_repo/utils ./\n\n# Cleanup\n!rm -rf temp_repo\n\n# Verify package structure\nimport sys\nimport os\n\n# Add current directory to Python path\nif './' not in sys.path:\n    sys.path.insert(0, './')\n\n# Verify utils package is importable\ntry:\n    import utils\n    print(f\"\u2705 Utils package loaded (version {utils.__version__})\")\n    \n    # Verify package structure\n    utils_path = os.path.join(os.getcwd(), 'utils')\n    subdirs = ['adapters', 'tokenization', 'training', 'ui']\n    \n    for subdir in subdirs:\n        subdir_path = os.path.join(utils_path, subdir)\n        if os.path.exists(subdir_path):\n            print(f\"\u2705 {subdir}/ directory found\")\n        else:\n            print(f\"\u26a0\ufe0f  {subdir}/ directory missing\")\n    \n    # Test importing test functions (backward compatibility)\n    from utils import (\n        test_shape_robustness,\n        test_gradient_flow,\n        test_output_stability,\n        run_all_tier1_tests\n    )\n    print(\"\u2705 Test functions importable\")\n    \n    print(\"\\n\u2705 Utils package ready!\")\n    \nexcept ImportError as e:\n    print(f\"\u274c Failed to import utils package: {e}\")\n    print(\"Falling back to direct file download...\")\n    # Fallback: download test_functions.py directly\n    !wget -q https://raw.githubusercontent.com/matt-hans/transformer-builder-colab-templates/main/utils/test_functions.py",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ==============================================================================\n# LOAD CUSTOM MODEL - v3.4.0 (Simple Modal Approach)\n# ==============================================================================\n\nimport os, re, json, urllib.request, urllib.error\n\nprint(\"=\" * 70)\nprint(\"MODEL LOADING\")\nprint(\"=\" * 70)\nprint()\n\n# ==============================================================================\n# VERIFY GIST ID WAS PROVIDED\n# ==============================================================================\n\nif 'GIST_ID' not in globals() or not GIST_ID:\n    print(\"\u274c ERROR: No Gist ID found!\")\n    print()\n    print(\"=\" * 70)\n    print(\"\ud83d\udd19 GO BACK TO CELL 3\")\n    print(\"=\" * 70)\n    print()\n    print(\"You must run Cell 3 first to provide your Gist ID.\")\n    print()\n    print(\"Steps:\")\n    print(\"  1. Scroll up to Cell 3\")\n    print(\"  2. Paste your Gist ID from Transformer Builder\")\n    print(\"  3. Run Cell 3\")\n    print(\"  4. Come back and run this cell\")\n    print()\n    raise ValueError(\"Gist ID required - please run Cell 3 first\")\n\ngist_id = GIST_ID\nmodel_name = \"Model\"  # Default name, will be overridden from config\n\nprint(f\"\ud83d\udce5 Loading model from GitHub Gist: {gist_id}\")\nprint()\n\n# ==============================================================================\n# FETCH GIST AND LOAD MODEL FILES\n# ==============================================================================\n\ndef _fetch_gist(gid: str) -> dict:\n    \"\"\"Fetch Gist data from GitHub API.\"\"\"\n    url = f\"https://api.github.com/gists/{gid}\"\n    req = urllib.request.Request(url, headers={\n        \"Accept\": \"application/vnd.github+json\",\n        \"User-Agent\": \"transformer-builder-colab\"\n    })\n    try:\n        with urllib.request.urlopen(req, timeout=20) as resp:\n            return json.loads(resp.read().decode(\"utf-8\"))\n    except urllib.error.HTTPError as e:\n        detail = f\"HTTP {e.code}\"\n        try:\n            body = e.read().decode(\"utf-8\")\n            if \"rate limit\" in body.lower():\n                detail += \" - GitHub API rate limit (try again in an hour)\"\n            elif e.code == 404:\n                detail += \" - Gist not found (check your Gist ID)\"\n        except:\n            pass\n        raise RuntimeError(f\"GitHub API error: {detail}\") from e\n    except Exception as e:\n        raise RuntimeError(f\"Network error: {e}\") from e\n\ndef _write(path: str, text: str):\n    \"\"\"Write text to file.\"\"\"\n    with open(path, \"w\") as f:\n        f.write(text)\n\n# Fetch Gist\ntry:\n    gist_data = _fetch_gist(gist_id)\n    files = gist_data.get(\"files\") or {}\n    \n    # Check for required files\n    if \"model.py\" not in files:\n        raise RuntimeError(\"Gist is missing 'model.py' - please re-export from Transformer Builder\")\n    if \"config.json\" not in files:\n        raise RuntimeError(\"Gist is missing 'config.json' - please re-export from Transformer Builder\")\n    \n    model_code = files[\"model.py\"].get(\"content\", \"\")\n    config_json = files[\"config.json\"].get(\"content\", \"\")\n    \n    if not model_code or not config_json:\n        raise RuntimeError(\"Empty content in model.py or config.json\")\n    \n    # Write to files\n    _write(\"custom_transformer.py\", model_code)\n    _write(\"config.json\", config_json)\n    \n    print(f\"\u2705 Model loaded successfully!\")\n    print(f\"\u2705 Gist URL: {gist_data.get('html_url', 'N/A')}\")\n    print(f\"\u2705 Model code: {len(model_code):,} bytes\")\n    print(f\"\u2705 Config: {len(config_json):,} bytes\")\n    print()\n    \n    # Parse model name from config if available\n    try:\n        config_dict = json.loads(config_json)\n        if 'model_name' in config_dict:\n            model_name = config_dict['model_name']\n            print(f\"\u2705 Model name: {model_name}\")\n            print()\n    except:\n        pass\n\nexcept Exception as e:\n    print(f\"\u274c Failed to load model from Gist!\")\n    print()\n    print(f\"Error: {e}\")\n    print()\n    print(\"=\" * 70)\n    print(\"TROUBLESHOOTING\")\n    print(\"=\" * 70)\n    print()\n    print(\"Common issues:\")\n    print(\"  1. Check your Gist ID is correct (go back to Cell 3)\")\n    print(\"  2. Ensure you exported from Transformer Builder successfully\")\n    print(\"  3. Check you're not hitting GitHub rate limit (60 requests/hour)\")\n    print(\"  4. Try re-exporting from Transformer Builder\")\n    print()\n    print(\"If the problem persists:\")\n    print(f\"  \u2022 Gist URL: https://gist.github.com/{gist_id}\")\n    print(\"  \u2022 Verify the Gist contains model.py and config.json\")\n    print()\n    raise\n\nprint(\"=\" * 70)\nprint(\"\u2705 MODEL LOADING COMPLETE\")\nprint(\"=\" * 70)\nprint()\nprint(\"Next: Continue to model instantiation and testing below!\")\nprint()\n\n# Store model_name for next cell\nparams = {\"name\": model_name}"
  },
  {
   "cell_type": "markdown",
   "source": "## \ud83d\udcc4 View Loaded Model Code\n\nThis cell displays the Python code that was loaded from your Transformer Builder export. You can review the architecture before running tests.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display the loaded model code for transparency\nprint(\"=\" * 80)\nprint(\"\ud83d\udcc4 LOADED MODEL CODE (custom_transformer.py)\")\nprint(\"=\" * 80)\nprint()\n\nwith open('custom_transformer.py', 'r') as f:\n    model_code_display = f.read()\n\n# Use syntax highlighting\nfrom IPython.display import Code\ndisplay(Code(model_code_display, language='python'))\n\nprint()\nprint(\"=\" * 80)\nprint(\"\ud83d\udccb MODEL CONFIGURATION (config.json)\")\nprint(\"=\" * 80)\nprint()\n\nwith open('config.json', 'r') as f:\n    config_display = json.load(f)\n\n# Pretty print JSON\nprint(json.dumps(config_display, indent=2))\nprint()\nprint(\"\u2705 You can now proceed to run the model instantiation and tests below!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Instantiate Model\n",
    "\n",
    "Load your custom transformer and prepare for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import inspect\n",
    "\n",
    "# Import the custom model\n",
    "exec(open('custom_transformer.py').read())\n",
    "\n",
    "# Load config\n",
    "with open('config.json') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "# Find the model class\n",
    "model_class = None\n",
    "for name, obj in list(globals().items()):\n",
    "    if isinstance(obj, type) and issubclass(obj, nn.Module) and obj is not nn.Module:\n",
    "        if name == params['name']:\n",
    "            model_class = obj\n",
    "            break\n",
    "\n",
    "if model_class is None:\n",
    "    # Fallback: find any nn.Module subclass\n",
    "    for name, obj in list(globals().items()):\n",
    "        if isinstance(obj, type) and issubclass(obj, nn.Module) and obj is not nn.Module:\n",
    "            model_class = obj\n",
    "            print(f\"\u26a0\ufe0f Using {name} (expected {params['name']})\")\n",
    "            break\n",
    "\n",
    "if model_class:\n",
    "    # Instantiate model - try both parameterless and parameterized approaches\n",
    "    try:\n",
    "        # Check if __init__ accepts parameters (besides self)\n",
    "        sig = inspect.signature(model_class.__init__)\n",
    "        params_list = [p for p in sig.parameters.values() if p.name != 'self']\n",
    "        \n",
    "        if len(params_list) == 0:\n",
    "            # Parameterless constructor (Transformer Builder models)\n",
    "            print(\"\u2139\ufe0f  Model has parameterless constructor (Transformer Builder export)\")\n",
    "            model = model_class()\n",
    "        else:\n",
    "            # Parameterized constructor (traditional models)\n",
    "            print(f\"\u2139\ufe0f  Model accepts {len(params_list)} parameter(s)\")\n",
    "            model = model_class(**config_dict)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"\u2705 Model instantiated: {model_class.__name__}\")\n",
    "        print(f\"\u2705 Total parameters: {total_params:,}\")\n",
    "        print(f\"\u2705 Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        print(f\"\u2705 Device: {device}\")\n",
    "        \n",
    "        # Display model summary (using native torch instead of torchinfo)\n",
    "        print()\n",
    "        print(\"=\" * 70)\n",
    "        print(\"MODEL SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "        print(model)\n",
    "        print()\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Total parameters:      {total_params:,}\")\n",
    "        print(f\"Trainable parameters:  {trainable_params:,}\")\n",
    "        print(f\"Non-trainable params:  {total_params - trainable_params:,}\")\n",
    "        \n",
    "        # Calculate model size\n",
    "        param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "        buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "        size_mb = (param_size + buffer_size) / 1024**2\n",
    "        print(f\"Model size:            {size_mb:.2f} MB\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Failed to instantiate model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "else:\n",
    "    raise RuntimeError(f\"Could not find model class '{params['name']}' in generated code\")\n",
    "\n",
    "# Create config object for test functions (with proper vocab_size)\n",
    "class ModelConfig:\n",
    "    def __init__(self, **kwargs):\n",
    "        # Set defaults\n",
    "        self.vocab_size = 50257\n",
    "        self.max_seq_len = 512\n",
    "        self.max_batch_size = 8\n",
    "        \n",
    "        # If nodes-based config, extract common params\n",
    "        if 'nodes' in kwargs:\n",
    "            for node in kwargs['nodes']:\n",
    "                node_params = node.get('params', {})\n",
    "                if 'vocab_size' in node_params:\n",
    "                    self.vocab_size = node_params['vocab_size']\n",
    "                if 'max_seq_len' in node_params or 'seq_length' in node_params:\n",
    "                    self.max_seq_len = node_params.get('max_seq_len') or node_params.get('seq_length', 512)\n",
    "        \n",
    "        # Override with flat params if present\n",
    "        for key, value in kwargs.items():\n",
    "            if key not in ['nodes', 'version', 'model_name']:\n",
    "                setattr(self, key, value)\n",
    "\n",
    "config = ModelConfig(**config_dict)\n",
    "print(f\"\u2705 Config prepared (vocab_size={config.vocab_size}, max_seq_len={config.max_seq_len})\")\n",
    "print(\"\u2705 Ready for testing!\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# \ud83d\udd0d Tier 1: Critical Validation\n\nThese tests verify your model is mathematically sound and ready for training.\n\n**Estimated time:** ~1 minute\n\n**What's tested:**\n- \u2705 Shape validation across edge cases\n- \u2705 Gradient flow (detect vanishing/exploding gradients)\n- \u2705 Numerical stability (NaN/Inf detection)\n- \u2705 Parameter initialization quality\n- \u2705 Memory footprint scaling\n- \u2705 Inference speed benchmarks",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import test utilities from the cloned utils package\nfrom utils.test_functions import (\n    test_shape_robustness,\n    test_gradient_flow,\n    test_output_stability,\n    test_parameter_initialization,\n    test_memory_footprint,\n    test_inference_speed\n)\n\nprint(\"\u2705 Test functions loaded from utils package\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 80)\nprint(\"TIER 1: CRITICAL VALIDATION\")\nprint(\"=\" * 80)\nprint()\n\n# Test 1: Shape Robustness\nprint(\"Test 1/6: Shape Validation\")\nprint(\"-\" * 80)\nshape_results = test_shape_robustness(model, config)\ndisplay(shape_results)\nprint()\n\n# Test 2: Gradient Flow\nprint(\"Test 2/6: Gradient Flow Analysis\")\nprint(\"-\" * 80)\ngrad_results = test_gradient_flow(model, config)\ndisplay(grad_results)\nprint()\n\n# Test 3: Output Stability\nprint(\"Test 3/6: Numerical Stability\")\nprint(\"-\" * 80)\nstability_stats = test_output_stability(model, config, n_samples=100)\nprint()\n\n# Test 4: Parameter Initialization\nprint(\"Test 4/6: Parameter Initialization\")\nprint(\"-\" * 80)\nparam_results = test_parameter_initialization(model)\ndisplay(param_results)\nprint()\n\n# Test 5: Memory Footprint\nprint(\"Test 5/6: Memory Footprint Analysis\")\nprint(\"-\" * 80)\nmemory_results = test_memory_footprint(model, config)\ndisplay(memory_results)\nprint()\n\n# Test 6: Inference Speed\nprint(\"Test 6/6: Inference Speed Benchmark\")\nprint(\"-\" * 80)\nspeed_stats = test_inference_speed(model, config, n_trials=50)\nprint()\n\nprint(\"=\" * 80)\nprint(\"\u2705 TIER 1 VALIDATION COMPLETE\")\nprint(\"=\" * 80)\nprint()\nprint(\"All critical tests passed! Your model is mathematically sound.\")\nprint()\nprint(\"\ud83d\udcdd Next steps:\")\nprint(\"   \u2022 Tier 2: Advanced analysis (attention patterns, attribution)\")\nprint(\"     \u2192 Install optional dependencies in the cell before Tier 2\")\nprint(\"     \u2192 Then run Tier 2 tests\")\nprint()\nprint(\"   \u2022 Tier 3: Training utilities (fine-tuning, hyperparameter search)\")\nprint(\"     \u2192 Install optional dependencies in the cell before Tier 3\")\nprint(\"     \u2192 Then run Tier 3 tests\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# \ud83d\udd2c Tier 2: Advanced Analysis\n\nDeep dive into model behavior with advanced diagnostic tools.\n\n**Estimated time:** ~3-5 minutes\n\n**What's tested:**\n- \ud83c\udfaf **Attention Patterns:** Visualize attention weights, detect collapsed attention, analyze head specialization\n- \ud83d\udd0d **Attribution Analysis:** Identify which input tokens contribute most to predictions (using Captum)\n- \ud83d\udee1\ufe0f **Robustness Testing:** Measure stability under input perturbations and noise\n\n**Note:** These tests are optional but highly recommended for understanding model behavior.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tier 2: Advanced Analysis\n",
    "\n",
    "**Note:** Tier 2 tests use only Colab pre-installed packages (no installation required).\n",
    "\n",
    "- Test 1: Attention Pattern Analysis (uses built-in PyTorch)\n",
    "- Test 2: Robustness Testing (uses numpy/torch)\n",
    "\n",
    "All tests run automatically in the cell below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Import Tier 2 test functions\n",
    "from utils.test_functions import (\n",
    "    test_attention_patterns,\n",
    "    test_robustness\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TIER 2: ADVANCED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Test 1: Attention Patterns\n",
    "print(\"Test 1/2: Attention Pattern Analysis\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    attention_results = test_attention_patterns(model, config)\n",
    "    if attention_results is not None:\n",
    "        display(attention_results)\n",
    "    print(\"\u2705 Attention analysis complete\")\n",
    "except Exception as e:\n",
    "    print(f\"\u26a0\ufe0f Attention analysis skipped: {e}\")\n",
    "print()\n",
    "\n",
    "# Test 2: Robustness Testing\n",
    "print(\"Test 2/2: Robustness Under Noise\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    robustness_results = test_robustness(model, config, n_samples=20)\n",
    "    if robustness_results is not None:\n",
    "        display(robustness_results)\n",
    "    print(\"\u2705 Robustness analysis complete\")\n",
    "except Exception as e:\n",
    "    print(f\"\u26a0\ufe0f Robustness analysis skipped: {e}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\u2705 TIER 2 ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"Next: Scroll down for Tier 3 (Training & Fine-Tuning)\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# \ud83d\ude80 Tier 3: Training & Production Utilities\n",
    "\n",
    "**Training utilities have been moved to a separate notebook to prevent dependency conflicts.**\n",
    "\n",
    "## \ud83d\udcd3 Continue to Training Notebook\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/matt-hans/transformer-builder-colab-templates/blob/main/training.ipynb)\n",
    "\n",
    "**Click the button above to open the training notebook in Colab.**\n",
    "\n",
    "### What's included in training.ipynb:\n",
    "- \ud83c\udf93 **Fine-Tuning:** Training loop with loss tracking and gradient monitoring\n",
    "- \ud83d\udd27 **Hyperparameter Search:** Automated optimization using Optuna\n",
    "- \ud83d\udcca **Benchmark Comparison:** Compare against production baselines (distilgpt2, bert-base)\n",
    "\n",
    "### Before running training.ipynb:\n",
    "1. **Runtime \u2192 Restart runtime** (fresh environment required)\n",
    "2. **Paste your same Gist ID** from Cell 3 above\n",
    "3. **Run all cells** - dependencies will install automatically\n",
    "\n",
    "**Estimated time:** 10-20 minutes (GPU recommended)\n",
    "\n",
    "---\n",
    "\n",
    "### Why separate notebooks?\n",
    "\n",
    "Training utilities require `pytorch-lightning` and `optuna`, which have NumPy version requirements that conflict with the zero-installation strategy used in this testing notebook.\n",
    "\n",
    "Running them in separate runtimes ensures:\n",
    "- \u2705 Testing notebook (this one) stays fast and dependency-free\n",
    "- \u2705 Training notebook has all the tools it needs without corruption\n",
    "- \u2705 Clear separation between validation and training workflows\n",
    "\n",
    "---\n",
    "\n",
    "**Repository:** [transformer-builder-colab-templates](https://github.com/matt-hans/transformer-builder-colab-templates)\n",
    "\n",
    "**Source:** Generated from [Transformer Builder](https://transformer-builder.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
