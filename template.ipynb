{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Transformer Builder - Advanced Testing Lab\n",
    "\n",
    "Welcome! This notebook provides comprehensive testing and training capabilities for your custom transformer architecture.\n",
    "\n",
    "**What's included:**\n",
    "- ‚úÖ **Tier 1:** Critical validation (shape, gradients, numerical stability)\n",
    "- üî¨ **Tier 2:** Advanced analysis (attention patterns, robustness, profiling)\n",
    "- üöÄ **Tier 3:** Training utilities (fine-tuning, hyperparameter sweeps, benchmarks)\n",
    "\n",
    "**Quick Start:**\n",
    "1. Click \"Run all\" (Runtime ‚Üí Run all)\n",
    "2. Review Tier 1 results (should complete in ~1 minute)\n",
    "3. Explore Tier 2/3 sections as needed\n",
    "\n",
    "**Source:** Generated from [Transformer Builder](https://transformer-builder.com)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies\n",
    "\n",
    "This may take 30-60 seconds on first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core dependencies\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers datasets evaluate accelerate\n",
    "!pip install -q scipy matplotlib seaborn pandas tqdm\n",
    "!pip install -q torchinfo  # For model summaries\n",
    "\n",
    "print(\"‚úì Core dependencies installed\")\n",
    "\n",
    "# Install Tier 2 dependencies (optional)\n",
    "!pip install -q captum  # For attribution analysis\n",
    "print(\"‚úì Tier 2 dependencies installed\")\n",
    "\n",
    "# Install Tier 3 dependencies (optional)\n",
    "!pip install -q optuna  # For hyperparameter optimization\n",
    "print(\"‚úì Tier 3 dependencies installed\")\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Custom Model from URL\n",
    "\n",
    "This cell extracts your model code from the URL fragment (passed from Transformer Builder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "from google.colab import output\n",
    "\n",
    "# JavaScript to extract URL fragment parameters\n",
    "js_script = \"\"\"\n",
    "function getFragmentParameter(name) {\n",
    "  const urlParams = new URLSearchParams(window.location.hash.substring(1));\n",
    "  return urlParams.get(name);\n",
    "}\n",
    "return {\n",
    "  model: getFragmentParameter('model'),\n",
    "  config: getFragmentParameter('config'),\n",
    "  name: getFragmentParameter('name') || 'CustomTransformer'\n",
    "};\n",
    "\"\"\"\n",
    "\n",
    "# Extract parameters\n",
    "params = output.eval_js(js_script)\n",
    "\n",
    "def decode_urlsafe_b64(s):\n",
    "    \"\"\"Decode URL-safe Base64 string\"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    # Convert URL-safe back to standard Base64\n",
    "    s = s.replace('-', '+').replace('_', '/')\n",
    "    # Add padding if needed\n",
    "    padding = 4 - (len(s) % 4)\n",
    "    if padding != 4:\n",
    "        s += '=' * padding\n",
    "    return base64.b64decode(s).decode('utf-8')\n",
    "\n",
    "if params['model'] and params['config']:\n",
    "    # Decode and write model code\n",
    "    model_code = decode_urlsafe_b64(params['model'])\n",
    "    with open('custom_transformer.py', 'w') as f:\n",
    "        f.write(model_code)\n",
    "    \n",
    "    # Decode and write config\n",
    "    config_json = decode_urlsafe_b64(params['config'])\n",
    "    with open('config.json', 'w') as f:\n",
    "        f.write(config_json)\n",
    "    \n",
    "    print(f\"‚úÖ Model code loaded successfully\")\n",
    "    print(f\"‚úÖ Model name: {params['name']}\")\n",
    "    print(f\"‚úÖ Code size: {len(model_code):,} bytes\")\n",
    "    print(f\"‚úÖ Config size: {len(config_json):,} bytes\")\n",
    "    \n",
    "    # Display model code preview\n",
    "    print(\"\\n--- Model Code Preview (first 30 lines) ---\")\n",
    "    print('\\n'.join(model_code.split('\\n')[:30]))\n",
    "    if len(model_code.split('\\n')) > 30:\n",
    "        print(\"...\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No model code found in URL\")\n",
    "    print(\"Loading example model for demonstration...\\n\")\n",
    "    \n",
    "    # Fallback: Create example model\n",
    "    example_code = \"\"\"import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ExampleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=50257, d_model=512, nhead=8, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.transformer(x)\n",
    "        return self.output_projection(x)\n",
    "\"\"\"\n",
    "    \n",
    "    with open('custom_transformer.py', 'w') as f:\n",
    "        f.write(example_code)\n",
    "    \n",
    "    with open('config.json', 'w') as f:\n",
    "        json.dump({\n",
    "            \"vocab_size\": 50257,\n",
    "            \"d_model\": 512,\n",
    "            \"nhead\": 8,\n",
    "            \"num_layers\": 6\n",
    "        }, f)\n",
    "    \n",
    "    params['name'] = 'ExampleTransformer'\n",
    "    print(\"‚úÖ Example model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Dependency Detection\n",
    "\n",
    "Automatically detect and install any custom dependencies your model needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Parse imports from generated code\n",
    "with open('custom_transformer.py', 'r') as f:\n",
    "    source_code = f.read()\n",
    "    tree = ast.parse(source_code)\n",
    "\n",
    "# Extract all imports\n",
    "imports = set()\n",
    "for node in ast.walk(tree):\n",
    "    if isinstance(node, ast.Import):\n",
    "        for alias in node.names:\n",
    "            imports.add(alias.name.split('.')[0])\n",
    "    elif isinstance(node, ast.ImportFrom):\n",
    "        if node.module:\n",
    "            imports.add(node.module.split('.')[0])\n",
    "\n",
    "print(f\"Detected imports: {', '.join(sorted(imports))}\")\n",
    "\n",
    "# Standard library modules (don't need pip install)\n",
    "stdlib_modules = {\n",
    "    'abc', 'collections', 'dataclasses', 'functools', 'json', 'math',\n",
    "    'typing', 'warnings', 'os', 'sys', 're', 'time', 'copy'\n",
    "}\n",
    "\n",
    "# Already installed\n",
    "installed_modules = {\n",
    "    'torch', 'transformers', 'numpy', 'scipy', 'matplotlib',\n",
    "    'pandas', 'seaborn', 'tqdm', 'torchinfo', 'captum', 'optuna'\n",
    "}\n",
    "\n",
    "# Find missing packages\n",
    "missing = imports - stdlib_modules - installed_modules\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\nInstalling additional dependencies: {', '.join(missing)}\")\n",
    "    for package in missing:\n",
    "        try:\n",
    "            subprocess.check_call(\n",
    "                [sys.executable, '-m', 'pip', 'install', '-q', package],\n",
    "                stdout=subprocess.DEVNULL,\n",
    "                stderr=subprocess.DEVNULL\n",
    "            )\n",
    "            print(f\"  ‚úÖ Installed {package}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"  ‚ö†Ô∏è Failed to install {package} (may not be a pip package)\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All dependencies already installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Instantiate Model\n",
    "\n",
    "Load your custom transformer and prepare for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "\n",
    "# Import the custom model\n",
    "exec(open('custom_transformer.py').read())\n",
    "\n",
    "# Load config\n",
    "with open('config.json') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "# Find the model class\n",
    "model_class = None\n",
    "for name, obj in list(globals().items()):\n",
    "    if isinstance(obj, type) and issubclass(obj, nn.Module) and obj is not nn.Module:\n",
    "        if name == params['name']:\n",
    "            model_class = obj\n",
    "            break\n",
    "\n",
    "if model_class is None:\n",
    "    # Fallback: find any nn.Module subclass\n",
    "    for name, obj in list(globals().items()):\n",
    "        if isinstance(obj, type) and issubclass(obj, nn.Module) and obj is not nn.Module:\n",
    "            model_class = obj\n",
    "            print(f\"‚ö†Ô∏è Using {name} (expected {params['name']})\")\n",
    "            break\n",
    "\n",
    "if model_class:\n",
    "    # Instantiate model\n",
    "    try:\n",
    "        model = model_class(**config_dict)\n",
    "        model.eval()\n",
    "        \n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"‚úÖ Model instantiated: {model_class.__name__}\")\n",
    "        print(f\"‚úÖ Total parameters: {total_params:,}\")\n",
    "        print(f\"‚úÖ Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        print(f\"‚úÖ Device: {device}\")\n",
    "        \n",
    "        # Display model summary\n",
    "        print(\"\\n--- Model Summary ---\")\n",
    "        try:\n",
    "            # Create dummy input based on config\n",
    "            vocab_size = config_dict.get('vocab_size', 50257)\n",
    "            dummy_input = torch.randint(0, vocab_size, (1, 32)).to(device)\n",
    "            summary(model, input_data=dummy_input, depth=3)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not generate summary: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to instantiate model: {e}\")\n",
    "        raise\n",
    "else:\n",
    "    raise RuntimeError(f\"Could not find model class '{params['name']}' in generated code\")\n",
    "\n",
    "# Create config object for test functions\n",
    "class ModelConfig:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "config = ModelConfig(**config_dict)\n",
    "print(\"\\n‚úÖ Ready for testing!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
