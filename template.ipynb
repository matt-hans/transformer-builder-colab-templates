{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Transformer Builder - Advanced Testing Lab\n",
    "\n",
    "Welcome! This notebook provides comprehensive testing and training capabilities for your custom transformer architecture.\n",
    "\n",
    "**What's included:**\n",
    "- ‚úÖ **Tier 1:** Critical validation (shape, gradients, numerical stability)\n",
    "- üî¨ **Tier 2:** Advanced analysis (attention patterns, robustness, profiling)\n",
    "- üöÄ **Tier 3:** Training utilities (fine-tuning, hyperparameter sweeps, benchmarks)\n",
    "\n",
    "**Quick Start:**\n",
    "1. Click \"Run all\" (Runtime ‚Üí Run all)\n",
    "2. Review Tier 1 results (should complete in ~1 minute)\n",
    "3. Explore Tier 2/3 sections as needed\n",
    "\n",
    "**Source:** Generated from [Transformer Builder](https://transformer-builder.com)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies\n",
    "\n",
    "This may take 30-60 seconds on first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core dependencies\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers datasets evaluate accelerate\n",
    "!pip install -q scipy matplotlib seaborn pandas tqdm\n",
    "!pip install -q torchinfo  # For model summaries\n",
    "\n",
    "print(\"‚úì Core dependencies installed\")\n",
    "\n",
    "# Install Tier 2 dependencies (optional)\n",
    "!pip install -q captum  # For attribution analysis\n",
    "print(\"‚úì Tier 2 dependencies installed\")\n",
    "\n",
    "# Install Tier 3 dependencies (optional)\n",
    "!pip install -q optuna  # For hyperparameter optimization\n",
    "print(\"‚úì Tier 3 dependencies installed\")\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Custom Model from URL\n",
    "\n",
    "This cell extracts your model code from the URL fragment (passed from Transformer Builder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import urllib.request\nimport urllib.parse\nimport json\nfrom google.colab import output\n\n# Extract gist_id from URL query parameters\njs_script = \"\"\"\nconst params = new URLSearchParams(window.location.search);\nreturn {\n  gist_id: params.get('gist_id'),\n  model_name: params.get('name') || 'CustomTransformer'\n};\n\"\"\"\n\nparams = output.eval_js(js_script)\ngist_id = params.get('gist_id')\n\nif gist_id:\n    try:\n        print(f\"üì• Loading model from GitHub Gist: {gist_id}\")\n        \n        # Fetch gist data from GitHub API\n        gist_url = f\"https://api.github.com/gists/{gist_id}\"\n        with urllib.request.urlopen(gist_url) as response:\n            gist_data = json.loads(response.read().decode('utf-8'))\n        \n        # Extract files from gist\n        files = gist_data.get('files', {})\n        \n        if 'model.py' not in files or 'config.json' not in files:\n            raise ValueError(\"Gist missing required files (model.py or config.json)\")\n        \n        model_code = files['model.py']['content']\n        config_json = files['config.json']['content']\n        \n        # Write files to disk\n        with open('custom_transformer.py', 'w') as f:\n            f.write(model_code)\n        \n        with open('config.json', 'w') as f:\n            f.write(config_json)\n        \n        model_name = params.get('model_name', 'CustomTransformer')\n        \n        print(f\"‚úÖ Model code loaded successfully!\")\n        print(f\"‚úÖ Model name: {model_name}\")\n        print(f\"‚úÖ Code size: {len(model_code):,} bytes\")\n        print(f\"‚úÖ Config size: {len(config_json):,} bytes\")\n        print(f\"‚úÖ Gist URL: {gist_data.get('html_url', 'N/A')}\")\n        \n    except Exception as e:\n        print(f\"‚ùå Failed to load model from Gist: {e}\")\n        print(\"‚ö†Ô∏è Falling back to example model...\")\n        gist_id = None\n\nif not gist_id:\n    print(\"‚ö†Ô∏è No gist_id found in URL\")\n    print(\"Loading example model for demonstration...\\n\")\n    \n    # Fallback: Create example model\n    example_code = \"\"\"import torch\nimport torch.nn as nn\n\nclass ExampleTransformer(nn.Module):\n    def __init__(self, vocab_size=50257, d_model=512, nhead=8, num_layers=6):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.output_projection = nn.Linear(d_model, vocab_size)\n    \n    def forward(self, input_ids):\n        x = self.embedding(input_ids)\n        x = self.transformer(x)\n        return self.output_projection(x)\n\"\"\"\n    \n    with open('custom_transformer.py', 'w') as f:\n        f.write(example_code)\n    \n    with open('config.json', 'w') as f:\n        json.dump({\n            \"vocab_size\": 50257,\n            \"d_model\": 512,\n            \"nhead\": 8,\n            \"num_layers\": 6\n        }, f)\n    \n    params = {'name': 'ExampleTransformer'}\n    print(\"‚úÖ Example model loaded\")"
  },
  {
   "cell_type": "markdown",
   "source": "## üìÑ View Loaded Model Code\n\nThis cell displays the Python code that was loaded from your Transformer Builder export. You can review the architecture before running tests.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display the loaded model code for transparency\nprint(\"=\" * 80)\nprint(\"üìÑ LOADED MODEL CODE (custom_transformer.py)\")\nprint(\"=\" * 80)\nprint()\n\nwith open('custom_transformer.py', 'r') as f:\n    model_code_display = f.read()\n\n# Use syntax highlighting\nfrom IPython.display import Code\ndisplay(Code(model_code_display, language='python'))\n\nprint()\nprint(\"=\" * 80)\nprint(\"üìã MODEL CONFIGURATION (config.json)\")\nprint(\"=\" * 80)\nprint()\n\nwith open('config.json', 'r') as f:\n    config_display = json.load(f)\n\n# Pretty print JSON\nprint(json.dumps(config_display, indent=2))\nprint()\nprint(\"‚úÖ You can now proceed to run the model instantiation and tests below!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Dependency Detection\n",
    "\n",
    "Automatically detect and install any custom dependencies your model needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Parse imports from generated code\n",
    "with open('custom_transformer.py', 'r') as f:\n",
    "    source_code = f.read()\n",
    "    tree = ast.parse(source_code)\n",
    "\n",
    "# Extract all imports\n",
    "imports = set()\n",
    "for node in ast.walk(tree):\n",
    "    if isinstance(node, ast.Import):\n",
    "        for alias in node.names:\n",
    "            imports.add(alias.name.split('.')[0])\n",
    "    elif isinstance(node, ast.ImportFrom):\n",
    "        if node.module:\n",
    "            imports.add(node.module.split('.')[0])\n",
    "\n",
    "print(f\"Detected imports: {', '.join(sorted(imports))}\")\n",
    "\n",
    "# Standard library modules (don't need pip install)\n",
    "stdlib_modules = {\n",
    "    'abc', 'collections', 'dataclasses', 'functools', 'json', 'math',\n",
    "    'typing', 'warnings', 'os', 'sys', 're', 'time', 'copy'\n",
    "}\n",
    "\n",
    "# Already installed\n",
    "installed_modules = {\n",
    "    'torch', 'transformers', 'numpy', 'scipy', 'matplotlib',\n",
    "    'pandas', 'seaborn', 'tqdm', 'torchinfo', 'captum', 'optuna'\n",
    "}\n",
    "\n",
    "# Find missing packages\n",
    "missing = imports - stdlib_modules - installed_modules\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\nInstalling additional dependencies: {', '.join(missing)}\")\n",
    "    for package in missing:\n",
    "        try:\n",
    "            subprocess.check_call(\n",
    "                [sys.executable, '-m', 'pip', 'install', '-q', package],\n",
    "                stdout=subprocess.DEVNULL,\n",
    "                stderr=subprocess.DEVNULL\n",
    "            )\n",
    "            print(f\"  ‚úÖ Installed {package}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"  ‚ö†Ô∏è Failed to install {package} (may not be a pip package)\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All dependencies already installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Instantiate Model\n",
    "\n",
    "Load your custom transformer and prepare for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "\n",
    "# Import the custom model\n",
    "exec(open('custom_transformer.py').read())\n",
    "\n",
    "# Load config\n",
    "with open('config.json') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "# Find the model class\n",
    "model_class = None\n",
    "for name, obj in list(globals().items()):\n",
    "    if isinstance(obj, type) and issubclass(obj, nn.Module) and obj is not nn.Module:\n",
    "        if name == params['name']:\n",
    "            model_class = obj\n",
    "            break\n",
    "\n",
    "if model_class is None:\n",
    "    # Fallback: find any nn.Module subclass\n",
    "    for name, obj in list(globals().items()):\n",
    "        if isinstance(obj, type) and issubclass(obj, nn.Module) and obj is not nn.Module:\n",
    "            model_class = obj\n",
    "            print(f\"‚ö†Ô∏è Using {name} (expected {params['name']})\")\n",
    "            break\n",
    "\n",
    "if model_class:\n",
    "    # Instantiate model\n",
    "    try:\n",
    "        model = model_class(**config_dict)\n",
    "        model.eval()\n",
    "        \n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"‚úÖ Model instantiated: {model_class.__name__}\")\n",
    "        print(f\"‚úÖ Total parameters: {total_params:,}\")\n",
    "        print(f\"‚úÖ Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        print(f\"‚úÖ Device: {device}\")\n",
    "        \n",
    "        # Display model summary\n",
    "        print(\"\\n--- Model Summary ---\")\n",
    "        try:\n",
    "            # Create dummy input based on config\n",
    "            vocab_size = config_dict.get('vocab_size', 50257)\n",
    "            dummy_input = torch.randint(0, vocab_size, (1, 32)).to(device)\n",
    "            summary(model, input_data=dummy_input, depth=3)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not generate summary: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to instantiate model: {e}\")\n",
    "        raise\n",
    "else:\n",
    "    raise RuntimeError(f\"Could not find model class '{params['name']}' in generated code\")\n",
    "\n",
    "# Create config object for test functions\n",
    "class ModelConfig:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "config = ModelConfig(**config_dict)\n",
    "print(\"\\n‚úÖ Ready for testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# üîç Tier 1: Critical Validation\n\nThese tests verify your model is mathematically sound and ready for training.\n\n**Estimated time:** ~1 minute\n\n**What's tested:**\n- ‚úÖ Shape validation across edge cases\n- ‚úÖ Gradient flow (detect vanishing/exploding gradients)\n- ‚úÖ Numerical stability (NaN/Inf detection)\n- ‚úÖ Parameter initialization quality\n- ‚úÖ Memory footprint scaling\n- ‚úÖ Inference speed benchmarks",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import test utilities\n!wget -q https://raw.githubusercontent.com/matt-hans/transformer-builder-colab-templates/main/utils/test_functions.py\nfrom test_functions import (\n    test_shape_robustness,\n    test_gradient_flow,\n    test_output_stability,\n    test_parameter_initialization,\n    test_memory_footprint,\n    test_inference_speed\n)\n\nprint(\"‚úÖ Test functions loaded\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 80)\nprint(\"TIER 1: CRITICAL VALIDATION\")\nprint(\"=\" * 80)\nprint()\n\n# Test 1: Shape Robustness\nprint(\"Test 1/6: Shape Validation\")\nprint(\"-\" * 80)\nshape_results = test_shape_robustness(model, config)\ndisplay(shape_results)\nprint()\n\n# Test 2: Gradient Flow\nprint(\"Test 2/6: Gradient Flow Analysis\")\nprint(\"-\" * 80)\ngrad_results = test_gradient_flow(model, config)\ndisplay(grad_results)\nprint()\n\n# Test 3: Output Stability\nprint(\"Test 3/6: Numerical Stability\")\nprint(\"-\" * 80)\nstability_stats = test_output_stability(model, config, n_samples=100)\nprint()\n\n# Test 4: Parameter Initialization\nprint(\"Test 4/6: Parameter Initialization\")\nprint(\"-\" * 80)\nparam_results = test_parameter_initialization(model)\ndisplay(param_results)\nprint()\n\n# Test 5: Memory Footprint\nprint(\"Test 5/6: Memory Footprint Analysis\")\nprint(\"-\" * 80)\nmemory_results = test_memory_footprint(model, config)\ndisplay(memory_results)\nprint()\n\n# Test 6: Inference Speed\nprint(\"Test 6/6: Inference Speed Benchmark\")\nprint(\"-\" * 80)\nspeed_stats = test_inference_speed(model, config, n_trials=50)\nprint()\n\nprint(\"=\" * 80)\nprint(\"‚úÖ TIER 1 VALIDATION COMPLETE\")\nprint(\"=\" * 80)\nprint()\nprint(\"All critical tests passed! Your model is ready for advanced analysis.\")\nprint()\nprint(\"Next steps:\")\nprint(\"‚Ä¢ Scroll down for Tier 2 (Advanced Analysis)\")\nprint(\"‚Ä¢ Or jump to Tier 3 (Training & Fine-Tuning)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}