{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# \ud83e\uddea Transformer Builder - Advanced Testing Lab\n\n**Welcome! This notebook tests your custom transformer architecture.**\n\n---\n\n## \ud83d\ude80 **Quick Start (3 Steps)**\n\n### **STEP 1:** Paste Your Gist ID\n\u2193 Scroll down to Cell 3 and paste the Gist ID you received from Transformer Builder\n\n### **STEP 2:** Run All Cells  \nClick **Runtime \u2192 Run all** (or run cells one-by-one)\n\n### **STEP 3:** Review Test Results\nYour model will be validated through 3 testing tiers\n\n---\n\n## \ud83d\udccb **What's Included:**\n\n- \u2705 **Tier 1:** Critical validation (shape, gradients, numerical stability)\n- \ud83d\udd2c **Tier 2:** Advanced analysis (attention patterns, robustness, profiling)\n- \ud83d\ude80 **Tier 3:** Training utilities (fine-tuning, hyperparameter sweeps, benchmarks)\n\n---\n\n## \u26a0\ufe0f **First Time Setup:**\n\nIf this is your first time OR you're continuing from a previous session:\n\n1. **Runtime** \u2192 **Restart runtime** (takes 5 seconds)\n2. **Edit** \u2192 **Clear all outputs** (optional, cleans up UI)\n3. **Scroll down to Cell 3** \u2192 Paste your Gist ID\n4. **Runtime** \u2192 **Run all**\n\nThis ensures a clean environment and prevents dependency conflicts.\n\n---\n\n**Source:** Generated from [Transformer Builder](https://transformer-builder.com)"
  },
  {
   "cell_type": "markdown",
   "source": "# \ud83e\uddea Transformer Builder - Advanced Testing Lab\n\nWelcome! This notebook provides comprehensive testing and training capabilities for your custom transformer architecture.\n\n**What's included:**\n- \u2705 **Tier 1:** Critical validation (shape, gradients, numerical stability)\n- \ud83d\udd2c **Tier 2:** Advanced analysis (attention patterns, robustness, profiling)\n- \ud83d\ude80 **Tier 3:** Training utilities (fine-tuning, hyperparameter sweeps, benchmarks)\n\n**Quick Start:**\n1. Click \"Run all\" (Runtime \u2192 Run all)\n2. Review Tier 1 results (should complete in ~1 minute)\n3. Explore Tier 2/3 sections as needed\n\n**Source:** Generated from [Transformer Builder](https://transformer-builder.com)\n\n---",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## \ud83d\udccb **STEP 1: Paste Your Gist ID**\n\nWhen you exported from **Transformer Builder**, you received a **Gist ID**.\n\n**Paste it in the cell below and run it.**\n\nIf you don't have a Gist ID yet, go back to Transformer Builder and click **\"Export to Colab\"**."
  },
  {
   "cell_type": "code",
   "source": "# ==============================================================================\n# GIST ID INPUT - Paste the ID from Transformer Builder\n# ==============================================================================\n\n#@title \ud83d\udce5 **Paste Your Gist ID Here**\nGIST_ID = \"\"  #@param {type:\"string\"}\n\n#@markdown ---\n#@markdown **Where to find your Gist ID:**\n#@markdown 1. Go to Transformer Builder\n#@markdown 2. Click \"Export to Colab\"\n#@markdown 3. Copy the Gist ID from the modal\n#@markdown 4. Paste it above and run this cell\n\nif not GIST_ID or not GIST_ID.strip():\n    print(\"=\" * 70)\n    print(\"\u26a0\ufe0f  NO GIST ID PROVIDED\")\n    print(\"=\" * 70)\n    print()\n    print(\"Please paste your Gist ID in the field above and re-run this cell.\")\n    print()\n    print(\"If you don't have a Gist ID:\")\n    print(\"  1. Go to Transformer Builder\")\n    print(\"  2. Click 'Export to Colab'\")\n    print(\"  3. Copy the Gist ID from the modal\")\n    print(\"  4. Come back here and paste it\")\n    print()\n    raise ValueError(\"Gist ID is required to load your custom model\")\nelse:\n    # Validate format\n    import re\n    if not re.fullmatch(r\"[A-Za-z0-9]+\", GIST_ID.strip()):\n        print(\"=\" * 70)\n        print(\"\u26a0\ufe0f  INVALID GIST ID FORMAT\")\n        print(\"=\" * 70)\n        print()\n        print(f\"The Gist ID you entered: {GIST_ID!r}\")\n        print()\n        print(\"Gist IDs should be alphanumeric (e.g., 'abc123def456')\")\n        print(\"Please check and re-enter.\")\n        print()\n        raise ValueError(\"Invalid Gist ID format\")\n    \n    # Store for later use\n    GIST_ID = GIST_ID.strip()\n    \n    print(\"=\" * 70)\n    print(\"\u2705 GIST ID SAVED\")\n    print(\"=\" * 70)\n    print()\n    print(f\"Gist ID: {GIST_ID}\")\n    print()\n    print(\"You can now proceed to run the cells below to:\")\n    print(\"  1. Install dependencies\")\n    print(\"  2. Load your custom model\")\n    print(\"  3. Run tests\")\n    print()\n    print(\"\ud83d\udca1 Tip: Click 'Runtime \u2192 Run all' to execute everything automatically\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#",
    " ",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "",
    "",
    "#",
    " ",
    "R",
    "U",
    "N",
    "T",
    "I",
    "M",
    "E",
    " ",
    "F",
    "R",
    "E",
    "S",
    "H",
    "N",
    "E",
    "S",
    "S",
    " ",
    "D",
    "E",
    "T",
    "E",
    "C",
    "T",
    "I",
    "O",
    "N",
    " ",
    "-",
    " ",
    "P",
    "r",
    "e",
    "v",
    "e",
    "n",
    "t",
    "s",
    " ",
    "r",
    "e",
    "u",
    "s",
    "e",
    "d",
    " ",
    "r",
    "u",
    "n",
    "t",
    "i",
    "m",
    "e",
    "s",
    " ",
    "w",
    "i",
    "t",
    "h",
    " ",
    "c",
    "o",
    "r",
    "r",
    "u",
    "p",
    "t",
    "e",
    "d",
    " ",
    "p",
    "a",
    "c",
    "k",
    "a",
    "g",
    "e",
    "s",
    "",
    "",
    "#",
    " ",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "=",
    "",
    "",
    "",
    "",
    "i",
    "m",
    "p",
    "o",
    "r",
    "t",
    " ",
    "o",
    "s",
    "",
    "",
    "",
    "",
    "#",
    " ",
    "C",
    "h",
    "e",
    "c",
    "k",
    " ",
    "i",
    "f",
    " ",
    "t",
    "h",
    "i",
    "s",
    " ",
    "r",
    "u",
    "n",
    "t",
    "i",
    "m",
    "e",
    " ",
    "w",
    "a",
    "s",
    " ",
    "p",
    "r",
    "e",
    "v",
    "i",
    "o",
    "u",
    "s",
    "l",
    "y",
    " ",
    "u",
    "s",
    "e",
    "d",
    "",
    "",
    "R",
    "U",
    "N",
    "T",
    "I",
    "M",
    "E",
    "_",
    "M",
    "A",
    "R",
    "K",
    "E",
    "R",
    " ",
    "=",
    " ",
    "\"",
    "/",
    "t",
    "m",
    "p",
    "/",
    "t",
    "r",
    "a",
    "n",
    "s",
    "f",
    "o",
    "r",
    "m",
    "e",
    "r",
    "_",
    "b",
    "u",
    "i",
    "l",
    "d",
    "e",
    "r",
    "_",
    "r",
    "u",
    "n",
    "t",
    "i",
    "m",
    "e",
    "_",
    "u",
    "s",
    "e",
    "d",
    "\"",
    "",
    "",
    "",
    "",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "\"",
    "=",
    "\"",
    " ",
    "*",
    " ",
    "7",
    "0",
    ")",
    "",
    "",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "\"",
    "\ud83d\udd0d",
    " ",
    "R",
    "U",
    "N",
    "T",
    "I",
    "M",
    "E",
    " ",
    "F",
    "R",
    "E",
    "S",
    "H",
    "N",
    "E",
    "S",
    "S",
    " ",
    "C",
    "H",
    "E",
    "C",
    "K",
    "\"",
    ")",
    "",
    "",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "\"",
    "=",
    "\"",
    " ",
    "*",
    " ",
    "7",
    "0",
    ")",
    "",
    "",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    ")",
    "",
    "",
    "",
    "",
    "i",
    "f",
    " ",
    "o",
    "s",
    ".",
    "p",
    "a",
    "t",
    "h",
    ".",
    "e",
    "x",
    "i",
    "s",
    "t",
    "s",
    "(",
    "R",
    "U",
    "N",
    "T",
    "I",
    "M",
    "E",
    "_",
    "M",
    "A",
    "R",
    "K",
    "E",
    "R",
    ")",
    ":",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "\"",
    "\u26a0",
    "\ufe0f",
    " ",
    " ",
    "W",
    "A",
    "R",
    "N",
    "I",
    "N",
    "G",
    ":",
    " ",
    "T",
    "h",
    "i",
    "s",
    " ",
    "r",
    "u",
    "n",
    "t",
    "i",
    "m",
    "e",
    " ",
    "w",
    "a",
    "s",
    " ",
    "p",
    "r",
    "e",
    "v",
    "i",
    "o",
    "u",
    "s",
    "l",
    "y",
    " ",
    "u",
    "s",
    "e",
    "d",
    "!",
    "\"",
    ")",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    ")",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "\"",
    "R",
    "e",
    "u",
    "s",
    "i",
    "n",
    "g",
    " ",
    "r",
    "u",
    "n",
    "t",
    "i",
    "m",
    "e",
    "s",
    " ",
    "c",
    "a",
    "n",
    " ",
    "c",
    "a",
    "u",
    "s",
    "e",
    " ",
    "d",
    "e",
    "p",
    "e",
    "n",
    "d",
    "e",
    "n",
    "c",
    "y",
    " ",
    "c",
    "o",
    "n",
    "f",
    "l",
    "i",
    "c",
    "t",
    "s",
    ".",
    "\"",
    ")",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    ")",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "\"",
    "\u2705",
    " ",
    "R",
    "E",
    "C",
    "O",
    "M",
    "M",
    "E",
    "N",
    "D",
    "E",
    "D",
    ":",
    " ",
    "R",
    "e",
    "s",
    "t",
    "a",
    "r",
    "t",
    " ",
    "r",
    "u",
    "n",
    "t",
    "i",
    "m",
    "e",
    " ",
    "f",
    "o",
    "r",
    " ",
    "c",
    "l",
    "e",
    "a",
    "n",
    " ",
    "e",
    "n",
    "v",
    "i",
    "r",
    "o",
    "n",
    "m",
    "e",
    "n",
    "t",
    "\"",
    ")",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "\"",
    " ",
    " ",
    " ",
    "(",
    "R",
    "u",
    "n",
    "t",
    "i",
    "m",
    "e",
    " ",
    "\u2192",
    " ",
    "R",
    "e",
    "s",
    "t",
    "a",
    "r",
    "t",
    " ",
    "r",
    "u",
    "n",
    "t",
    "i",
    "m",
    "e",
    " ",
    "\u2192",
    " ",
    "R",
    "u",
    "n",
    " ",
    "a",
    "l",
    "l",
    ")",
    "\"",
    ")",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    ")",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "\"",
    "=",
    "\"",
    " ",
    "*",
    " ",
    "7",
    "0",
    ")",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    "#",
    " ",
    "G",
    "i",
    "v",
    "e",
    " ",
    "u",
    "s",
    "e",
    "r",
    " ",
    "o",
    "p",
    "t",
    "i",
    "o",
    "n",
    " ",
    "t",
    "o",
    " ",
    "c",
    "o",
    "n",
    "t",
    "i",
    "n",
    "u",
    "e",
    " ",
    "a",
    "n",
    "y",
    "w",
    "a",
    "y",
    " ",
    "(",
    "a",
    "d",
    "v",
    "a",
    "n",
    "c",
    "e",
    "d",
    " ",
    "u",
    "s",
    "e",
    "r",
    "s",
    ")",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    "r",
    "e",
    "s",
    "p",
    "o",
    "n",
    "s",
    "e",
    " ",
    "=",
    " ",
    "i",
    "n",
    "p",
    "u",
    "t",
    "(",
    "\"",
    "C",
    "o",
    "n",
    "t",
    "i",
    "n",
    "u",
    "e",
    " ",
    "w",
    "i",
    "t",
    "h",
    " ",
    "r",
    "e",
    "u",
    "s",
    "e",
    "d",
    " ",
    "r",
    "u",
    "n",
    "t",
    "i",
    "m",
    "e",
    " ",
    "a",
    "n",
    "y",
    "w",
    "a",
    "y",
    "?",
    " ",
    "[",
    "y",
    "/",
    "N",
    "]",
    ":",
    " ",
    "\"",
    ")",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    "i",
    "f",
    " ",
    "r",
    "e",
    "s",
    "p",
    "o",
    "n",
    "s",
    "e",
    ".",
    "l",
    "o",
    "w",
    "e",
    "r",
    "(",
    ")",
    " ",
    "!",
    "=",
    " ",
    "'",
    "y",
    "'",
    ":",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    ")",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "\"",
    "\u2705",
    " ",
    "G",
    "o",
    "o",
    "d",
    " ",
    "c",
    "h",
    "o",
    "i",
    "c",
    "e",
    "!",
    " ",
    "P",
    "l",
    "e",
    "a",
    "s",
    "e",
    " ",
    "r",
    "e",
    "s",
    "t",
    "a",
    "r",
    "t",
    " ",
    "t",
    "h",
    "e",
    " ",
    "r",
    "u",
    "n",
    "t",
    "i",
    "m",
    "e",
    " ",
    "a",
    "n",
    "d",
    " ",
    "t",
    "r",
    "y",
    " ",
    "a",
    "g",
    "a",
    "i",
    "n",
    ".",
    "\"",
    ")",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    "r",
    "a",
    "i",
    "s",
    "e",
    " ",
    "R",
    "u",
    "n",
    "t",
    "i",
    "m",
    "e",
    "E",
    "r",
    "r",
    "o",
    "r",
    "(",
    "\"",
    "R",
    "u",
    "n",
    "t",
    "i",
    "m",
    "e",
    " ",
    "r",
    "e",
    "s",
    "t",
    "a",
    "r",
    "t",
    " ",
    "r",
    "e",
    "c",
    "o",
    "m",
    "m",
    "e",
    "n",
    "d",
    "e",
    "d",
    " ",
    "f",
    "o",
    "r",
    " ",
    "c",
    "l",
    "e",
    "a",
    "n",
    " ",
    "e",
    "n",
    "v",
    "i",
    "r",
    "o",
    "n",
    "m",
    "e",
    "n",
    "t",
    "\"",
    ")",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    "e",
    "l",
    "s",
    "e",
    ":",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    ")",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "\"",
    "\u26a0",
    "\ufe0f",
    " ",
    " ",
    "P",
    "r",
    "o",
    "c",
    "e",
    "e",
    "d",
    "i",
    "n",
    "g",
    " ",
    "w",
    "i",
    "t",
    "h",
    " ",
    "r",
    "e",
    "u",
    "s",
    "e",
    "d",
    " ",
    "r",
    "u",
    "n",
    "t",
    "i",
    "m",
    "e",
    ".",
    ".",
    ".",
    "\"",
    ")",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    ")",
    "",
    "",
    "e",
    "l",
    "s",
    "e",
    ":",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    "#",
    " ",
    "M",
    "a",
    "r",
    "k",
    " ",
    "r",
    "u",
    "n",
    "t",
    "i",
    "m",
    "e",
    " ",
    "a",
    "s",
    " ",
    "u",
    "s",
    "e",
    "d",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    "w",
    "i",
    "t",
    "h",
    " ",
    "o",
    "p",
    "e",
    "n",
    "(",
    "R",
    "U",
    "N",
    "T",
    "I",
    "M",
    "E",
    "_",
    "M",
    "A",
    "R",
    "K",
    "E",
    "R",
    ",",
    " ",
    "'",
    "w",
    "'",
    ")",
    " ",
    "a",
    "s",
    " ",
    "f",
    ":",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    "f",
    ".",
    "w",
    "r",
    "i",
    "t",
    "e",
    "(",
    "\"",
    "u",
    "s",
    "e",
    "d",
    "\"",
    ")",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "\"",
    "\u2705",
    " ",
    "F",
    "r",
    "e",
    "s",
    "h",
    " ",
    "r",
    "u",
    "n",
    "t",
    "i",
    "m",
    "e",
    " ",
    "d",
    "e",
    "t",
    "e",
    "c",
    "t",
    "e",
    "d",
    "!",
    "\"",
    ")",
    "",
    "",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    ")",
    "",
    "",
    "",
    "",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "\"",
    "\ud83d\udccc",
    " ",
    "V",
    "e",
    "r",
    "s",
    "i",
    "o",
    "n",
    ":",
    " ",
    "v",
    "3",
    ".",
    "3",
    ".",
    "2",
    " ",
    "(",
    "2",
    "0",
    "2",
    "5",
    "-",
    "0",
    "1",
    "-",
    "1",
    "3",
    ")",
    "\"",
    ")",
    "",
    "",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "\"",
    "\ud83d\udccc",
    " ",
    "F",
    "i",
    "x",
    ":",
    " ",
    "P",
    "r",
    "o",
    "a",
    "c",
    "t",
    "i",
    "v",
    "e",
    " ",
    "n",
    "u",
    "m",
    "p",
    "y",
    " ",
    "r",
    "e",
    "p",
    "a",
    "i",
    "r",
    " ",
    "+",
    " ",
    "m",
    "i",
    "n",
    "i",
    "m",
    "a",
    "l",
    " ",
    "d",
    "e",
    "p",
    "e",
    "n",
    "d",
    "e",
    "n",
    "c",
    "i",
    "e",
    "s",
    "\"",
    ")",
    "",
    "",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    ")",
    "",
    "",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "\"",
    "=",
    "\"",
    " ",
    "*",
    " ",
    "7",
    "0",
    ")",
    "",
    "",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "\"",
    "\u2705",
    " ",
    "R",
    "u",
    "n",
    "t",
    "i",
    "m",
    "e",
    " ",
    "c",
    "h",
    "e",
    "c",
    "k",
    " ",
    "c",
    "o",
    "m",
    "p",
    "l",
    "e",
    "t",
    "e",
    "\"",
    ")",
    "",
    "",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "\"",
    "=",
    "\"",
    " ",
    "*",
    " ",
    "7",
    "0",
    ")",
    "",
    "",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    ")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================",
    "# DEPENDENCY VERIFICATION - v3.4.0 (Zero Installation Strategy)",
    "# ==============================================================================",
    "",
    "print(\"=\" * 70)",
    "print(\"\ud83d\udce6 DEPENDENCY VERIFICATION\")",
    "print(\"=\" * 70)",
    "print()",
    "print(\"Strategy: Use Colab pre-installed packages (no pip install)\")",
    "print(\"This prevents NumPy corruption caused by package reinstallation.\")",
    "print()",
    "",
    "# ==============================================================================",
    "# VERIFY CORE DEPENDENCIES (All pre-installed in Google Colab 2025)",
    "# ==============================================================================",
    "",
    "required = {",
    "    'torch': '2.6+',",
    "    'numpy': '2.3+',",
    "    'pandas': '1.5+',",
    "    'matplotlib': '3.7+',",
    "    'seaborn': '0.12+',",
    "}",
    "",
    "print(\"Checking pre-installed packages...\")",
    "print()",
    "",
    "all_good = True",
    "for package, min_version in required.items():",
    "    try:",
    "        module = __import__(package)",
    "        version = getattr(module, '__version__', 'unknown')",
    "        print(f\"  \u2705 {package:15s} {version:10s} (required: {min_version})\")",
    "    except ImportError:",
    "        print(f\"  \u274c {package:15s} NOT FOUND (should be pre-installed!)\")",
    "        all_good = False",
    "",
    "print()",
    "",
    "# ==============================================================================",
    "# NUMPY INTEGRITY CHECK",
    "# ==============================================================================",
    "",
    "print(\"Checking NumPy integrity...\")",
    "try:",
    "    from numpy._core.umath import _center",
    "    print(\"  \u2705 NumPy C extensions intact\")",
    "except ImportError as e:",
    "    print(\"  \u274c NumPy corrupted!\")",
    "    print()",
    "    print(\"=\" * 70)",
    "    print(\"ERROR: NumPy corruption detected\")",
    "    print(\"=\" * 70)",
    "    print()",
    "    print(\"This usually happens if you:\")",
    "    print(\"  1. Ran this notebook before without restarting runtime\")",
    "    print(\"  2. Manually installed packages that corrupted NumPy\")",
    "    print()",
    "    print(\"FIX: Runtime \u2192 Restart runtime, then run all cells again\")",
    "    print()",
    "    raise ImportError(\"NumPy corrupted - please restart runtime\") from e",
    "",
    "print()",
    "",
    "if not all_good:",
    "    print(\"=\" * 70)",
    "    print(\"ERROR: Missing required packages\")",
    "    print(\"=\" * 70)",
    "    print()",
    "    print(\"This shouldn't happen in Google Colab.\")",
    "    print(\"Are you running this notebook in a different environment?\")",
    "    print()",
    "    raise RuntimeError(\"Required packages not found\")",
    "",
    "print(\"=\" * 70)",
    "print(\"\u2705 ALL DEPENDENCIES VERIFIED\")",
    "print(\"=\" * 70)",
    "print()",
    "print(\"\u2705 No installation needed - using Colab pre-installed packages\")",
    "print(\"\u2705 NumPy corruption risk: ELIMINATED\")",
    "print()",
    "print(\"Note: Advanced features (Tier 2/3) will install packages on-demand\")",
    "print()",
    ""
   ]
  },
  {
   "cell_type": "code",
   "source": "# ==============================================================================\n# DOWNLOAD UTILS PACKAGE\n# ==============================================================================\n\nprint(\"\ud83d\udce6 Downloading test utilities package...\")\n\n# Remove old utils directory if exists\n!rm -rf utils/\n\n# Download complete utils package from GitHub\n!git clone --depth 1 --branch main https://github.com/matt-hans/transformer-builder-colab-templates.git temp_repo 2>/dev/null\n\n# Copy utils directory\n!cp -r temp_repo/utils ./\n\n# Cleanup\n!rm -rf temp_repo\n\n# Verify package structure\nimport sys\nimport os\n\n# Add current directory to Python path\nif './' not in sys.path:\n    sys.path.insert(0, './')\n\n# Verify utils package is importable\ntry:\n    import utils\n    print(f\"\u2705 Utils package loaded (version {utils.__version__})\")\n    \n    # Verify package structure\n    utils_path = os.path.join(os.getcwd(), 'utils')\n    subdirs = ['adapters', 'tokenization', 'training', 'ui']\n    \n    for subdir in subdirs:\n        subdir_path = os.path.join(utils_path, subdir)\n        if os.path.exists(subdir_path):\n            print(f\"\u2705 {subdir}/ directory found\")\n        else:\n            print(f\"\u26a0\ufe0f  {subdir}/ directory missing\")\n    \n    # Test importing test functions (backward compatibility)\n    from utils import (\n        test_shape_robustness,\n        test_gradient_flow,\n        test_output_stability,\n        run_all_tier1_tests\n    )\n    print(\"\u2705 Test functions importable\")\n    \n    print(\"\\n\u2705 Utils package ready!\")\n    \nexcept ImportError as e:\n    print(f\"\u274c Failed to import utils package: {e}\")\n    print(\"Falling back to direct file download...\")\n    # Fallback: download test_functions.py directly\n    !wget -q https://raw.githubusercontent.com/matt-hans/transformer-builder-colab-templates/main/utils/test_functions.py",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ==============================================================================\n# LOAD CUSTOM MODEL - v3.4.0 (Simple Modal Approach)\n# ==============================================================================\n\nimport os, re, json, urllib.request, urllib.error\n\nprint(\"=\" * 70)\nprint(\"MODEL LOADING\")\nprint(\"=\" * 70)\nprint()\n\n# ==============================================================================\n# VERIFY GIST ID WAS PROVIDED\n# ==============================================================================\n\nif 'GIST_ID' not in globals() or not GIST_ID:\n    print(\"\u274c ERROR: No Gist ID found!\")\n    print()\n    print(\"=\" * 70)\n    print(\"\ud83d\udd19 GO BACK TO CELL 3\")\n    print(\"=\" * 70)\n    print()\n    print(\"You must run Cell 3 first to provide your Gist ID.\")\n    print()\n    print(\"Steps:\")\n    print(\"  1. Scroll up to Cell 3\")\n    print(\"  2. Paste your Gist ID from Transformer Builder\")\n    print(\"  3. Run Cell 3\")\n    print(\"  4. Come back and run this cell\")\n    print()\n    raise ValueError(\"Gist ID required - please run Cell 3 first\")\n\ngist_id = GIST_ID\nmodel_name = \"Model\"  # Default name, will be overridden from config\n\nprint(f\"\ud83d\udce5 Loading model from GitHub Gist: {gist_id}\")\nprint()\n\n# ==============================================================================\n# FETCH GIST AND LOAD MODEL FILES\n# ==============================================================================\n\ndef _fetch_gist(gid: str) -> dict:\n    \"\"\"Fetch Gist data from GitHub API.\"\"\"\n    url = f\"https://api.github.com/gists/{gid}\"\n    req = urllib.request.Request(url, headers={\n        \"Accept\": \"application/vnd.github+json\",\n        \"User-Agent\": \"transformer-builder-colab\"\n    })\n    try:\n        with urllib.request.urlopen(req, timeout=20) as resp:\n            return json.loads(resp.read().decode(\"utf-8\"))\n    except urllib.error.HTTPError as e:\n        detail = f\"HTTP {e.code}\"\n        try:\n            body = e.read().decode(\"utf-8\")\n            if \"rate limit\" in body.lower():\n                detail += \" - GitHub API rate limit (try again in an hour)\"\n            elif e.code == 404:\n                detail += \" - Gist not found (check your Gist ID)\"\n        except:\n            pass\n        raise RuntimeError(f\"GitHub API error: {detail}\") from e\n    except Exception as e:\n        raise RuntimeError(f\"Network error: {e}\") from e\n\ndef _write(path: str, text: str):\n    \"\"\"Write text to file.\"\"\"\n    with open(path, \"w\") as f:\n        f.write(text)\n\n# Fetch Gist\ntry:\n    gist_data = _fetch_gist(gist_id)\n    files = gist_data.get(\"files\") or {}\n    \n    # Check for required files\n    if \"model.py\" not in files:\n        raise RuntimeError(\"Gist is missing 'model.py' - please re-export from Transformer Builder\")\n    if \"config.json\" not in files:\n        raise RuntimeError(\"Gist is missing 'config.json' - please re-export from Transformer Builder\")\n    \n    model_code = files[\"model.py\"].get(\"content\", \"\")\n    config_json = files[\"config.json\"].get(\"content\", \"\")\n    \n    if not model_code or not config_json:\n        raise RuntimeError(\"Empty content in model.py or config.json\")\n    \n    # Write to files\n    _write(\"custom_transformer.py\", model_code)\n    _write(\"config.json\", config_json)\n    \n    print(f\"\u2705 Model loaded successfully!\")\n    print(f\"\u2705 Gist URL: {gist_data.get('html_url', 'N/A')}\")\n    print(f\"\u2705 Model code: {len(model_code):,} bytes\")\n    print(f\"\u2705 Config: {len(config_json):,} bytes\")\n    print()\n    \n    # Parse model name from config if available\n    try:\n        config_dict = json.loads(config_json)\n        if 'model_name' in config_dict:\n            model_name = config_dict['model_name']\n            print(f\"\u2705 Model name: {model_name}\")\n            print()\n    except:\n        pass\n\nexcept Exception as e:\n    print(f\"\u274c Failed to load model from Gist!\")\n    print()\n    print(f\"Error: {e}\")\n    print()\n    print(\"=\" * 70)\n    print(\"TROUBLESHOOTING\")\n    print(\"=\" * 70)\n    print()\n    print(\"Common issues:\")\n    print(\"  1. Check your Gist ID is correct (go back to Cell 3)\")\n    print(\"  2. Ensure you exported from Transformer Builder successfully\")\n    print(\"  3. Check you're not hitting GitHub rate limit (60 requests/hour)\")\n    print(\"  4. Try re-exporting from Transformer Builder\")\n    print()\n    print(\"If the problem persists:\")\n    print(f\"  \u2022 Gist URL: https://gist.github.com/{gist_id}\")\n    print(\"  \u2022 Verify the Gist contains model.py and config.json\")\n    print()\n    raise\n\nprint(\"=\" * 70)\nprint(\"\u2705 MODEL LOADING COMPLETE\")\nprint(\"=\" * 70)\nprint()\nprint(\"Next: Continue to model instantiation and testing below!\")\nprint()\n\n# Store model_name for next cell\nparams = {\"name\": model_name}"
  },
  {
   "cell_type": "markdown",
   "source": "## \ud83d\udcc4 View Loaded Model Code\n\nThis cell displays the Python code that was loaded from your Transformer Builder export. You can review the architecture before running tests.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display the loaded model code for transparency\nprint(\"=\" * 80)\nprint(\"\ud83d\udcc4 LOADED MODEL CODE (custom_transformer.py)\")\nprint(\"=\" * 80)\nprint()\n\nwith open('custom_transformer.py', 'r') as f:\n    model_code_display = f.read()\n\n# Use syntax highlighting\nfrom IPython.display import Code\ndisplay(Code(model_code_display, language='python'))\n\nprint()\nprint(\"=\" * 80)\nprint(\"\ud83d\udccb MODEL CONFIGURATION (config.json)\")\nprint(\"=\" * 80)\nprint()\n\nwith open('config.json', 'r') as f:\n    config_display = json.load(f)\n\n# Pretty print JSON\nprint(json.dumps(config_display, indent=2))\nprint()\nprint(\"\u2705 You can now proceed to run the model instantiation and tests below!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Dependency Detection\n",
    "\n",
    "Automatically detect and install any custom dependencies your model needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Parse imports from generated code\n",
    "with open('custom_transformer.py', 'r') as f:\n",
    "    source_code = f.read()\n",
    "    tree = ast.parse(source_code)\n",
    "\n",
    "# Extract all imports\n",
    "imports = set()\n",
    "for node in ast.walk(tree):\n",
    "    if isinstance(node, ast.Import):\n",
    "        for alias in node.names:\n",
    "            imports.add(alias.name.split('.')[0])\n",
    "    elif isinstance(node, ast.ImportFrom):\n",
    "        if node.module:\n",
    "            imports.add(node.module.split('.')[0])\n",
    "\n",
    "print(f\"Detected imports: {', '.join(sorted(imports))}\")\n",
    "\n",
    "# Standard library modules (don't need pip install)\n",
    "stdlib_modules = {\n",
    "    'abc', 'collections', 'dataclasses', 'functools', 'json', 'math',\n",
    "    'typing', 'warnings', 'os', 'sys', 're', 'time', 'copy'\n",
    "}\n",
    "\n",
    "# Already installed\n",
    "installed_modules = {\n",
    "    'torch', 'transformers', 'numpy', 'scipy', 'matplotlib',\n",
    "    'pandas', 'seaborn', 'tqdm', 'torchinfo', 'captum', 'optuna'\n",
    "}\n",
    "\n",
    "# Find missing packages\n",
    "missing = imports - stdlib_modules - installed_modules\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\nInstalling additional dependencies: {', '.join(missing)}\")\n",
    "    for package in missing:\n",
    "        try:\n",
    "            subprocess.check_call(\n",
    "                [sys.executable, '-m', 'pip', 'install', '-q', package],\n",
    "                stdout=subprocess.DEVNULL,\n",
    "                stderr=subprocess.DEVNULL\n",
    "            )\n",
    "            print(f\"  \u2705 Installed {package}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"  \u26a0\ufe0f Failed to install {package} (may not be a pip package)\")\n",
    "else:\n",
    "    print(\"\\n\u2705 All dependencies already installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Instantiate Model\n",
    "\n",
    "Load your custom transformer and prepare for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch",
    "import torch.nn as nn",
    "import inspect",
    "",
    "# Import the custom model",
    "exec(open('custom_transformer.py').read())",
    "",
    "# Load config",
    "with open('config.json') as f:",
    "    config_dict = json.load(f)",
    "",
    "# Find the model class",
    "model_class = None",
    "for name, obj in list(globals().items()):",
    "    if isinstance(obj, type) and issubclass(obj, nn.Module) and obj is not nn.Module:",
    "        if name == params['name']:",
    "            model_class = obj",
    "            break",
    "",
    "if model_class is None:",
    "    # Fallback: find any nn.Module subclass",
    "    for name, obj in list(globals().items()):",
    "        if isinstance(obj, type) and issubclass(obj, nn.Module) and obj is not nn.Module:",
    "            model_class = obj",
    "            print(f\"\u26a0\ufe0f Using {name} (expected {params['name']})\")",
    "            break",
    "",
    "if model_class:",
    "    # Instantiate model - try both parameterless and parameterized approaches",
    "    try:",
    "        # Check if __init__ accepts parameters (besides self)",
    "        sig = inspect.signature(model_class.__init__)",
    "        params_list = [p for p in sig.parameters.values() if p.name != 'self']",
    "        ",
    "        if len(params_list) == 0:",
    "            # Parameterless constructor (Transformer Builder models)",
    "            print(\"\u2139\ufe0f  Model has parameterless constructor (Transformer Builder export)\")",
    "            model = model_class()",
    "        else:",
    "            # Parameterized constructor (traditional models)",
    "            print(f\"\u2139\ufe0f  Model accepts {len(params_list)} parameter(s)\")",
    "            model = model_class(**config_dict)",
    "        ",
    "        model.eval()",
    "        ",
    "        total_params = sum(p.numel() for p in model.parameters())",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)",
    "        ",
    "        print(f\"\u2705 Model instantiated: {model_class.__name__}\")",
    "        print(f\"\u2705 Total parameters: {total_params:,}\")",
    "        print(f\"\u2705 Trainable parameters: {trainable_params:,}\")",
    "        ",
    "        # Move to GPU if available",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
    "        model = model.to(device)",
    "        print(f\"\u2705 Device: {device}\")",
    "        ",
    "        # Display model summary (using native torch instead of torchinfo)",
    "        print()",
    "        print(\"=\" * 70)",
    "        print(\"MODEL SUMMARY\")",
    "        print(\"=\" * 70)",
    "        print()",
    "        print(model)",
    "        print()",
    "        print(\"=\" * 70)",
    "        print(f\"Total parameters:      {total_params:,}\")",
    "        print(f\"Trainable parameters:  {trainable_params:,}\")",
    "        print(f\"Non-trainable params:  {total_params - trainable_params:,}\")",
    "        ",
    "        # Calculate model size",
    "        param_size = sum(p.numel() * p.element_size() for p in model.parameters())",
    "        buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())",
    "        size_mb = (param_size + buffer_size) / 1024**2",
    "        print(f\"Model size:            {size_mb:.2f} MB\")",
    "        print(\"=\" * 70)",
    "        print()",
    "        ",
    "    except Exception as e:",
    "        print(f\"\u274c Failed to instantiate model: {e}\")",
    "        import traceback",
    "        traceback.print_exc()",
    "        raise",
    "else:",
    "    raise RuntimeError(f\"Could not find model class '{params['name']}' in generated code\")",
    "",
    "# Create config object for test functions (with proper vocab_size)",
    "class ModelConfig:",
    "    def __init__(self, **kwargs):",
    "        # Set defaults",
    "        self.vocab_size = 50257",
    "        self.max_seq_len = 512",
    "        self.max_batch_size = 8",
    "        ",
    "        # If nodes-based config, extract common params",
    "        if 'nodes' in kwargs:",
    "            for node in kwargs['nodes']:",
    "                node_params = node.get('params', {})",
    "                if 'vocab_size' in node_params:",
    "                    self.vocab_size = node_params['vocab_size']",
    "                if 'max_seq_len' in node_params or 'seq_length' in node_params:",
    "                    self.max_seq_len = node_params.get('max_seq_len') or node_params.get('seq_length', 512)",
    "        ",
    "        # Override with flat params if present",
    "        for key, value in kwargs.items():",
    "            if key not in ['nodes', 'version', 'model_name']:",
    "                setattr(self, key, value)",
    "",
    "config = ModelConfig(**config_dict)",
    "print(f\"\u2705 Config prepared (vocab_size={config.vocab_size}, max_seq_len={config.max_seq_len})\")",
    "print(\"\u2705 Ready for testing!\")",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# \ud83d\udd0d Tier 1: Critical Validation\n\nThese tests verify your model is mathematically sound and ready for training.\n\n**Estimated time:** ~1 minute\n\n**What's tested:**\n- \u2705 Shape validation across edge cases\n- \u2705 Gradient flow (detect vanishing/exploding gradients)\n- \u2705 Numerical stability (NaN/Inf detection)\n- \u2705 Parameter initialization quality\n- \u2705 Memory footprint scaling\n- \u2705 Inference speed benchmarks",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import test utilities from the cloned utils package\nfrom utils.test_functions import (\n    test_shape_robustness,\n    test_gradient_flow,\n    test_output_stability,\n    test_parameter_initialization,\n    test_memory_footprint,\n    test_inference_speed\n)\n\nprint(\"\u2705 Test functions loaded from utils package\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 80)\nprint(\"TIER 1: CRITICAL VALIDATION\")\nprint(\"=\" * 80)\nprint()\n\n# Test 1: Shape Robustness\nprint(\"Test 1/6: Shape Validation\")\nprint(\"-\" * 80)\nshape_results = test_shape_robustness(model, config)\ndisplay(shape_results)\nprint()\n\n# Test 2: Gradient Flow\nprint(\"Test 2/6: Gradient Flow Analysis\")\nprint(\"-\" * 80)\ngrad_results = test_gradient_flow(model, config)\ndisplay(grad_results)\nprint()\n\n# Test 3: Output Stability\nprint(\"Test 3/6: Numerical Stability\")\nprint(\"-\" * 80)\nstability_stats = test_output_stability(model, config, n_samples=100)\nprint()\n\n# Test 4: Parameter Initialization\nprint(\"Test 4/6: Parameter Initialization\")\nprint(\"-\" * 80)\nparam_results = test_parameter_initialization(model)\ndisplay(param_results)\nprint()\n\n# Test 5: Memory Footprint\nprint(\"Test 5/6: Memory Footprint Analysis\")\nprint(\"-\" * 80)\nmemory_results = test_memory_footprint(model, config)\ndisplay(memory_results)\nprint()\n\n# Test 6: Inference Speed\nprint(\"Test 6/6: Inference Speed Benchmark\")\nprint(\"-\" * 80)\nspeed_stats = test_inference_speed(model, config, n_trials=50)\nprint()\n\nprint(\"=\" * 80)\nprint(\"\u2705 TIER 1 VALIDATION COMPLETE\")\nprint(\"=\" * 80)\nprint()\nprint(\"All critical tests passed! Your model is mathematically sound.\")\nprint()\nprint(\"\ud83d\udcdd Next steps:\")\nprint(\"   \u2022 Tier 2: Advanced analysis (attention patterns, attribution)\")\nprint(\"     \u2192 Install optional dependencies in the cell before Tier 2\")\nprint(\"     \u2192 Then run Tier 2 tests\")\nprint()\nprint(\"   \u2022 Tier 3: Training utilities (fine-tuning, hyperparameter search)\")\nprint(\"     \u2192 Install optional dependencies in the cell before Tier 3\")\nprint(\"     \u2192 Then run Tier 3 tests\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# \ud83d\udd2c Tier 2: Advanced Analysis\n\nDeep dive into model behavior with advanced diagnostic tools.\n\n**Estimated time:** ~3-5 minutes\n\n**What's tested:**\n- \ud83c\udfaf **Attention Patterns:** Visualize attention weights, detect collapsed attention, analyze head specialization\n- \ud83d\udd0d **Attribution Analysis:** Identify which input tokens contribute most to predictions (using Captum)\n- \ud83d\udee1\ufe0f **Robustness Testing:** Measure stability under input perturbations and noise\n\n**Note:** These tests are optional but highly recommended for understanding model behavior.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ==============================================================================",
    "# TIER 2 OPTIONAL DEPENDENCIES - Lazy Installation",
    "# ==============================================================================",
    "",
    "print(\"=\" * 70)",
    "print(\"\ud83d\udce6 TIER 2: OPTIONAL PACKAGE INSTALLATION\")",
    "print(\"=\" * 70)",
    "print()",
    "print(\"\u26a0\ufe0f  WARNING: Installing packages may cause NumPy corruption\")",
    "print(\"\u26a0\ufe0f  If you encounter errors, restart runtime and skip this tier\")",
    "print()",
    "print(\"Tier 2 requires: captum (for feature attribution analysis)\")",
    "print()",
    "",
    "# Check if already installed",
    "try:",
    "    import captum",
    "    print(\"\u2705 Captum already available\")",
    "    print()",
    "except ImportError:",
    "    print(\"\ud83d\udce6 Installing captum...\")",
    "    print(\"\u23f3 This may take 10-15 seconds...\")",
    "    print()",
    "    ",
    "    # Install with --no-deps to reduce corruption risk",
    "    !pip install -q --no-deps captum",
    "    ",
    "    print()",
    "    ",
    "    # Verify installation",
    "    try:",
    "        import captum",
    "        print(\"\u2705 Captum installed successfully\")",
    "        print()",
    "        ",
    "        # Check numpy integrity after install",
    "        from numpy._core.umath import _center",
    "        print(\"\u2705 NumPy still intact after installation\")",
    "        print()",
    "        ",
    "    except ImportError as e:",
    "        print(\"\u274c Installation failed or NumPy corrupted\")",
    "        print()",
    "        print(\"=\" * 70)",
    "        print(\"RECOVERY STEPS:\")",
    "        print(\"=\" * 70)",
    "        print()",
    "        print(\"1. Runtime \u2192 Restart runtime\")",
    "        print(\"2. Run all cells EXCEPT this one\")",
    "        print(\"3. Skip to Tier 3 or just use Tier 1 results\")",
    "        print()",
    "        print(\"Note: Tier 1 tests are the most important - Tier 2 is optional\")",
    "        print()",
    "        raise",
    "",
    "print(\"=\" * 70)",
    "print(\"\u2705 Tier 2 dependencies ready\")",
    "print(\"=\" * 70)",
    "print()",
    "print(\"You can now run the Tier 2 tests below\")",
    "print()",
    ""
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## \ud83d\udce6 Optional: Install Tier 2 Dependencies\n\n**Note:** Tier 2 tests use lazy imports - they will skip gracefully if dependencies are missing.\n\n**To enable all Tier 2 functionality, run the cell below:**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ==============================================================================",
    "# TIER 3 OPTIONAL DEPENDENCIES - Lazy Installation",
    "# ==============================================================================",
    "",
    "print(\"=\" * 70)",
    "print(\"\ud83d\udce6 TIER 3: OPTIONAL PACKAGE INSTALLATION\")",
    "print(\"=\" * 70)",
    "print()",
    "print(\"\u26a0\ufe0f  WARNING: Installing packages may cause NumPy corruption\")",
    "print(\"\u26a0\ufe0f  If you encounter errors, restart runtime and skip this tier\")",
    "print()",
    "print(\"Tier 3 requires:\")",
    "print(\"  \u2022 pytorch-lightning (training utilities)\")",
    "print(\"  \u2022 optuna (hyperparameter optimization)\")",
    "print()",
    "",
    "# Check if already installed",
    "try:",
    "    import pytorch_lightning as pl",
    "    import optuna",
    "    print(\"\u2705 Training dependencies already available\")",
    "    print()",
    "except ImportError:",
    "    print(\"\ud83d\udce6 Installing training dependencies...\")",
    "    print(\"\u23f3 This may take 20-30 seconds...\")",
    "    print()",
    "    ",
    "    # Install with --no-deps to reduce corruption risk",
    "    !pip install -q --no-deps pytorch-lightning torchmetrics lightning-utilities",
    "    !pip install -q --no-deps optuna alembic colorlog sqlalchemy",
    "    ",
    "    print()",
    "    ",
    "    # Verify installation",
    "    try:",
    "        import pytorch_lightning as pl",
    "        import optuna",
    "        print(\"\u2705 Training dependencies installed successfully\")",
    "        print()",
    "        ",
    "        # Check numpy integrity after install",
    "        from numpy._core.umath import _center",
    "        print(\"\u2705 NumPy still intact after installation\")",
    "        print()",
    "        ",
    "    except ImportError as e:",
    "        print(\"\u274c Installation failed or NumPy corrupted\")",
    "        print()",
    "        print(\"=\" * 70)",
    "        print(\"RECOVERY STEPS:\")",
    "        print(\"=\" * 70)",
    "        print()",
    "        print(\"1. Runtime \u2192 Restart runtime\")",
    "        print(\"2. Run all cells EXCEPT this one\")",
    "        print(\"3. Use Tier 1/2 results only\")",
    "        print()",
    "        print(\"Note: Tier 3 is compute-intensive and optional\")",
    "        print()",
    "        raise",
    "",
    "print(\"=\" * 70)",
    "print(\"\u2705 Tier 3 dependencies ready\")",
    "print(\"=\" * 70)",
    "print()",
    "print(\"You can now run the Tier 3 tests below\")",
    "print()",
    ""
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## \ud83d\udce6 Optional: Install Tier 3 Dependencies\n\n**Note:** Tier 3 tests use lazy imports - they will skip gracefully if dependencies are missing.\n\n**To enable all Tier 3 functionality, run the cell below:**\n\n\u26a0\ufe0f **Warning:** These dependencies are more complex and may take longer to install.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import Tier 2 test functions\nfrom utils.test_functions import (\n    test_attention_patterns,\n    test_attribution_analysis,\n    test_robustness\n)\n\nprint(\"=\" * 80)\nprint(\"TIER 2: ADVANCED ANALYSIS\")\nprint(\"=\" * 80)\nprint()\n\n# Test 1: Attention Patterns\nprint(\"Test 1/3: Attention Pattern Analysis\")\nprint(\"-\" * 80)\ntry:\n    attention_results = test_attention_patterns(model, config)\n    if attention_results is not None:\n        display(attention_results)\n    print(\"\u2705 Attention analysis complete\")\nexcept Exception as e:\n    print(f\"\u26a0\ufe0f Attention analysis skipped: {e}\")\nprint()\n\n# Test 2: Attribution Analysis\nprint(\"Test 2/3: Input Attribution Analysis\")\nprint(\"-\" * 80)\ntry:\n    attribution_results = test_attribution_analysis(model, config)\n    if attribution_results is not None:\n        print(\"\\nTop Contributing Tokens:\")\n        for token, score in attribution_results.get(\"top_tokens\", []):\n            print(f\"  {token:20s}: {score:+.4f}\")\n    print(\"\u2705 Attribution analysis complete\")\nexcept Exception as e:\n    print(f\"\u26a0\ufe0f Attribution analysis skipped: {e}\")\nprint()\n\n# Test 3: Robustness Testing\nprint(\"Test 3/3: Robustness Under Noise\")\nprint(\"-\" * 80)\ntry:\n    robustness_results = test_robustness(model, config, n_samples=20)\n    if robustness_results is not None:\n        display(robustness_results)\n    print(\"\u2705 Robustness analysis complete\")\nexcept Exception as e:\n    print(f\"\u26a0\ufe0f Robustness analysis skipped: {e}\")\nprint()\n\nprint(\"=\" * 80)\nprint(\"\u2705 TIER 2 ANALYSIS COMPLETE\")\nprint(\"=\" * 80)\nprint()\nprint(\"Next: Scroll down for Tier 3 (Training & Fine-Tuning)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# \ud83d\ude80 Tier 3: Training & Production Utilities\n\nAdvanced utilities for fine-tuning, hyperparameter optimization, and production benchmarking.\n\n**Estimated time:** ~10-20 minutes (depends on training iterations)\n\n**What's included:**\n- \ud83c\udf93 **Fine-Tuning:** Basic training loop with loss tracking and gradient monitoring\n- \ud83d\udd27 **Hyperparameter Search:** Automated optimization using Optuna (learning rate, batch size, warmup)\n- \ud83d\udcca **Benchmark Comparison:** Compare your model against production baselines (distilgpt2, bert-base, etc.)\n\n**Note:** These are compute-intensive operations. Consider using GPU runtime for faster execution.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import Tier 3 training utilities\nfrom utils.test_functions import (\n    test_fine_tuning,\n    test_hyperparameter_search,\n    test_benchmark_comparison\n)\n\nprint(\"=\" * 80)\nprint(\"TIER 3: TRAINING & PRODUCTION UTILITIES\")\nprint(\"=\" * 80)\nprint()\n\n# Test 1: Fine-Tuning\nprint(\"Test 1/3: Fine-Tuning Demo\")\nprint(\"-\" * 80)\nprint(\"Running 3 epochs of fine-tuning with synthetic data...\")\ntry:\n    fine_tune_results = test_fine_tuning(\n        model, \n        config, \n        num_epochs=3,\n        batch_size=2,\n        learning_rate=5e-5\n    )\n    print(f\"\\nFinal Loss: {fine_tune_results['final_loss']:.4f}\")\n    print(f\"Best Loss: {fine_tune_results['best_loss']:.4f}\")\n    print(\"\u2705 Fine-tuning complete\")\nexcept Exception as e:\n    print(f\"\u26a0\ufe0f Fine-tuning skipped: {e}\")\nprint()\n\n# Test 2: Hyperparameter Search (OPTIONAL - Comment out to skip)\nprint(\"Test 2/3: Hyperparameter Optimization\")\nprint(\"-\" * 80)\nprint(\"\u26a0\ufe0f Skipping hyperparameter search (compute-intensive)\")\nprint(\"To enable: uncomment the code block below\")\nprint()\n# Uncomment to run:\n# try:\n#     hp_results = test_hyperparameter_search(\n#         model,\n#         config,\n#         n_trials=5,\n#         epochs_per_trial=2\n#     )\n#     print(\"\\nBest Parameters:\")\n#     for param, value in hp_results['best_params'].items():\n#         print(f\"  {param}: {value}\")\n#     print(\"\u2705 Hyperparameter search complete\")\n# except Exception as e:\n#     print(f\"\u26a0\ufe0f Hyperparameter search failed: {e}\")\n\n# Test 3: Benchmark Comparison\nprint(\"Test 3/3: Benchmark Against Baseline\")\nprint(\"-\" * 80)\nprint(\"Comparing against distilgpt2 baseline...\")\ntry:\n    benchmark_results = test_benchmark_comparison(\n        model,\n        config,\n        baseline_model=\"distilgpt2\",\n        n_samples=10\n    )\n    if benchmark_results is not None:\n        display(benchmark_results)\n    print(\"\u2705 Benchmark comparison complete\")\nexcept Exception as e:\n    print(f\"\u26a0\ufe0f Benchmark comparison skipped: {e}\")\nprint()\n\nprint(\"=\" * 80)\nprint(\"\u2705 TIER 3 TRAINING UTILITIES COMPLETE\")\nprint(\"=\" * 80)\nprint()\nprint(\"\ud83c\udf89 All testing tiers complete! Your model is production-ready.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}