---
id: T006
title: "W&B Model Comparison - Side-by-Side Run Analysis"
status: pending
priority: P3
dependencies: [T001, T002]
tags: [mlops, experiment-tracking, wandb, phase-1]
estimated_tokens: 6000
actual_tokens: 0
created_at: "2025-01-15"
updated_at: "2025-01-15"
---

## Description

Create utilities for comparing multiple W&B runs side-by-side. Enable users to analyze differences in hyperparameters, metrics, and model architectures across experiments.

## Business Context

**Problem**: Users train many variations (different architectures, datasets, hyperparameters) but struggle to identify what drove performance improvements. Manual comparison is tedious and error-prone.

**Value**: One-click comparison of any experiments. Quickly identify which hyperparameters or architectural choices led to better results.

**User Story**: As an ML researcher, I want to compare my 10 best runs side-by-side so that I can understand which configuration choices improved perplexity the most.

## Acceptance Criteria

1. [ ] Create Python function to compare N runs by IDs or tags
2. [ ] Generate comparison table showing key metrics (loss, perplexity, accuracy)
3. [ ] Highlight differences in hyperparameters across runs
4. [ ] Create comparison plots (loss curves overlaid, bar charts of final metrics)
5. [ ] Add model architecture diff (param count, layer counts)
6. [ ] Enable filtering runs by tag or date range
7. [ ] Export comparison report as markdown or HTML
8. [ ] Add example comparison cell in notebook
9. [ ] Document how to use W&B UI comparison features
10. [ ] Test with 5+ runs from different experiments

## Test Scenarios

### Scenario 1: Compare Top 3 Runs
```
Given: 20 completed runs, best 3 have val_perplexity of 15.3, 16.1, 16.8
When: compare_runs() called with top 3 run IDs
Then: Table shows side-by-side metrics
  And: Hyperparameter differences highlighted (e.g., LR: 5e-5 vs 1e-4 vs 3e-5)
  And: Loss curves overlaid on same chart
```

### Scenario 2: Architecture Comparison
```
Given: 3 runs with different model sizes (50M, 100M, 200M params)
When: Comparison includes architecture analysis
Then: Table shows param counts, layer counts, hidden sizes
  And: Performance vs model size chart displayed
  And: Efficiency metric (perplexity per 1M params) calculated
```

### Scenario 3: Filter by Tags
```
Given: 50 runs with various tags ("gpt", "bert", "baseline", "experimental")
When: compare_runs(tags=["gpt", "experimental"])
Then: Only matching runs included in comparison
  And: User can see just relevant experiments
```

## Technical Implementation

```python
# Run Comparison Utility

def compare_runs(run_ids=None, tags=None, project="transformer-builder-training", top_k=None):
    """
    Compare multiple W&B runs side-by-side.

    Args:
        run_ids: List of run IDs to compare (or None to use tags/top_k)
        tags: List of tags to filter runs (e.g., ["gpt", "best"])
        project: W&B project name
        top_k: Int, compare top K runs by val_loss (if run_ids and tags both None)

    Returns:
        comparison_df: pandas DataFrame with comparison data
    """
    import wandb
    api = wandb.Api()

    # Fetch runs
    if run_ids:
        runs = [api.run(f"{project}/{rid}") for rid in run_ids]
    elif tags:
        all_runs = api.runs(project, filters={"tags": {"$in": tags}})
        runs = list(all_runs)
    elif top_k:
        all_runs = api.runs(project, order="-summary_metrics.val/loss")
        runs = list(all_runs)[:top_k]
    else:
        raise ValueError("Must provide run_ids, tags, or top_k")

    # Extract comparison data
    comparison_data = []
    for run in runs:
        data = {
            'run_id': run.id,
            'run_name': run.name,
            'created_at': run.created_at,
            'state': run.state,

            # Metrics
            'final_train_loss': run.summary.get('train/loss'),
            'final_val_loss': run.summary.get('val/loss'),
            'best_val_loss': run.summary.get('best_val_loss'),
            'final_perplexity': run.summary.get('val/perplexity'),
            'final_accuracy': run.summary.get('val/accuracy_top1'),

            # Hyperparameters
            'learning_rate': run.config.get('learning_rate'),
            'batch_size': run.config.get('batch_size'),
            'epochs': run.config.get('epochs'),
            'warmup_ratio': run.config.get('warmup_ratio'),
            'weight_decay': run.config.get('weight_decay'),

            # Architecture
            'model_type': run.config.get('model_type'),
            'vocab_size': run.config.get('vocab_size'),
            'total_params_M': run.config.get('total_params_millions'),

            # Tags
            'tags': ', '.join(run.tags),
        }
        comparison_data.append(data)

    # Create DataFrame
    import pandas as pd
    comparison_df = pd.DataFrame(comparison_data)

    # Sort by best_val_loss
    comparison_df = comparison_df.sort_values('best_val_loss')

    return comparison_df


# Visualization Functions

def plot_run_comparison(comparison_df):
    """Create comparison visualizations."""
    import matplotlib.pyplot as plt

    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    # Plot 1: Final metrics comparison (bar chart)
    ax = axes[0]
    x = range(len(comparison_df))
    width = 0.35

    ax.bar([i - width/2 for i in x], comparison_df['final_train_loss'],
           width, label='Train Loss', alpha=0.7)
    ax.bar([i + width/2 for i in x], comparison_df['final_val_loss'],
           width, label='Val Loss', alpha=0.7)

    ax.set_xlabel('Run')
    ax.set_ylabel('Loss')
    ax.set_title('Final Loss Comparison')
    ax.set_xticks(x)
    ax.set_xticklabels(comparison_df['run_name'], rotation=45, ha='right')
    ax.legend()

    # Plot 2: Perplexity comparison
    ax = axes[1]
    ax.bar(x, comparison_df['final_perplexity'], color='skyblue')
    ax.set_xlabel('Run')
    ax.set_ylabel('Perplexity (lower is better)')
    ax.set_title('Final Perplexity Comparison')
    ax.set_xticks(x)
    ax.set_xticklabels(comparison_df['run_name'], rotation=45, ha='right')

    # Plot 3: Learning rate vs performance
    ax = axes[2]
    scatter = ax.scatter(
        comparison_df['learning_rate'],
        comparison_df['best_val_loss'],
        s=comparison_df['total_params_M'] * 10,  # Size by param count
        c=comparison_df['batch_size'],  # Color by batch size
        cmap='viridis',
        alpha=0.6
    )
    ax.set_xlabel('Learning Rate')
    ax.set_ylabel('Best Val Loss')
    ax.set_xscale('log')
    ax.set_title('LR vs Performance (size=params, color=batch_size)')
    plt.colorbar(scatter, ax=ax, label='Batch Size')

    plt.tight_layout()
    return fig


# Markdown Report Export

def export_comparison_report(comparison_df, output_path="run_comparison.md"):
    """Export comparison as markdown report."""

    with open(output_path, 'w') as f:
        f.write("# W&B Run Comparison Report\n\n")
        f.write(f"**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**Runs Compared**: {len(comparison_df)}\n\n")

        # Summary table
        f.write("## Summary\n\n")
        f.write(comparison_df.to_markdown(index=False))
        f.write("\n\n")

        # Best run
        best_run = comparison_df.iloc[0]
        f.write("## Best Run\n\n")
        f.write(f"**Run Name**: {best_run['run_name']}\n")
        f.write(f"**Run ID**: {best_run['run_id']}\n")
        f.write(f"**Best Val Loss**: {best_run['best_val_loss']:.4f}\n")
        f.write(f"**Final Perplexity**: {best_run['final_perplexity']:.2f}\n\n")

        f.write("### Hyperparameters\n")
        f.write(f"- Learning Rate: {best_run['learning_rate']}\n")
        f.write(f"- Batch Size: {best_run['batch_size']}\n")
        f.write(f"- Epochs: {best_run['epochs']}\n")
        f.write(f"- Warmup Ratio: {best_run['warmup_ratio']}\n")

    print(f"âœ… Comparison report saved to {output_path}")
```

## Dependencies

- **T001**: W&B Basic Integration (requires project with runs)
- **T002**: W&B Metrics Logging (requires metrics to compare)

## Design Decisions

**Decision 1**: Use W&B API for comparison, not UI exclusively
- **Rationale**: Programmatic access enables custom analysis, export, automation
- **Alternative considered**: Only use W&B UI comparison (less flexible)
- **Trade-off**: More code, but empowers power users

**Decision 2**: Export as markdown for portability
- **Rationale**: Markdown readable in GitHub, notebooks, docs sites
- **Alternative considered**: PDF export (requires extra dependencies)
- **Trade-off**: Less polished formatting, but universal compatibility

## Risks & Mitigations

### Risk 1: API Rate Limits
- **Impact**: LOW - Fetch might fail for large comparisons
- **Likelihood**: LOW - Free tier allows 100 runs/min
- **Mitigation**: Batch API calls, add retry logic, document limits

## Progress Log

- [ ] Created task specification (this file)
- [ ] Implemented compare_runs() utility
- [ ] Added visualization functions
- [ ] Created markdown export
- [ ] Tested with 5+ runs
- [ ] Committed changes with message: "feat(mlops): add W&B run comparison utilities"

## Completion Checklist

- [ ] All acceptance criteria met (10/10 checked)
- [ ] All test scenarios pass (3/3 verified)
- [ ] Code follows style guide
- [ ] Documentation complete
- [ ] No regressions
- [ ] Tested in Colab
- [ ] Git committed
