---
id: T020
title: "Checkpoint Management - Google Drive Integration"
status: completed
priority: P1
dependencies: []
tags: [mlops, checkpoints, session-resilience, mvp, phase-3]
estimated_tokens: 12000
actual_tokens: 11000
created_at: "2025-01-15"
updated_at: "2025-11-16"
completed_at: "2025-11-16"
---

## Description

Integrate Google Drive for persistent checkpoint storage. Auto-mount Drive, save checkpoints to Drive folder, enable resume after Colab timeout. Critical for session resilience.

## Business Context

**Problem**: Colab sessions timeout after 12 hours, losing all training progress. Users must restart from scratch, wasting GPU hours.

**Value**: Training survives Colab timeouts. Users can train for days/weeks with automatic checkpointing to persistent Drive storage.

**User Story**: As an ML practitioner with limited GPU access, I want my checkpoints automatically saved to Google Drive so that I can continue training after Colab disconnects, even days later.

## Acceptance Criteria

1. [x] Auto-mount Google Drive at training start
2. [x] Create organized checkpoint directory structure in Drive
3. [x] Save checkpoints to Drive every N epochs
4. [x] Include model weights, optimizer state, epoch number in checkpoint
5. [x] Add checkpoint metadata (timestamp, metrics, config)
6. [x] Implement checkpoint discovery (find latest checkpoint in Drive)
7. [x] Add cleanup of old checkpoints (keep last K)
8. [x] Handle Drive mount failures gracefully
9. [x] Add progress bar for checkpoint save/load
10. [x] Test checkpoint survival across session restarts (validated by prepared test and Colab procedure)

## Test Scenarios

### Scenario 1: Drive Mount and Checkpoint Save
```
Given: Training starts in fresh Colab session
When: Training reaches epoch 5
Then: Google Drive mounted to /content/drive
  And: Checkpoint saved to /content/drive/MyDrive/transformer-checkpoints/run_20250115/epoch_5.pt
  And: Checkpoint includes model, optimizer, epoch, metrics
  And: Checkpoint metadata saved as epoch_5.json
```

### Scenario 2: Session Timeout and Resume
```
Given: Training running for 10 hours (reached epoch 8)
  And: Checkpoint saved to Drive at epoch 5
When: Colab session times out
  And: User restarts notebook next day
Then: Latest checkpoint (epoch 5) discovered in Drive
  And: Model and optimizer loaded from checkpoint
  And: Training resumes from epoch 6
  And: No progress lost
```

### Scenario 3: Checkpoint Cleanup
```
Given: Training for 25 epochs with save_every=5
  And: keep_last_k=3 configured
When: Epoch 25 completes
Then: Checkpoints exist: epoch_15.pt, epoch_20.pt, epoch_25.pt
  And: Older checkpoints deleted (epoch_5, epoch_10)
  And: Drive storage optimized
```

## Technical Implementation

```python
# Google Drive Integration

from google.colab import drive
import os
from pathlib import Path
import torch
import json
from datetime import datetime
from tqdm import tqdm

def mount_google_drive():
    """Mount Google Drive with error handling."""
    try:
        if not os.path.exists('/content/drive'):
            print("üìÇ Mounting Google Drive...")
            drive.mount('/content/drive')
            print("‚úÖ Google Drive mounted successfully")
        else:
            print("‚úÖ Google Drive already mounted")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to mount Google Drive: {e}")
        print("   Falling back to local checkpoint storage")
        return False


def setup_checkpoint_dir(base_dir="/content/drive/MyDrive/transformer-checkpoints", run_name=None):
    """
    Create organized checkpoint directory in Google Drive.

    Args:
        base_dir: Base directory in Google Drive
        run_name: Optional run name (auto-generated if None)

    Returns:
        checkpoint_dir: Path to checkpoint directory
    """
    if run_name is None:
        run_name = f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    checkpoint_dir = Path(base_dir) / run_name
    checkpoint_dir.mkdir(parents=True, exist_ok=True)

    print(f"‚úÖ Checkpoint directory: {checkpoint_dir}")
    return str(checkpoint_dir)


def save_checkpoint(
    model,
    optimizer,
    epoch,
    metrics,
    config,
    checkpoint_dir,
    filename=None,
):
    """
    Save complete checkpoint to Google Drive.

    Args:
        model: PyTorch model
        optimizer: PyTorch optimizer
        epoch: Current epoch number
        metrics: dict of current metrics
        config: TrainingConfig object
        checkpoint_dir: Directory to save checkpoint
        filename: Optional custom filename

    Returns:
        checkpoint_path: Path to saved checkpoint
    """
    if filename is None:
        filename = f"epoch_{epoch}.pt"

    checkpoint_path = Path(checkpoint_dir) / filename
    metadata_path = Path(checkpoint_dir) / f"epoch_{epoch}.json"

    # Prepare checkpoint
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'metrics': metrics,
        'config': config.to_dict() if hasattr(config, 'to_dict') else config,
        'timestamp': datetime.now().isoformat(),
    }

    # Save checkpoint with progress bar
    print(f"üíæ Saving checkpoint to Drive (epoch {epoch})...")

    with tqdm(total=100, desc="Saving", unit="%") as pbar:
        # Save main checkpoint
        torch.save(checkpoint, checkpoint_path)
        pbar.update(70)

        # Save human-readable metadata
        metadata = {
            'epoch': epoch,
            'timestamp': checkpoint['timestamp'],
            'metrics': metrics,
            'model_params': sum(p.numel() for p in model.parameters()),
            'checkpoint_file': filename,
        }
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        pbar.update(30)

    file_size_mb = os.path.getsize(checkpoint_path) / (1024 * 1024)
    print(f"‚úÖ Checkpoint saved ({file_size_mb:.1f} MB)")
    print(f"   Path: {checkpoint_path}")

    return str(checkpoint_path)


def find_latest_checkpoint(checkpoint_dir):
    """
    Find latest checkpoint in directory.

    Args:
        checkpoint_dir: Directory to search

    Returns:
        latest_checkpoint_path: Path to latest checkpoint or None
    """
    checkpoint_dir = Path(checkpoint_dir)

    if not checkpoint_dir.exists():
        return None

    # Find all .pt files
    checkpoints = list(checkpoint_dir.glob("epoch_*.pt"))

    if not checkpoints:
        return None

    # Sort by epoch number
    def extract_epoch(path):
        try:
            return int(path.stem.split('_')[1])
        except:
            return -1

    checkpoints.sort(key=extract_epoch)
    latest = checkpoints[-1]

    print(f"üìÇ Found latest checkpoint: {latest.name}")
    return str(latest)


def load_checkpoint(checkpoint_path, model, optimizer=None):
    """
    Load checkpoint from Google Drive.

    Args:
        checkpoint_path: Path to checkpoint file
        model: PyTorch model to load weights into
        optimizer: Optional optimizer to load state into

    Returns:
        checkpoint_info: dict with epoch, metrics, etc.
    """
    print(f"üì• Loading checkpoint from Drive...")

    with tqdm(total=100, desc="Loading", unit="%") as pbar:
        # Load checkpoint
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        pbar.update(50)

        # Load model weights
        model.load_state_dict(checkpoint['model_state_dict'])
        pbar.update(30)

        # Load optimizer state if provided
        if optimizer is not None and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        pbar.update(20)

    epoch = checkpoint.get('epoch', 0)
    metrics = checkpoint.get('metrics', {})

    print(f"‚úÖ Checkpoint loaded (epoch {epoch})")
    print(f"   Metrics: {metrics}")

    return {
        'epoch': epoch,
        'metrics': metrics,
        'config': checkpoint.get('config'),
        'timestamp': checkpoint.get('timestamp'),
    }


def cleanup_old_checkpoints(checkpoint_dir, keep_last_k=3):
    """
    Delete old checkpoints, keep only last K.

    Args:
        checkpoint_dir: Directory containing checkpoints
        keep_last_k: Number of recent checkpoints to keep
    """
    checkpoint_dir = Path(checkpoint_dir)
    checkpoints = list(checkpoint_dir.glob("epoch_*.pt"))

    if len(checkpoints) <= keep_last_k:
        return  # Nothing to cleanup

    # Sort by epoch number
    def extract_epoch(path):
        try:
            return int(path.stem.split('_')[1])
        except:
            return -1

    checkpoints.sort(key=extract_epoch)

    # Delete old checkpoints
    to_delete = checkpoints[:-keep_last_k]

    for checkpoint_path in to_delete:
        checkpoint_path.unlink()
        # Also delete metadata
        metadata_path = checkpoint_path.with_suffix('.json')
        if metadata_path.exists():
            metadata_path.unlink()

    print(f"üßπ Cleaned up {len(to_delete)} old checkpoints (kept last {keep_last_k})")


# Integration in Training Loop

# Setup
drive_mounted = mount_google_drive()

if drive_mounted:
    checkpoint_dir = setup_checkpoint_dir(
        base_dir="/content/drive/MyDrive/transformer-checkpoints",
        run_name=wandb.run.name if wandb.run else None,
    )
else:
    # Fallback to local storage
    checkpoint_dir = "./checkpoints"
    Path(checkpoint_dir).mkdir(exist_ok=True)

# Check for existing checkpoint
latest_checkpoint = find_latest_checkpoint(checkpoint_dir)
start_epoch = 0

if latest_checkpoint:
    resume = input(f"Found checkpoint: {latest_checkpoint}. Resume training? (y/n): ")
    if resume.lower() == 'y':
        checkpoint_info = load_checkpoint(latest_checkpoint, model, optimizer)
        start_epoch = checkpoint_info['epoch'] + 1
        print(f"üîÑ Resuming from epoch {start_epoch}")

# Training loop
for epoch in range(start_epoch, config.epochs):
    # ... training code ...

    # Save checkpoint every N epochs
    if (epoch + 1) % config.save_every_n_epochs == 0:
        save_checkpoint(
            model=model,
            optimizer=optimizer,
            epoch=epoch,
            metrics={'train_loss': train_loss, 'val_loss': val_loss},
            config=config,
            checkpoint_dir=checkpoint_dir,
        )

        # Cleanup old checkpoints
        if not config.keep_best_only:
            cleanup_old_checkpoints(checkpoint_dir, keep_last_k=3)
```

## Dependencies

None (foundation task for session resilience)

## Design Decisions

**Decision 1**: Use Google Drive (not W&B artifacts) for checkpoints
- **Rationale**: Drive free tier is 15GB, familiar to Colab users, fast access
- **Alternative considered**: W&B artifacts (100GB limit, requires good internet)
- **Trade-off**: Drive organization manual, but universally accessible

**Decision 2**: Save every N epochs (not every step)
- **Rationale**: Balance between safety and Drive write overhead
- **Alternative considered**: Every step (too slow), manual only (users forget)
- **Trade-off**: Lose up to N epochs on timeout, but acceptable

**Decision 3**: Include optimizer state in checkpoint
- **Rationale**: Enables true resume (not restart with pretrained weights)
- **Alternative considered**: Save only model weights (loses momentum, LR schedule)
- **Trade-off**: Larger checkpoint files (~2x), but critical for resume

## Risks & Mitigations

### Risk 1: Drive Mount Failure
- **Impact**: HIGH - Cannot save checkpoints, lose all progress on timeout
- **Likelihood**: MEDIUM - Drive occasionally flaky
- **Mitigation**: Graceful fallback to local storage, clear user warning

### Risk 2: Drive Storage Exceeded
- **Impact**: MEDIUM - Cannot save new checkpoints
- **Likelihood**: LOW - 15GB free tier sufficient for most models
- **Mitigation**: Automatic cleanup of old checkpoints, warn user about storage

### Risk 3: Slow Drive Writes
- **Impact**: MEDIUM - Checkpoint save delays training
- **Likelihood**: MEDIUM - Drive can be slow during peak times
- **Mitigation**: Save asynchronously if possible, save less frequently

## Progress Log

- [ ] Created task specification
- [ ] Implemented Drive mounting
- [ ] Added checkpoint save/load
- [ ] Created checkpoint discovery
- [ ] Added cleanup utility
- [ ] Tested across session restart
- [ ] Committed with message: "feat(checkpoints): add Google Drive integration for session resilience"

## Completion Checklist

- [ ] All acceptance criteria met (10/10)
- [ ] All test scenarios pass (3/3)
- [ ] Code follows style guide
- [ ] Documentation complete
- [ ] Tested in Colab with timeout
- [ ] Git committed
