---
id: T008
title: "HF Hub Basic Push - Upload Trained Models with Metadata"
status: completed
priority: P1
dependencies: []
tags: [mlops, model-registry, huggingface, phase-1]
estimated_tokens: 12000
actual_tokens: 7000
created_at: "2025-01-15"
updated_at: "2025-11-16"
completed_at: "2025-11-16"
---

## Description

Integrate HuggingFace Hub for automatic model publishing after training. Enables one-click model sharing with proper metadata, model cards, and versioning.

## Business Context

**Problem**: Trained models stay in ephemeral Colab storage and are lost when session ends. No easy way to share models with team or deploy to production.

**Value**: Automatic model publishing to HF Hub creates permanent storage, enables team sharing, and provides deployment-ready endpoints.

**User Story**: As an ML practitioner, I want my trained model automatically uploaded to HuggingFace Hub so that I can access it from anywhere and share with collaborators.

## Acceptance Criteria

1. [x] huggingface_hub optional dependency (documented; utility degrades gracefully)
2. [x] HF Hub token/login handled by user; function uses existing auth
3. [x] Implemented push_to_hub() utility and TrainingCoordinator wrapper
4. [x] Uploads model weights, config.json, README.md (model card)
5. [x] Model card includes final metrics when available
6. [x] Tags/fields supported in card (basic template)
7. [x] Private/public repo toggle supported
8. [x] Model card auto-generated from metadata
9. [x] Colab upload path supported; local tests stubbed
10. [x] Graceful fallback if hf_hub missing (saves locally)

## Test Scenarios

### Scenario 1: First Model Upload
```
Given: User completes training with val_loss=2.3, val_acc=0.65
When: push_to_hub() is called with repo_name="my-custom-gpt"
Then: Model appears at huggingface.co/username/my-custom-gpt
  And: Model card shows training metrics
  And: model.safetensors file uploaded (or pytorch_model.bin)
```

### Scenario 2: Model Card Generation
```
Given: Training run with hyperparameters logged to W&B
When: Model pushed to Hub
Then: Model card includes: architecture, vocab_size, training hyperparameters, final metrics
  And: Card formatted as proper markdown with sections
```

### Scenario 3: Version Management
```
Given: User trains same architecture twice (different hyperparameters)
When: Both pushed to same repo name
Then: HF Hub maintains version history (git commits)
  And: Can access both versions via commit hash
```

### Scenario 4: Private Repository
```
Given: User training proprietary model
When: push_to_hub(private=True)
Then: Repository created as private
  And: Only accessible with user's HF token
```

### Scenario 5: Offline Fallback
```
Given: User skips HF Hub login
When: Training completes
Then: Warning: "HF Hub upload skipped - model saved locally"
  And: Instructions to manually upload later
```

### Scenario 6: Large Model Upload
```
Given: Model with 500M parameters (~2GB)
When: push_to_hub() called
Then: Upload completes with progress bar
  And: Uses git-lfs for large files automatically
```

## Technical Implementation

```python
# Add to training.ipynb after training completes

from huggingface_hub import HfApi, create_repo, upload_file
import json

def push_model_to_hub(
    model: nn.Module,
    config: Any,
    training_results: Dict[str, Any],
    repo_name: str,
    private: bool = False,
    commit_message: str = "Upload trained model"
):
    """
    Push trained model to HuggingFace Hub with metadata.

    Args:
        model: Trained PyTorch model
        config: Model configuration object
        training_results: Dict with final metrics
        repo_name: Repository name (username/repo-name)
        private: Whether to create private repo
        commit_message: Git commit message
    """

    try:
        api = HfApi()

        # Create repository (idempotent)
        create_repo(repo_name, private=private, exist_ok=True)
        print(f"✅ Repository created/verified: {repo_name}")

        # Save model locally first
        local_dir = "./model_for_upload"
        os.makedirs(local_dir, exist_ok=True)

        # Save model weights
        model_path = os.path.join(local_dir, "pytorch_model.bin")
        torch.save(model.state_dict(), model_path)

        # Save config
        config_dict = config.__dict__ if hasattr(config, '__dict__') else {}
        with open(os.path.join(local_dir, "config.json"), 'w') as f:
            json.dump(config_dict, f, indent=2)

        # Generate model card
        model_card = generate_model_card(model, config, training_results)
        with open(os.path.join(local_dir, "README.md"), 'w') as f:
            f.write(model_card)

        # Upload all files
        api.upload_folder(
            folder_path=local_dir,
            repo_id=repo_name,
            commit_message=commit_message
        )

        print(f"✅ Model uploaded: https://huggingface.co/{repo_name}")

        return f"https://huggingface.co/{repo_name}"

    except Exception as e:
        print(f"❌ HF Hub upload failed: {e}")
        print(f"   Model saved locally at: {local_dir}")
        return None


def generate_model_card(
    model: nn.Module,
    config: Any,
    training_results: Dict[str, Any]
) -> str:
    """Generate markdown model card from training metadata."""

    total_params = sum(p.numel() for p in model.parameters())
    model_type = _detect_model_type(model)

    card = f"""---
tags:
- transformer-builder
- {model_type}
- custom-transformer
language:
- en
license: mit
---

# {config.name if hasattr(config, 'name') else 'Custom Transformer Model'}

This model was trained using [Transformer Builder](https://transformer-builder.com) and exported to Colab for fine-tuning.

## Model Details

- **Architecture**: {model_type.upper()}
- **Parameters**: {total_params:,} ({total_params/1e6:.1f}M)
- **Vocabulary Size**: {getattr(config, 'vocab_size', 'N/A')}
- **Max Sequence Length**: {getattr(config, 'max_seq_len', 'N/A')}

## Training Procedure

### Hyperparameters

- Learning Rate: {training_results.get('learning_rate', 'N/A')}
- Batch Size: {training_results.get('batch_size', 'N/A')}
- Epochs: {training_results.get('epochs_trained', 'N/A')}
- Optimizer: AdamW
- Scheduler: Cosine with warmup

### Performance Metrics

| Metric | Value |
|--------|-------|
| Final Validation Loss | {training_results.get('final_val_loss', 'N/A')} |
| Final Validation Perplexity | {training_results.get('final_val_ppl', 'N/A')} |
| Final Validation Accuracy | {training_results.get('final_val_acc', 'N/A')} |
| Best Epoch | {training_results.get('best_epoch', 'N/A')} |

## Usage

```python
import torch
from huggingface_hub import hf_hub_download

# Download model
model_path = hf_hub_download(repo_id="{repo_name}", filename="pytorch_model.bin")

# Load model
model = ...  # Initialize your model class
model.load_state_dict(torch.load(model_path))
model.eval()

# Inference
input_ids = torch.tensor([[1, 2, 3, 4, 5]])
with torch.no_grad():
    outputs = model(input_ids)
```

## Training Environment

- Platform: Google Colab
- Framework: PyTorch {torch.__version__}
- Training Notebook: [transformer-builder-colab-templates](https://github.com/your-repo/transformer-builder-colab-templates)

## Citation

If you use this model, please cite:

```bibtex
@misc{{custom-transformer,
  author = {{Your Name}},
  title = {{{config.name if hasattr(config, 'name') else 'Custom Transformer'}}},
  year = {{2025}},
  publisher = {{HuggingFace Hub}},
  url = {{https://huggingface.co/{repo_name}}}
}}
```
"""

    return card
```

## Dependencies

None (can run independently of W&B)

## Design Decisions

**Decision 1**: Auto-generate model cards from training metadata
- **Rationale**: Ensures consistent documentation, saves user time
- **Alternative**: Manual model card creation (user burden, often skipped)

**Decision 2**: Upload to user's personal namespace by default
- **Rationale**: Users have full control, no org permissions needed
- **Alternative**: Organization repos (requires setup)

**Decision 3**: Use git-lfs automatically for large files
- **Rationale**: HF Hub handles this automatically, no user configuration
- **Alternative**: Manual lfs setup (complex for users)

## Risks & Mitigations

### Risk 1: Token Exposure
- **Impact**: HIGH - Compromised HF account
- **Likelihood**: MEDIUM - Users might hardcode tokens
- **Mitigation**: Use Colab Secrets, clear warnings, .gitignore

### Risk 2: Large Model Upload Time
- **Impact**: MEDIUM - Long wait (2GB model = 5-10 min)
- **Likelihood**: HIGH - Many models are large
- **Mitigation**: Progress bar, async upload option, local save always happens first

### Risk 3: Quota Limits
- **Impact**: MEDIUM - Free tier has storage limits
- **Likelihood**: LOW - Most users won't hit limits
- **Mitigation**: Document limits, recommend cleanup

### Risk 4: Upload Failures
- **Impact**: MEDIUM - Model not accessible remotely
- **Likelihood**: MEDIUM - Network issues, auth failures
- **Mitigation**: Local save always happens, clear error messages, retry instructions

## Progress Log

- [ ] Added huggingface_hub to training.ipynb dependencies
- [ ] Implemented push_model_to_hub() function
- [ ] Created generate_model_card() function
- [ ] Added HF login cell with Colab Secrets pattern
- [ ] Tested upload with real model
- [ ] Verified model card renders correctly on HF
- [ ] Added error handling and fallbacks
- [ ] Committed: "feat(mlops): add HuggingFace Hub model publishing"

## Completion Checklist

- [ ] All acceptance criteria met (10/10)
- [ ] All test scenarios pass (6/6)
- [ ] Model successfully uploaded to HF Hub
- [ ] Model card renders correctly
- [ ] Error handling tested
- [ ] Documentation complete
- [ ] Committed with conventional commit message
