---
id: T037
title: "Training Loop Improvements - Gradient Accumulation Support"
status: pending
priority: P2
dependencies: []
tags: [ml-training, training-loop, memory, mvp, phase-2]
estimated_tokens: 8000
actual_tokens: 0
created_at: "2025-01-15"
updated_at: "2025-01-15"
---

## Description

Implement gradient accumulation to simulate larger batch sizes. Accumulate gradients over N steps before optimizer update. Enables large effective batch sizes on limited GPU memory.

## Business Context

**Problem**: GPU memory limits batch size to 4. Larger batches (32+) improve training stability but don't fit.

**Value**: Simulate batch_size=32 with 4GB GPU by accumulating 8 steps. Better training dynamics without expensive hardware.

**User Story**: As an ML practitioner with limited GPU memory, I want gradient accumulation so that I can use large effective batch sizes for stable training.

## Acceptance Criteria

1. [ ] Add gradient_accumulation_steps to config
2. [ ] Accumulate gradients over N steps before optimizer update
3. [ ] Scale loss by accumulation steps
4. [ ] Update learning rate scheduler correctly
5. [ ] Log effective batch size to W&B
6. [ ] Test: effective_batch_size = batch_size * accum_steps
7. [ ] Verify training stability improves with larger effective batch

## Technical Implementation

```python
# Config
gradient_accumulation_steps = 8  # Effective batch = 4 * 8 = 32

optimizer.zero_grad()

for step, batch in enumerate(train_loader):
    input_ids = batch['input_ids'].to(device)
    labels = batch['labels'].to(device)
    
    # Forward pass
    outputs = model(input_ids)
    loss = F.cross_entropy(outputs.view(-1, vocab_size), labels.view(-1))
    
    # Scale loss by accumulation steps
    loss = loss / gradient_accumulation_steps
    loss.backward()
    
    # Update every N steps
    if (step + 1) % gradient_accumulation_steps == 0:
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()
        optimizer.zero_grad()
        
        if lr_scheduler:
            lr_scheduler.step()
```

## Progress Log

- [ ] Implemented gradient accumulation
- [ ] Tested effective batch size
- [ ] Committed with message: "feat(training): add gradient accumulation for larger effective batches"
