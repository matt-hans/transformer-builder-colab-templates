---
id: T003
title: "W&B Visualization - Custom Charts and Dashboards"
status: pending
priority: P2
dependencies: [T002]
tags: [mlops, experiment-tracking, wandb, phase-1]
estimated_tokens: 8000
actual_tokens: 0
created_at: "2025-01-15"
updated_at: "2025-01-15"
---

## Description

Create custom W&B visualizations including loss curves, learning rate schedules, gradient norm tracking, and perplexity plots. Build reusable dashboard templates for transformer training analysis.

## Business Context

**Problem**: Default W&B scalar plots are basic. Users need specialized visualizations for transformer training patterns like attention weight distributions, layer-wise gradient flow, and token-level perplexity.

**Value**: Professional-grade visualizations that surface insights faster. Easier to diagnose training issues like vanishing gradients, learning rate problems, or overfitting.

**User Story**: As an ML researcher, I want custom W&B dashboards for transformer metrics so that I can quickly identify training problems and compare architectural variations.

## Acceptance Criteria

1. [ ] Log learning rate per step (not just per epoch)
2. [ ] Log gradient norms (total, per-layer if available)
3. [ ] Create custom W&B panel for loss curves (train + validation)
4. [ ] Add perplexity visualization (log scale recommended)
5. [ ] Log token prediction accuracy (top-1, top-5)
6. [ ] Create custom panel comparing multiple runs side-by-side
7. [ ] Add attention weight distribution histograms (if model exposes attn weights)
8. [ ] Save dashboard template to .wandb/ for reuse
9. [ ] Document how to create custom panels in notebook markdown
10. [ ] Test visualizations with 3+ training runs

## Test Scenarios

### Scenario 1: Learning Rate Schedule Visualization
```
Given: Training with warmup schedule (0 → 5e-5 over 500 steps, then cosine decay)
When: Dashboard viewed during training
Then: LR curve shows warmup ramp, then smooth decay
  And: Current LR value annotated on chart
```

### Scenario 2: Gradient Norm Tracking
```
Given: Training with gradient clipping at max_norm=1.0
When: Model experiences gradient explosion (norms >10)
Then: Chart shows spike in gradient norms
  And: Clipping indicator shows when clipping activated
  And: User can correlate with loss spikes
```

### Scenario 3: Multi-Run Comparison
```
Given: 3 runs with different learning rates (1e-5, 5e-5, 1e-4)
When: Custom comparison dashboard opened
Then: All 3 loss curves overlaid on same chart
  And: Legend shows LR for each run
  And: Final perplexity values annotated
```

### Scenario 4: Attention Weight Distribution
```
Given: GPT model with 6 attention heads
When: Training logged attention weights (sample every 100 steps)
Then: Histogram panel shows weight distributions per head
  And: User can identify dead heads (low variance)
  And: Heatmap shows head activation patterns over time
```

## Technical Implementation

### File Changes

**training.ipynb**:
- Modify training loop to log additional metrics
- Add custom W&B chart definitions
- Create dashboard configuration cell

### Code Structure

```python
# Enhanced Metrics Logging in Training Loop

def log_training_metrics(step, epoch, loss, model, optimizer, scaler=None):
    """Log comprehensive training metrics to W&B."""

    metrics = {
        "train/loss": loss.item(),
        "train/perplexity": torch.exp(loss).item(),
        "train/epoch": epoch,
        "train/step": step,
    }

    # Learning rate (handle multiple param groups)
    for i, param_group in enumerate(optimizer.param_groups):
        lr_key = f"train/lr_group_{i}" if len(optimizer.param_groups) > 1 else "train/lr"
        metrics[lr_key] = param_group['lr']

    # Gradient norms
    total_grad_norm = 0.0
    for p in model.parameters():
        if p.grad is not None:
            param_norm = p.grad.data.norm(2)
            total_grad_norm += param_norm.item() ** 2
    total_grad_norm = total_grad_norm ** 0.5
    metrics["train/grad_norm"] = total_grad_norm

    # Layer-wise gradient norms (optional, for debugging)
    if step % 100 == 0:  # Log less frequently to reduce overhead
        for name, param in model.named_parameters():
            if param.grad is not None:
                layer_grad_norm = param.grad.data.norm(2).item()
                metrics[f"gradients/{name}"] = layer_grad_norm

    # Mixed precision scale factor (if using AMP)
    if scaler is not None:
        metrics["train/loss_scale"] = scaler.get_scale()

    wandb.log(metrics, step=step)


# Custom Dashboard Creation

def create_wandb_dashboard():
    """Define custom W&B dashboard layout for transformer training."""

    # Custom panels configuration
    panels = [
        # Loss Curves Panel
        {
            "title": "Training & Validation Loss",
            "type": "line",
            "metrics": ["train/loss", "val/loss"],
            "smoothing": 0.6,
            "log_scale": False,
        },

        # Perplexity Panel (log scale recommended)
        {
            "title": "Perplexity (Lower is Better)",
            "type": "line",
            "metrics": ["train/perplexity", "val/perplexity"],
            "smoothing": 0.6,
            "log_scale": True,
        },

        # Learning Rate Schedule
        {
            "title": "Learning Rate Schedule",
            "type": "line",
            "metrics": ["train/lr"],
            "smoothing": 0.0,  # No smoothing for precise LR tracking
            "log_scale": False,
        },

        # Gradient Monitoring
        {
            "title": "Gradient Norm (Clipping at 1.0)",
            "type": "line",
            "metrics": ["train/grad_norm"],
            "smoothing": 0.3,
            "log_scale": False,
            "thresholds": [{"value": 1.0, "label": "Clip Threshold"}],
        },

        # Accuracy Metrics
        {
            "title": "Token Prediction Accuracy",
            "type": "line",
            "metrics": ["train/accuracy_top1", "val/accuracy_top1", "val/accuracy_top5"],
            "smoothing": 0.6,
            "log_scale": False,
        },
    ]

    # Save dashboard config
    with open(".wandb/dashboard_template.json", "w") as f:
        json.dump(panels, f, indent=2)

    print("✅ Custom dashboard template saved to .wandb/dashboard_template.json")
    print("   Load in W&B UI: Settings → Import Dashboard")

    return panels


# Attention Weight Logging (optional, model-specific)

def log_attention_patterns(model, step, sample_input_ids):
    """
    Log attention weight distributions if model exposes them.

    Args:
        model: Transformer model
        step: Current training step
        sample_input_ids: (batch_size, seq_len) tensor for sampling
    """
    if not hasattr(model, 'get_attention_weights'):
        return  # Skip if model doesn't expose attention

    with torch.no_grad():
        # Get attention weights: (batch, num_heads, seq_len, seq_len)
        attn_weights = model.get_attention_weights(sample_input_ids)

        if attn_weights is not None:
            # Log per-head statistics
            for head_idx in range(attn_weights.size(1)):
                head_weights = attn_weights[0, head_idx]  # First batch item

                wandb.log({
                    f"attention/head_{head_idx}_mean": head_weights.mean().item(),
                    f"attention/head_{head_idx}_std": head_weights.std().item(),
                    f"attention/head_{head_idx}_max": head_weights.max().item(),
                }, step=step)

            # Log attention heatmap (sample)
            if step % 500 == 0:
                import matplotlib.pyplot as plt
                fig, axes = plt.subplots(2, 3, figsize=(15, 10))
                for idx, ax in enumerate(axes.flat):
                    if idx < attn_weights.size(1):
                        ax.imshow(attn_weights[0, idx].cpu().numpy(), cmap='viridis')
                        ax.set_title(f'Head {idx}')
                        ax.axis('off')

                wandb.log({"attention/heatmap": wandb.Image(fig)}, step=step)
                plt.close(fig)
```

## Dependencies

- **T002**: W&B Metrics Logging (requires basic wandb.log() setup)

## Design Decisions

**Decision 1**: Log gradient norms per-step, layer-wise less frequently
- **Rationale**: Balance insight with logging overhead
- **Alternative considered**: Log everything every step (too expensive)
- **Trade-off**: Miss some granular layer details, but acceptable for diagnostics

**Decision 2**: Use log scale for perplexity charts
- **Rationale**: Perplexity ranges can be very wide (10 to 1000+), log scale shows trends better
- **Alternative considered**: Linear scale (hard to see improvements at low perplexity)
- **Trade-off**: Slightly less intuitive, but standard in NLP

**Decision 3**: Save dashboard templates as JSON files
- **Rationale**: Reusable across projects, version controllable
- **Alternative considered**: Manual dashboard creation each time (tedious)
- **Trade-off**: Requires one-time import step, but much faster after

## Risks & Mitigations

### Risk 1: Logging Overhead
- **Impact**: MEDIUM - Too many metrics slow down training
- **Likelihood**: MEDIUM - Easy to over-log
- **Mitigation**: Log expensive metrics (attention, layer gradients) infrequently (every 100-500 steps)

### Risk 2: Dashboard Complexity
- **Impact**: LOW - Too many panels overwhelming
- **Likelihood**: LOW - We provide sensible defaults
- **Mitigation**: Start with 5 core panels, document how to add custom ones

### Risk 3: Attention Weight Extraction Model-Specific
- **Impact**: LOW - Not all models expose attention
- **Likelihood**: HIGH - Custom architectures vary
- **Mitigation**: Make attention logging optional, check hasattr() before accessing

## Progress Log

- [ ] Created task specification (this file)
- [ ] Enhanced metrics logging in training loop
- [ ] Implemented custom dashboard creation
- [ ] Added attention weight visualization (optional)
- [ ] Tested with 3+ runs in W&B
- [ ] Documented dashboard setup in notebook
- [ ] Committed changes with message: "feat(mlops): add W&B custom visualizations and dashboards"

## Completion Checklist

- [ ] All acceptance criteria met (10/10 checked)
- [ ] All test scenarios pass (4/4 verified)
- [ ] Code follows style guide
- [ ] Documentation complete
- [ ] No regressions
- [ ] Tested in Colab
- [ ] Git committed
