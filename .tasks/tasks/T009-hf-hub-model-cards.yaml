---
id: T009
title: "HF Hub Model Cards - Auto-Generate Comprehensive Documentation"
status: pending
priority: P2
dependencies: [T008]
tags: [mlops, model-registry, huggingface, documentation, phase-1]
estimated_tokens: 10000
actual_tokens: 0
created_at: "2025-01-15"
updated_at: "2025-01-15"
---

## Description

Auto-generate professional HuggingFace model cards with training details, architecture specs, usage examples, and limitations. Makes models discoverable, trustworthy, and easy to use for collaborators.

## Business Context

**Problem**: Uploaded models have minimal documentation. Users struggle to understand model capabilities, training data, or how to use them.

**Value**: Professional model cards boost trust, discoverability, and adoption. Clear documentation prevents misuse and sets proper expectations.

**User Story**: As an ML researcher sharing models, I want auto-generated model cards so that others can quickly understand what my model does, how it was trained, and how to use it - without writing documentation manually.

## Acceptance Criteria

1. [ ] Auto-generate model card with metadata (architecture, params, training config)
2. [ ] Include training details (dataset, hyperparameters, final metrics)
3. [ ] Add model usage examples (Python code snippet)
4. [ ] Document limitations and intended use
5. [ ] Add training curves visualization to model card
6. [ ] Include evaluation results section
7. [ ] Add citation information
8. [ ] Save model card as README.md in model repo
9. [ ] Test model card renders correctly on HF Hub
10. [ ] Document how to customize model cards

## Test Scenarios

### Scenario 1: Auto-Generated Model Card Content
```
Given: Trained GPT model (50M params, wikitext-103, val_ppl=18.5)
When: Model uploaded to HF Hub with auto-generated card
Then: README.md includes:
  - Model name and description
  - Architecture details (50M params, 12 layers, 8 heads)
  - Training dataset (wikitext-103)
  - Hyperparameters (LR, batch size, epochs)
  - Final metrics (perplexity, accuracy)
  - Usage code snippet
```

### Scenario 2: Training Curves in Model Card
```
Given: Training run with logged loss curves
When: Model card generated
Then: Model card includes embedded W&B training curve image
  And: Image shows train/val loss over epochs
  And: Final perplexity annotated on chart
```

### Scenario 3: Customizable Sections
```
Given: User wants to add custom "Known Issues" section
When: model_card_generator() called with custom_sections param
Then: Generated README.md includes custom section
  And: Standard sections still present
  And: Formatting preserved
```

## Technical Implementation

```python
# Model Card Generation

def generate_model_card(
    model_name,
    model,
    config,
    training_args,
    metrics,
    dataset_name,
    wandb_run_url=None,
    custom_sections=None,
):
    """
    Generate HuggingFace model card (README.md).

    Args:
        model_name: str, model identifier
        model: PyTorch model
        config: Model configuration object
        training_args: dict of training hyperparameters
        metrics: dict of final metrics
        dataset_name: str, name of training dataset
        wandb_run_url: Optional W&B run URL for tracking
        custom_sections: dict of additional markdown sections

    Returns:
        model_card: str, markdown content for README.md
    """

    # Detect model architecture
    model_type = _detect_model_type(model)
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    # Build model card
    card = f"""---
language: en
license: mit
tags:
- transformer
- {model_type}
- transformer-builder
datasets:
- {dataset_name}
metrics:
- perplexity
- accuracy
---

# {model_name}

## Model Description

This is a **{model_type.upper()}** transformer model trained using [Transformer Builder](https://transformer-builder.com). The model was trained on {dataset_name} for next-token prediction.

## Model Details

- **Model Type**: {model_type.upper()} (Decoder-only / Encoder-only / Encoder-Decoder)
- **Parameters**: {total_params:,} total ({trainable_params:,} trainable)
- **Vocabulary Size**: {getattr(config, 'vocab_size', 50257):,}
- **Max Sequence Length**: {getattr(config, 'max_seq_len', 128)}
- **Training Framework**: PyTorch + Transformer Builder
- **Developed by**: [Your Name/Organization]

### Architecture

```
Total Parameters: {total_params / 1e6:.1f}M
Embedding Dimension: {getattr(config, 'd_model', 'N/A')}
Number of Layers: {getattr(config, 'num_layers', 'N/A')}
Number of Attention Heads: {getattr(config, 'num_heads', 'N/A')}
FFN Hidden Dimension: {getattr(config, 'd_ff', 'N/A')}
```

## Training Data

**Dataset**: {dataset_name}

- **Source**: {"HuggingFace Datasets" if dataset_name in ["wikitext-103", "bookcorpus"] else "Custom"}
- **Split**: Training
- **Preprocessing**: Tokenization, padding to max_seq_len={getattr(config, 'max_seq_len', 128)}

## Training Procedure

### Hyperparameters

```yaml
learning_rate: {training_args.get('learning_rate', 'N/A')}
batch_size: {training_args.get('batch_size', 'N/A')}
epochs: {training_args.get('epochs', 'N/A')}
warmup_ratio: {training_args.get('warmup_ratio', 0.1)}
weight_decay: {training_args.get('weight_decay', 0.01)}
optimizer: {training_args.get('optimizer', 'AdamW')}
lr_scheduler: {training_args.get('lr_scheduler', 'cosine')}
gradient_clipping: {training_args.get('max_grad_norm', 1.0)}
mixed_precision: {training_args.get('use_amp', True)}
```

### Training Environment

- **Hardware**: Google Colab (T4 GPU / A100 GPU / CPU)
- **Training Time**: ~{metrics.get('training_time_hours', 'N/A')} hours
- **Framework Versions**:
  - PyTorch: {torch.__version__}
  - Transformers: {transformers.__version__ if 'transformers' in sys.modules else 'N/A'}

## Evaluation Results

### Final Metrics

| Metric | Train | Validation |
|--------|-------|------------|
| **Loss** | {metrics.get('final_train_loss', 'N/A')} | {metrics.get('final_val_loss', 'N/A')} |
| **Perplexity** | {metrics.get('final_train_ppl', 'N/A')} | {metrics.get('final_val_ppl', 'N/A')} |
| **Accuracy (Top-1)** | {metrics.get('final_train_acc', 'N/A')} | {metrics.get('final_val_acc', 'N/A')} |

### Training Curves

"""

    # Add W&B training curve if available
    if wandb_run_url:
        card += f"""
![Training Curves]({wandb_run_url}/files/media/images/training_curves.png)

[View full training run on W&B]({wandb_run_url})
"""

    card += f"""

## Usage

### Direct PyTorch Inference

```python
import torch
from huggingface_hub import hf_hub_download

# Download model
model_path = hf_hub_download(repo_id="{model_name}", filename="pytorch_model.bin")
config_path = hf_hub_download(repo_id="{model_name}", filename="config.json")

# Load config and model
import json
with open(config_path, 'r') as f:
    config = json.load(f)

# Instantiate model (requires your model class definition)
model = YourTransformerModel(config)
model.load_state_dict(torch.load(model_path, map_location='cpu'))
model.eval()

# Inference
input_ids = torch.tensor([[1, 2, 3, 4, 5]])  # Your tokenized input
with torch.no_grad():
    outputs = model(input_ids)
    predictions = outputs.argmax(dim=-1)

print(f"Next token predictions: {{predictions}}")
```

### HuggingFace Transformers (if compatible)

```python
from transformers import AutoModel, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("{model_name}")
model = AutoModel.from_pretrained("{model_name}")

text = "Hello, how are you"
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)
```

## Limitations

- **Domain**: This model was trained on {dataset_name}, and may not generalize well to other domains
- **Sequence Length**: Maximum sequence length is {getattr(config, 'max_seq_len', 128)} tokens
- **Language**: English only (if applicable to dataset)
- **Bias**: Model may exhibit biases present in the training data

## Intended Use

**Primary Use Cases**:
- Text generation
- Language modeling research
- Educational purposes

**Out-of-Scope Uses**:
- Production deployment without additional safety testing
- Generating harmful or biased content
- Medical, legal, or financial advice

## Citation

```bibtex
@misc{{{model_name.replace('/', '_')},
  author = {{Your Name}},
  title = {{{model_name}}},
  year = {{datetime.now().year}},
  publisher = {{HuggingFace}},
  howpublished = {{\\url{{https://huggingface.co/{model_name}}}}},
}}
```

## Model Card Authors

- [Your Name] - Model training and documentation
- Created using [Transformer Builder](https://transformer-builder.com)

## Model Card Contact

For questions or issues, please open an issue on the [model repository](https://huggingface.co/{model_name}/discussions).
"""

    # Add custom sections
    if custom_sections:
        for title, content in custom_sections.items():
            card += f"\n## {title}\n\n{content}\n"

    return card


# Save and Upload Model Card

def save_and_upload_model_card(model_card_content, model_name, repo_id):
    """Save model card locally and upload to HF Hub."""

    # Save locally
    with open("README.md", "w") as f:
        f.write(model_card_content)

    print("✅ Model card saved to README.md")

    # Upload to HF Hub
    from huggingface_hub import HfApi
    api = HfApi()

    api.upload_file(
        path_or_fileobj="README.md",
        path_in_repo="README.md",
        repo_id=repo_id,
        commit_message="Add auto-generated model card",
    )

    print(f"✅ Model card uploaded to {repo_id}")
    print(f"   View at: https://huggingface.co/{repo_id}")
```

## Dependencies

- **T008**: HF Hub Basic Push (requires model upload functionality)

## Design Decisions

**Decision 1**: Use structured markdown with YAML frontmatter
- **Rationale**: HF Hub parses frontmatter for search/filtering, markdown for readability
- **Alternative considered**: Plain text (not searchable) or HTML (not portable)
- **Trade-off**: Some redundancy between frontmatter and body, but necessary

**Decision 2**: Include usage examples in multiple formats
- **Rationale**: Users have different workflows (raw PyTorch vs transformers library)
- **Alternative considered**: Single example (leaves some users confused)
- **Trade-off**: Longer documentation, but higher adoption

**Decision 3**: Auto-embed W&B training curves
- **Rationale**: Visual proof of training quality, builds trust
- **Alternative considered**: Link only (requires extra click)
- **Trade-off**: Requires public W&B run, but most are public anyway

## Risks & Mitigations

### Risk 1: Auto-Generated Content Inaccurate
- **Impact**: MEDIUM - Wrong info misleads users
- **Likelihood**: LOW - We pull from verified sources (config, metrics)
- **Mitigation**: Allow manual customization, validate generated content

### Risk 2: W&B Image Embed Fails
- **Impact**: LOW - Model card still useful without image
- **Likelihood**: MEDIUM - Depends on W&B run being public
- **Mitigation**: Graceful fallback to text-only metrics, link to W&B

## Progress Log

- [ ] Created task specification (this file)
- [ ] Implemented model card generation
- [ ] Added HF Hub upload
- [ ] Tested model card rendering
- [ ] Committed changes with message: "feat(mlops): add auto-generated HF Hub model cards"

## Completion Checklist

- [ ] All acceptance criteria met (10/10 checked)
- [ ] All test scenarios pass (3/3 verified)
- [ ] Code follows style guide
- [ ] Documentation complete
- [ ] No regressions
- [ ] Tested in Colab
- [ ] Git committed
