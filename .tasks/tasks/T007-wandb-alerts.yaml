---
id: T007
title: "W&B Alerts - Automated Notifications for Training Issues"
status: pending
priority: P4
dependencies: [T001, T002]
tags: [mlops, experiment-tracking, wandb, monitoring, phase-1]
estimated_tokens: 7000
actual_tokens: 0
created_at: "2025-01-15"
updated_at: "2025-01-15"
---

## Description

Implement W&B Alerts for automated monitoring of training anomalies like NaN losses, gradient explosions, unexpectedly low accuracy, or training stalls. Enables proactive intervention before wasting compute.

## Business Context

**Problem**: Users leave long training runs unattended in Colab, only to discover hours later that training failed due to NaN loss, gradient explosion, or data pipeline issues.

**Value**: Instant notifications (email/Slack) when training goes wrong. Save compute hours by catching failures early.

**User Story**: As an ML practitioner, I want automated alerts when my training run encounters NaN loss so that I can stop it and fix the issue instead of wasting 6 hours of compute.

## Acceptance Criteria

1. [ ] Create alert for NaN or Inf loss detection
2. [ ] Create alert for gradient norm explosions (>100)
3. [ ] Create alert for training stall (no improvement for N epochs)
4. [ ] Create alert for unexpectedly low accuracy (<5% after epoch 1)
5. [ ] Add W&B alert configuration in notebook cell
6. [ ] Document how to set up email/Slack notifications
7. [ ] Test alert triggering with intentional training failure
8. [ ] Add graceful training termination when critical alert fired
9. [ ] Log alert events to W&B run summary
10. [ ] Create markdown cell explaining alert types and thresholds

## Test Scenarios

### Scenario 1: NaN Loss Detection
```
Given: Training run with unstable learning rate (1e-2, too high)
When: Loss becomes NaN at step 150
Then: W&B alert fired: "NaN loss detected at step 150"
  And: Email notification sent to user
  And: Training gracefully terminates with checkpoint saved
```

### Scenario 2: Gradient Explosion
```
Given: Training without gradient clipping
When: Gradient norm exceeds 1000 (explosion)
Then: W&B alert fired: "Gradient explosion detected (norm=1234.5)"
  And: User notified
  And: Training continues (non-critical alert) but issue flagged
```

### Scenario 3: Training Stall Detection
```
Given: Training for 20 epochs, val_loss hasn't improved since epoch 12
When: Patience threshold (5 epochs) exceeded
Then: W&B alert fired: "Training stalled - no improvement for 5 epochs"
  And: User notified
  And: Early stopping triggered, training terminates
```

## Technical Implementation

```python
# Alert Configuration

def setup_wandb_alerts(run, alert_email=None, alert_slack_channel=None):
    """
    Configure W&B alerts for training monitoring.

    Args:
        run: wandb.Run object
        alert_email: Email for notifications (or None for default W&B account email)
        alert_slack_channel: Slack channel for notifications (or None)
    """

    # Alert 1: NaN Loss Detection (CRITICAL)
    run.alert(
        title="NaN Loss Detected",
        text="Training encountered NaN loss - likely learning rate too high or numerical instability",
        level="ERROR",
        wait_duration=0,  # Fire immediately
    )

    # Alert 2: Gradient Explosion (WARNING)
    run.alert(
        title="Gradient Explosion",
        text="Gradient norm exceeded 100 - consider enabling gradient clipping",
        level="WARN",
        wait_duration=60,  # Fire after 1 minute of sustained issue
    )

    # Alert 3: Training Stall (INFO)
    run.alert(
        title="Training Stalled",
        text="No validation improvement for 5 epochs - consider early stopping",
        level="INFO",
        wait_duration=0,
    )

    print("âœ… W&B alerts configured")
    if alert_email:
        print(f"   Email alerts will be sent to: {alert_email}")
    if alert_slack_channel:
        print(f"   Slack alerts will be sent to: {alert_slack_channel}")


# Alert Checking in Training Loop

def check_training_alerts(run, step, loss, grad_norm, val_loss_history, patience=5):
    """
    Check for training anomalies and fire alerts.

    Args:
        run: wandb.Run object
        step: Current training step
        loss: Current loss value
        grad_norm: Current gradient norm
        val_loss_history: List of recent validation losses
        patience: Number of epochs without improvement before stall alert

    Returns:
        should_stop: bool, whether training should terminate
    """
    should_stop = False

    # Check for NaN/Inf loss
    if math.isnan(loss) or math.isinf(loss):
        run.alert(
            title="ðŸš¨ NaN Loss Detected - Training Terminated",
            text=f"Training encountered {'NaN' if math.isnan(loss) else 'Inf'} loss at step {step}. "
                 f"This usually indicates:\n"
                 f"- Learning rate too high (try reducing by 10x)\n"
                 f"- Numerical instability (enable mixed precision)\n"
                 f"- Bad data batch (check dataset for corrupted samples)",
            level="ERROR",
        )
        run.summary['termination_reason'] = 'nan_loss'
        run.summary['nan_step'] = step
        should_stop = True
        return should_stop

    # Check for gradient explosion
    if grad_norm > 100:
        run.alert(
            title="âš ï¸ Gradient Explosion Detected",
            text=f"Gradient norm is {grad_norm:.2f} at step {step}. "
                 f"Consider:\n"
                 f"- Enabling gradient clipping (max_norm=1.0)\n"
                 f"- Reducing learning rate\n"
                 f"- Checking for data outliers",
            level="WARN",
        )
        wandb.log({"alerts/gradient_explosion": 1}, step=step)
        # Don't stop training, just warn

    # Check for training stall
    if len(val_loss_history) >= patience + 1:
        recent_losses = val_loss_history[-patience:]
        best_recent = min(recent_losses)
        previous_best = min(val_loss_history[:-patience])

        if best_recent >= previous_best:
            run.alert(
                title="ðŸ“‰ Training Stalled",
                text=f"Validation loss hasn't improved for {patience} epochs. "
                     f"Best recent: {best_recent:.4f}, Previous best: {previous_best:.4f}. "
                     f"Consider early stopping.",
                level="INFO",
            )
            run.summary['stall_detected'] = True
            run.summary['epochs_without_improvement'] = patience
            # Don't auto-stop, let user decide (unless early stopping enabled)

    # Check for unexpectedly low accuracy (after first epoch)
    if step > 1000 and run.summary.get('val/accuracy_top1', 0) < 0.05:
        run.alert(
            title="âš ï¸ Unexpectedly Low Accuracy",
            text=f"Validation accuracy is {run.summary.get('val/accuracy_top1', 0)*100:.1f}% after {step} steps. "
                 f"This may indicate:\n"
                 f"- Model not learning (check loss is decreasing)\n"
                 f"- Data/label mismatch\n"
                 f"- Incorrect evaluation metric",
            level="WARN",
        )

    return should_stop


# Integration in Training Loop

# Setup alerts at training start
setup_wandb_alerts(wandb.run, alert_email="user@example.com")

# In training loop
for step, batch in enumerate(train_loader):
    # ... training code ...

    # Check alerts
    should_stop = check_training_alerts(
        run=wandb.run,
        step=step,
        loss=loss.item(),
        grad_norm=total_grad_norm,
        val_loss_history=val_loss_history,
        patience=5,
    )

    if should_stop:
        print(f"â›” Training terminated at step {step} due to critical alert")
        # Save emergency checkpoint
        save_checkpoint(model, optimizer, step, "./emergency_checkpoint.pt")
        break
```

## Dependencies

- **T001**: W&B Basic Integration (requires wandb.run)
- **T002**: W&B Metrics Logging (requires metrics to monitor)

## Design Decisions

**Decision 1**: Three alert levels (ERROR, WARN, INFO)
- **Rationale**: Match severity to user action required
- **Alternative considered**: Single alert type (too noisy or not urgent enough)
- **Trade-off**: More configuration, but better user experience

**Decision 2**: Auto-terminate on NaN loss, warn-only on gradient explosion
- **Rationale**: NaN loss is unrecoverable, gradient explosion might self-correct
- **Alternative considered**: Auto-terminate on all alerts (too aggressive)
- **Trade-off**: Some discretion needed, but sensible defaults

## Risks & Mitigations

### Risk 1: Alert Fatigue
- **Impact**: MEDIUM - Too many alerts â†’ users ignore them
- **Likelihood**: LOW - We use conservative thresholds
- **Mitigation**: Tiered alerts, wait_duration to avoid spam, user-configurable thresholds

### Risk 2: False Positives
- **Impact**: LOW - Alert fired when training actually okay
- **Likelihood**: MEDIUM - Gradient spikes can be normal early in training
- **Mitigation**: Use wait_duration, check alert history, document expected vs problematic patterns

## Progress Log

- [ ] Created task specification (this file)
- [ ] Implemented alert configuration
- [ ] Added alert checking in training loop
- [ ] Tested with intentional failure
- [ ] Documented alert setup
- [ ] Committed changes with message: "feat(mlops): add W&B alerts for training anomaly detection"

## Completion Checklist

- [ ] All acceptance criteria met (10/10 checked)
- [ ] All test scenarios pass (3/3 verified)
- [ ] Code follows style guide
- [ ] Documentation complete
- [ ] No regressions
- [ ] Tested in Colab
- [ ] Git committed
