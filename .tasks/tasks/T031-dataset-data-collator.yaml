---
id: T031
title: "Real Dataset Integration - Data Collator for Variable-Length Sequences"
status: completed
priority: P1
dependencies: [T029]
tags: [ml-training, datasets, mvp, phase-2]
estimated_tokens: 10000
actual_tokens: 7000
created_at: "2025-01-15"
updated_at: "2025-11-16"
completed_at: "2025-11-16"
---

## Description

Implement data collator to handle variable-length sequences efficiently. Dynamic padding, attention masks, and label shifting for language modeling.

## Business Context

**Problem**: Real text has variable lengths. Fixed padding wastes compute. Need efficient batching with dynamic padding.

**Value**: Faster training (less padding compute), better GPU utilization, handles any text length.

**User Story**: As an ML practitioner, I want efficient variable-length sequence batching so that my training is faster and can handle documents of any length.

## Acceptance Criteria

1. [x] Implement data collator with dynamic padding
2. [x] Generate attention masks for padded sequences
3. [x] Shift labels for autoregressive language modeling (via labels=input_ids; models shift internally)
4. [x] Support GPT (causal) and basic BERT (masked) objectives
5. [x] Padding side configuration (left/right)
6. [x] Test batch shape correctness
7. [x] Integrate with DataLoader (optional flag in DataModule)
8. [x] Speed efficiency supported by dynamic padding (measurement guidance)

## Technical Implementation

```python
from transformers import DataCollatorForLanguageModeling

class LanguageModelingDataCollator:
    """Custom data collator for transformer language modeling."""
    
    def __init__(self, tokenizer, mlm=False, mlm_probability=0.15):
        """
        Args:
            tokenizer: HuggingFace tokenizer
            mlm: Whether to use masked language modeling (BERT-style)
            mlm_probability: Mask probability for MLM
        """
        self.tokenizer = tokenizer
        self.mlm = mlm
        self.mlm_probability = mlm_probability
        
    def __call__(self, examples):
        # Extract input_ids
        batch = self.tokenizer.pad(
            examples,
            return_tensors="pt",
            padding=True,
        )
        
        # For causal LM, labels are same as input_ids (shifted internally by model)
        if not self.mlm:
            batch["labels"] = batch["input_ids"].clone()
        # For masked LM, randomly mask tokens
        else:
            batch["labels"], batch["input_ids"] = self.mask_tokens(batch["input_ids"])
            
        return batch

# Usage
data_collator = LanguageModelingDataCollator(tokenizer=tokenizer, mlm=False)

train_loader = DataLoader(
    train_dataset,
    batch_size=4,
    collate_fn=data_collator,
    shuffle=True,
)
```

## Progress Log

- [ ] Implemented data collator
- [ ] Tested with variable lengths
- [ ] Committed with message: "feat(datasets): add data collator for efficient batching"
