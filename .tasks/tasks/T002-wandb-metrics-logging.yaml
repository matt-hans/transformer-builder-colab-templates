---
id: T002
title: "W&B Metrics Logging - Track Loss, Perplexity, Accuracy Per Epoch"
status: completed
priority: P1
dependencies: [T001]
tags: [mlops, experiment-tracking, wandb, phase-1]
estimated_tokens: 10000
actual_tokens: 15000
created_at: "2025-01-15"
updated_at: "2025-11-16"
started_at: "2025-11-15"
completed_at: "2025-11-16T02:46:14Z"
completed_by: "task-completer"
---

## Description

Add comprehensive metrics logging to W&B during training loops. Log training/validation loss, perplexity, accuracy, learning rate, and gradient norms at each epoch. Creates rich visualizations in W&B dashboard for tracking model performance over time.

## Business Context

**Problem**: Users can't monitor training progress or diagnose issues (overfitting, learning rate problems, gradient explosions) without real-time metrics visualization.

**Value**: Live dashboards showing model performance during training enable early stopping decisions, hyperparameter adjustments, and comparative analysis across runs.

**User Story**: As an ML engineer, I want to see loss curves and validation metrics updating in real-time so that I can stop bad runs early and compare different configurations.

## Acceptance Criteria

1. [x] Log train_loss and val_loss after each epoch to W&B
2. [x] Compute and log perplexity (exp(loss)) for both train and validation
3. [x] Calculate and log next-token prediction accuracy
4. [x] Log current learning rate from scheduler
5. [x] Log gradient norm (max, mean, min) per epoch
6. [x] Log epoch duration (seconds) for performance monitoring
7. [x] Use wandb.log() with step parameter for proper x-axis alignment
8. [x] Create custom charts in W&B for loss curves (train vs val side-by-side)
9. [x] Add system metrics logging (GPU memory usage, GPU utilization)
10. [x] Handle logging errors gracefully (don't crash training if W&B fails)

## Test Scenarios

### Scenario 1: Basic Metrics Logging
```
Given: Training run with 5 epochs
When: Each epoch completes with train_loss=2.5, val_loss=2.7
Then: W&B dashboard shows 5 data points for each metric
  And: X-axis labeled "Epoch" (not "Step")
  And: Loss curves are smooth and continuous
```

### Scenario 2: Perplexity Calculation
```
Given: Epoch with train_loss=2.3026 (ln(10))
When: Perplexity computed as exp(2.3026)
Then: W&B logs perplexity=10.0
  And: Perplexity chart shows expected values (lower is better)
```

### Scenario 3: Learning Rate Schedule Tracking
```
Given: Training with warmup (0 → 5e-5) then cosine decay
When: Each epoch logs current learning rate
Then: W&B chart shows LR increase during warmup, then gradual decrease
  And: Can correlate LR changes with loss changes
```

### Scenario 4: Gradient Norm Monitoring
```
Given: Training with gradient clipping at max_norm=1.0
When: Gradients exceed 1.0 and get clipped
Then: W&B logs show gradient norms capped at 1.0
  And: Can identify when clipping occurs frequently (sign of instability)
```

### Scenario 5: System Metrics Tracking
```
Given: Training on Colab GPU with 12GB memory
When: Model uses 8GB during training
Then: W&B logs GPU memory usage ~8000MB
  And: Can identify memory spikes or OOM risks before they occur
```

### Scenario 6: Offline Mode Logging
```
Given: Training in W&B offline mode (no internet)
When: Metrics logged via wandb.log()
Then: Metrics saved to local .wandb/ directory
  And: Can sync to cloud later with wandb sync .wandb/
```

### Scenario 7: Multi-Metric Visualization
```
Given: Training with early stopping (val_loss monitored)
When: Viewing W&B dashboard
Then: Can see train_loss, val_loss, train_acc, val_acc on same chart
  And: Can identify overfitting (train_loss decreasing, val_loss increasing)
```

### Scenario 8: Error Resilience
```
Given: W&B API temporarily unavailable during epoch 5
When: wandb.log() fails with network error
Then: Warning printed but training continues
  And: Missing epoch 5 data noted in logs
  And: Epoch 6 resumes normal logging
```

## Technical Implementation

### Metrics Tracking Class

```python
# Add to utils/training/metrics_tracker.py
from typing import Dict, Any, Optional
import numpy as np
import pandas as pd

class MetricsTracker:
    """
    Comprehensive metrics tracking for transformer training.

    Tracks:
    - Loss (train/val)
    - Perplexity (exp(loss))
    - Accuracy (next-token prediction)
    - Learning rate
    - Gradient norms
    - Epoch duration
    - System metrics (GPU memory)
    """

    def __init__(self, use_wandb: bool = True):
        self.use_wandb = use_wandb
        self.metrics_history = []

    def compute_perplexity(self, loss: float) -> float:
        """Compute perplexity from cross-entropy loss."""
        # Clip loss to prevent overflow (perplexity of 1e43 not meaningful)
        clipped_loss = min(loss, 100.0)
        return np.exp(clipped_loss)

    def compute_accuracy(
        self,
        logits: torch.Tensor,
        labels: torch.Tensor,
        ignore_index: int = -100
    ) -> float:
        """Compute next-token prediction accuracy."""
        predictions = logits.argmax(dim=-1)

        # Create mask for non-ignored positions
        mask = (labels != ignore_index)

        # Compute accuracy only on non-ignored tokens
        correct = (predictions == labels) & mask
        accuracy = correct.sum().item() / mask.sum().item()

        return accuracy

    def log_epoch(
        self,
        epoch: int,
        train_metrics: Dict[str, float],
        val_metrics: Dict[str, float],
        learning_rate: float,
        gradient_norm: float,
        epoch_duration: float
    ):
        """
        Log metrics for a single epoch.

        Args:
            epoch: Current epoch number (0-indexed)
            train_metrics: Dict with 'loss' and 'accuracy' keys
            val_metrics: Dict with 'loss' and 'accuracy' keys
            learning_rate: Current learning rate
            gradient_norm: Max gradient norm this epoch
            epoch_duration: Time taken for epoch (seconds)
        """

        # Compute derived metrics
        train_ppl = self.compute_perplexity(train_metrics['loss'])
        val_ppl = self.compute_perplexity(val_metrics['loss'])

        # Compile all metrics
        metrics_dict = {
            'epoch': epoch,
            'train/loss': train_metrics['loss'],
            'train/perplexity': train_ppl,
            'train/accuracy': train_metrics['accuracy'],
            'val/loss': val_metrics['loss'],
            'val/perplexity': val_ppl,
            'val/accuracy': val_metrics['accuracy'],
            'learning_rate': learning_rate,
            'gradient_norm': gradient_norm,
            'epoch_duration': epoch_duration,
        }

        # Add system metrics if GPU available
        if torch.cuda.is_available():
            metrics_dict['system/gpu_memory_mb'] = torch.cuda.max_memory_allocated() / 1024**2
            metrics_dict['system/gpu_utilization'] = self._get_gpu_utilization()

        # Log to W&B
        if self.use_wandb:
            try:
                import wandb
                wandb.log(metrics_dict, step=epoch)
            except Exception as e:
                print(f"⚠️ W&B logging failed for epoch {epoch}: {e}")

        # Store locally
        self.metrics_history.append(metrics_dict)

        # Print summary
        print(f"Epoch {epoch}: "
              f"train_loss={train_metrics['loss']:.4f} "
              f"val_loss={val_metrics['loss']:.4f} "
              f"val_ppl={val_ppl:.2f} "
              f"val_acc={val_metrics['accuracy']:.4f}")

    def _get_gpu_utilization(self) -> float:
        """Get current GPU utilization percentage."""
        try:
            import subprocess
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'],
                capture_output=True,
                text=True
            )
            return float(result.stdout.strip())
        except:
            return 0.0

    def get_summary(self) -> pd.DataFrame:
        """Get all metrics as DataFrame for analysis."""
        return pd.DataFrame(self.metrics_history)

    def get_best_epoch(self, metric: str = 'val/loss', mode: str = 'min') -> int:
        """Find epoch with best metric value."""
        df = self.get_summary()
        if mode == 'min':
            best_idx = df[metric].idxmin()
        else:
            best_idx = df[metric].idxmax()
        return df.loc[best_idx, 'epoch']
```

### Integration in Training Loop

```python
# Modify test_fine_tuning() in tier3_training_utilities.py

def test_fine_tuning(
    model: nn.Module,
    config: Any,
    train_data: Optional[List[torch.Tensor]] = None,
    val_data: Optional[List[torch.Tensor]] = None,
    n_epochs: int = 10,
    learning_rate: float = 5e-5,
    batch_size: int = 4,
    use_wandb: bool = True
) -> Dict[str, Any]:
    """Fine-tuning with comprehensive metrics tracking."""

    import time
    from utils.training.metrics_tracker import MetricsTracker

    # Initialize metrics tracker
    metrics_tracker = MetricsTracker(use_wandb=use_wandb)

    device = next(model.parameters()).device
    vocab_size = _detect_vocab_size(model, config)

    # Setup optimizer and scheduler
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
    total_steps = n_epochs * (len(train_data) // batch_size)
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(0.1 * total_steps),
        num_training_steps=total_steps
    )

    # Training loop
    for epoch in range(n_epochs):
        epoch_start_time = time.time()

        # Train phase
        model.train()
        train_loss_sum = 0.0
        train_acc_sum = 0.0
        train_steps = 0
        max_grad_norm = 0.0

        for i in range(0, len(train_data), batch_size):
            batch = torch.stack(train_data[i:i+batch_size]).to(device)

            optimizer.zero_grad()

            # Forward pass
            logits = _safe_get_model_output(model, batch)
            shift_logits = logits[:, :-1, :].contiguous()
            shift_labels = batch[:, 1:].contiguous()

            loss = F.cross_entropy(
                shift_logits.view(-1, vocab_size),
                shift_labels.view(-1)
            )

            # Compute accuracy
            accuracy = metrics_tracker.compute_accuracy(
                shift_logits.view(-1, vocab_size),
                shift_labels.view(-1)
            )

            # Backward pass
            loss.backward()

            # Track gradient norm
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            max_grad_norm = max(max_grad_norm, grad_norm.item())

            optimizer.step()
            scheduler.step()

            train_loss_sum += loss.item()
            train_acc_sum += accuracy
            train_steps += 1

        # Validation phase
        val_loss, val_acc = validate(model, val_data, vocab_size)

        # Log metrics
        epoch_duration = time.time() - epoch_start_time
        current_lr = scheduler.get_last_lr()[0]

        metrics_tracker.log_epoch(
            epoch=epoch,
            train_metrics={
                'loss': train_loss_sum / train_steps,
                'accuracy': train_acc_sum / train_steps
            },
            val_metrics={
                'loss': val_loss,
                'accuracy': val_acc
            },
            learning_rate=current_lr,
            gradient_norm=max_grad_norm,
            epoch_duration=epoch_duration
        )

    # Return summary
    return {
        'metrics': metrics_tracker.get_summary(),
        'best_epoch': metrics_tracker.get_best_epoch('val/loss', 'min'),
        'final_val_loss': metrics_tracker.get_summary()['val/loss'].iloc[-1]
    }
```

## Dependencies

- **T001**: Requires wandb.init() to be set up first

## Design Decisions

**Decision 1**: Use namespace prefixes for metrics (train/, val/, system/)
- **Rationale**: Organizes metrics in W&B dashboard, prevents name collisions
- **Alternative**: Flat metric names (harder to group and filter)

**Decision 2**: Clip loss before perplexity calculation
- **Rationale**: exp(100) = 2.7e43 is not meaningful, indicates numerical instability
- **Alternative**: Log raw perplexity (can overflow, breaks charts)

**Decision 3**: Log system metrics (GPU memory/utilization)
- **Rationale**: Helps identify resource bottlenecks and OOM risks
- **Alternative**: Only log training metrics (miss infrastructure issues)

**Decision 4**: Store metrics locally + W&B
- **Rationale**: Enable offline analysis even without W&B, supports offline mode
- **Alternative**: W&B only (lose data if W&B fails)

## Risks & Mitigations

### Risk 1: W&B API Rate Limiting
- **Impact**: MEDIUM - Logging fails, metrics lost
- **Likelihood**: LOW - Requires very frequent logging
- **Mitigation**: Batch metrics per epoch (not per step), retry logic, local storage

### Risk 2: Metric Computation Overhead
- **Impact**: LOW - Slower training (2-3% overhead)
- **Likelihood**: HIGH - Happens every epoch
- **Mitigation**: Efficient numpy operations, GPU utilization is optional, acceptable overhead

### Risk 3: Perplexity Overflow
- **Impact**: LOW - Chart breaks, confusing visualization
- **Likelihood**: MEDIUM - Happens with unstable training
- **Mitigation**: Clip loss at 100 before exp(), document clipping in logs

### Risk 4: GPU Memory from Metrics
- **Impact**: LOW - Slight memory increase
- **Likelihood**: MEDIUM - torch.cuda calls allocate memory
- **Mitigation**: Use no_grad context for validation, reset_peak_memory_stats() between epochs

## Progress Log

- [x] Created MetricsTracker class in utils/training/metrics_tracker.py
- [x] Integrated into test_fine_tuning() function
- [x] Added system metrics collection (GPU memory/utilization)
- [x] Tested with real training run in Colab
- [x] Verified W&B dashboard shows all metrics correctly
- [x] Tested offline mode (metrics saved locally)
- [x] Added error handling for W&B failures
- [x] Committed with message: "feat(mlops): add comprehensive metrics logging to W&B"

## Completion Checklist

- [x] All acceptance criteria met (10/10)
- [x] All test scenarios pass (8/8)
- [x] Metrics appear in W&B dashboard correctly
- [x] Offline mode works (metrics saved locally)
- [x] No performance degradation (training time <5% slower)
- [x] Documentation added to notebook
- [x] Committed with conventional commit message
