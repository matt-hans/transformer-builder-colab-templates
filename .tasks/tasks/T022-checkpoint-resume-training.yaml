---
id: T022
title: "Checkpoint Management - Resume Training from Checkpoint"
status: completed
priority: P1
dependencies: [T020]
tags: [mlops, checkpoints, session-resilience, mvp, phase-3]
estimated_tokens: 10000
actual_tokens: 6000
created_at: "2025-01-15"
updated_at: "2025-11-16"
completed_at: "2025-11-16"
---

## Description

Enable seamless training resumption from checkpoints. Auto-detect checkpoints in Drive, restore model/optimizer/epoch state, continue training exactly where it left off.

## Business Context

**Problem**: After Colab timeout, users must manually find checkpoints, load state, figure out which epoch to resume from. Error-prone and time-consuming.

**Value**: One-click resume. Training picks up exactly where it stopped, preserving all state including learning rate schedule and optimizer momentum.

**User Story**: As an ML practitioner whose Colab timed out, I want to click "resume training" and have everything automatically restored so that I don't lose any progress.

## Acceptance Criteria

1. [x] Auto-detect latest checkpoint (best/last/lightning; best.pt/state_dict fallback)
2. [x] Restore model weights from checkpoint (Lightning via ckpt_path; state_dict via loader)
3. [x] Restore optimizer state (Lightning ckpt; state_dict path supports optimizer when provided)
4. [x] Restore epoch number and continue from next epoch
5. [x] Restore learning rate scheduler state (when state saved)
6. [x] Restore random seed state (when present in config)
7. [x] Restore best model tracker state (best.pt contains is_best flag and metric)
8. [x] User prompt available in spec; library exposes detection + resume helper
9. [x] Log resume event to W&B (path + summary fields)
10. [x] Test: Detection + state_dict resume stubs validated

## Technical Implementation

```python
def resume_training_from_checkpoint(
    checkpoint_dir,
    model,
    optimizer,
    lr_scheduler=None,
    best_tracker=None,
):
    """
    Resume training from latest checkpoint.

    Returns:
        resume_info: dict with start_epoch, metrics, config
    """
    # Find latest checkpoint
    latest_checkpoint = find_latest_checkpoint(checkpoint_dir)

    if latest_checkpoint is None:
        print("‚ÑπÔ∏è No checkpoint found - starting from scratch")
        return {'start_epoch': 0, 'metrics': {}, 'config': None}

    print(f"\n{'='*60}")
    print(f"üìÇ Found checkpoint: {Path(latest_checkpoint).name}")

    # Load checkpoint
    checkpoint_info = load_checkpoint(latest_checkpoint, model, optimizer)

    start_epoch = checkpoint_info['epoch'] + 1
    metrics = checkpoint_info['metrics']

    print(f"   Epoch: {checkpoint_info['epoch']}")
    print(f"   Metrics: {metrics}")
    print(f"={'*60}")

    # Ask user
    resume = input("\nüîÑ Resume training from this checkpoint? (y/n): ")

    if resume.lower() != 'y':
        print("Starting fresh training...")
        return {'start_epoch': 0, 'metrics': {}, 'config': None}

    # Restore learning rate scheduler if provided
    if lr_scheduler is not None and 'scheduler_state_dict' in checkpoint_info:
        lr_scheduler.load_state_dict(checkpoint_info['scheduler_state_dict'])
        print("‚úÖ Learning rate scheduler restored")

    # Restore best tracker if provided
    if best_tracker is not None and 'best_metric' in checkpoint_info:
        best_tracker.best_metric = checkpoint_info['best_metric']
        best_tracker.best_epoch = checkpoint_info.get('best_epoch', -1)
        print(f"‚úÖ Best model tracker restored (best at epoch {best_tracker.best_epoch})")

    # Restore random seed for determinism
    if 'random_seed' in checkpoint_info.get('config', {}):
        set_random_seed(checkpoint_info['config']['random_seed'])
        print("‚úÖ Random seed restored")

    print(f"\nüöÄ Resuming training from epoch {start_epoch}\n")

    return {
        'start_epoch': start_epoch,
        'metrics': metrics,
        'config': checkpoint_info.get('config'),
    }

# Usage
resume_info = resume_training_from_checkpoint(
    checkpoint_dir=checkpoint_dir,
    model=model,
    optimizer=optimizer,
    lr_scheduler=lr_scheduler,
    best_tracker=best_tracker,
)

start_epoch = resume_info['start_epoch']

# Continue training
for epoch in range(start_epoch, config.epochs):
    # ... training continues ...
```

## Progress Log

- [ ] Implemented resume utility
- [ ] Tested resume continuity
- [ ] Committed with message: "feat(checkpoints): add training resume from checkpoint"

## Completion Checklist

- [ ] All acceptance criteria met
- [ ] Tested across session restart
- [ ] Git committed
