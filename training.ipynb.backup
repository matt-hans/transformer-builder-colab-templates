{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "error-handling-setup"},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ERROR HANDLING SETUP ‚Äî Full Tracebacks and Formatting\n",
    "# ==============================================================================\n",
    "import sys, traceback\n",
    "sys.tracebacklimit = 50  # show up to 50 frames\n",
    "\n",
    "def format_exception(e: Exception, context_lines: int = 5) -> str:\n",
    "    \"\"\"Format exception with full traceback.\n",
    "\n",
    "    Args:\n",
    "        e: Exception instance\n",
    "        context_lines: Unused placeholder for future code context.\n",
    "    Returns: String with full traceback.\n",
    "    \"\"\"\n",
    "    tb_lines = traceback.format_exception(type(e), e, e.__traceback__)\n",
    "    return ''.join(tb_lines)\n",
    "\n",
    "# Install IPython custom exception handler to avoid truncation\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    ip = get_ipython()\n",
    "    if ip is not None:\n",
    "        def _custom_exc(shell, etype, evalue, tb, tb_offset=None):\n",
    "            print('‚ùå Exception occurred')\n",
    "            print('=' * 60)\n",
    "            print(''.join(traceback.format_exception(etype, evalue, tb)))\n",
    "            print('=' * 60)\n",
    "            return True\n",
    "        ip.set_custom_exc((Exception,), _custom_exc)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ==============================================================================\n",
    "# NETWORK RETRY MONKEY-PATCH ‚Äî urllib.urlopen with retries (GitHub/HF)\n",
    "# ==============================================================================\n",
    "try:\n",
    "    import urllib.request as _ur, urllib.error as _ue, time as _t, random as _r\n",
    "    _orig_urlopen = _ur.urlopen\n",
    "    def _retrying_urlopen(req, timeout=20, max_retries=5, backoff=1.0):\n",
    "        attempt = 0\n",
    "        while True:\n",
    "            try:\n",
    "                return _orig_urlopen(req, timeout=timeout)\n",
    "            except _ue.HTTPError as e:\n",
    "                code = getattr(e, 'code', None)\n",
    "                if code == 404:\n",
    "                    raise\n",
    "                if code in (429, 500, 502, 503, 504, 403):\n",
    "                    attempt += 1\n",
    "                    if attempt > max_retries:\n",
    "                        raise\n",
    "                    ra = getattr(e, 'headers', {{}}).get('Retry-After') if hasattr(e, 'headers') else None\n",
    "                    try:\n",
    "                        ra_val = float(ra) if ra is not None else None\n",
    "                    except Exception:\n",
    "                        ra_val = None\n",
    "                    sleep_for = ra_val if ra_val is not None else backoff * (2 ** (attempt - 1))\n",
    "                    sleep_for += _r.random() * 0.25 * sleep_for\n",
    "                    print(f\"‚è≥ Network retry {attempt}/{max_retries} in {sleep_for:.1f}s (HTTP {code})\")\n",
    "                    _t.sleep(min(sleep_for, 30.0))\n",
    "                    continue\n",
    "                raise\n",
    "            except Exception:\n",
    "                attempt += 1\n",
    "                if attempt > max_retries:\n",
    "                    raise\n",
    "                sleep_for = backoff * (2 ** (attempt - 1))\n",
    "                sleep_for += _r.random() * 0.25 * sleep_for\n",
    "                print(f\"‚è≥ Network retry {attempt}/{max_retries} in {sleep_for:.1f}s\")\n",
    "                _t.sleep(min(sleep_for, 30.0))\n",
    "    def urlopen_with_retry(req, timeout=20):\n",
    "        return _retrying_urlopen(req, timeout=timeout)\n",
    "    _ur.urlopen = urlopen_with_retry\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "troubleshooting"},
   "source": [
     "---\n",
     "\n",
     "### üõ†Ô∏è Troubleshooting\n",
     "\n",
     "- Full Python tracebacks (up to 50 frames) are enabled.\n",
     "- Check ImportError messages and verify dependencies in this runtime.\n",
     "- For training failures (NaN loss), inspect gradient clipping logs in the output.\n",
     "\n",
     "Use \`print(format_exception(e))\` in custom try/except blocks to print a full traceback.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Transformer Builder - Training Utilities\n",
    "\n",
    "This notebook provides training and optimization utilities for your custom transformer.\n",
    "\n",
    "**Prerequisites:**\n",
    "- ‚úÖ You've already run `template.ipynb` and your model passed Tier 1 validation\n",
    "- ‚úÖ You have your Gist ID from Transformer Builder\n",
    "\n",
    "**What's included:**\n",
    "- üéì **Fine-Tuning:** Training loop with loss tracking and gradient monitoring\n",
    "- üîß **Hyperparameter Search:** Automated optimization using Optuna\n",
    "- üìä **Benchmark Comparison:** Compare against production baselines\n",
    "\n",
    "**Estimated time:** 10-20 minutes (GPU recommended)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Important: Fresh Runtime Required\n",
    "\n",
    "This notebook installs `pytorch-lightning` and `optuna`, which require NumPy version changes.\n",
    "\n",
    "**Before running:**\n",
    "1. **Runtime ‚Üí Restart runtime** (ensures clean environment)\n",
    "2. **Run all cells** in order\n",
    "\n",
    "**Note:** Do NOT try to run this in the same runtime as `template.ipynb` - you'll get NumPy corruption errors.\n",
    "\n",
    "---\n",
    "\n",
    "**Source:** Generated from [Transformer Builder](https://transformer-builder.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã STEP 1: Paste Your Gist ID\n",
    "\n",
    "Same Gist ID you used in `template.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# GIST ID INPUT - Paste the ID from Transformer Builder\n",
    "# ==============================================================================\n",
    "\n",
    "#@title üì• **Paste Your Gist ID Here**\n",
    "GIST_ID = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown **Where to find your Gist ID:**\n",
    "#@markdown 1. Use the same Gist ID from template.ipynb\n",
    "#@markdown 2. Or go to Transformer Builder ‚Üí Export to Colab ‚Üí Copy Gist ID\n",
    "\n",
    "if not GIST_ID or not GIST_ID.strip():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚ö†Ô∏è  NO GIST ID PROVIDED\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(\"Please paste your Gist ID in the field above and re-run this cell.\")\n",
    "    print()\n",
    "    raise ValueError(\"Gist ID is required\")\n",
    "else:\n",
    "    import re\n",
    "    if not re.fullmatch(r\"[A-Za-z0-9]+\", GIST_ID.strip()):\n",
    "        print(\"=\" * 70)\n",
    "        print(\"‚ö†Ô∏è  INVALID GIST ID FORMAT\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "        print(f\"The Gist ID you entered: {GIST_ID!r}\")\n",
    "        print()\n",
    "        raise ValueError(\"Invalid Gist ID format\")\n",
    "    \n",
    "    GIST_ID = GIST_ID.strip()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚úÖ GIST ID SAVED\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(f\"Gist ID: {GIST_ID}\")\n",
    "    print()\n",
    "    print(\"Continue to next cell to install training dependencies...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ STEP 2: Install Training Dependencies\n",
    "\n",
    "This cell installs:\n",
    "- `pytorch-lightning` (training framework)\n",
    "- `optuna` (hyperparameter optimization)\n",
    "- `torchmetrics` (evaluation metrics)\n",
    "\n",
    "**Note:** This will reinstall NumPy to a compatible version. This is why we need a fresh runtime separate from `template.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ==============================================================================\n# INSTALL TRAINING DEPENDENCIES\n# ==============================================================================\n\nprint(\"=\" * 70)\nprint(\"üì¶ INSTALLING TRAINING DEPENDENCIES\")\nprint(\"=\" * 70)\nprint()\nprint(\"Installing from requirements-training.txt for exact reproducibility...\")\nprint()\nprint(\"‚è≥ This may take 30-60 seconds...\")\nprint()\n\n# Download requirements file from repository\n!wget -q https://raw.githubusercontent.com/matt-hans/transformer-builder-colab-templates/main/requirements-training.txt\n\n# Install from requirements file (exact versions for reproducibility)\n!pip install -q -r requirements-training.txt\n\nprint()\nprint(\"=\" * 70)\nprint(\"‚úÖ INSTALLATION COMPLETE\")\nprint(\"=\" * 70)\nprint()\n\n# Verify installation\ntry:\n    import pytorch_lightning as pl\n    import optuna\n    import torchmetrics\n    import torch\n    import numpy as np\n    import wandb\n    \n    print(f\"‚úÖ pytorch-lightning: {pl.__version__}\")\n    print(f\"‚úÖ optuna: {optuna.__version__}\")\n    print(f\"‚úÖ torchmetrics: {torchmetrics.__version__}\")\n    print(f\"‚úÖ torch: {torch.__version__}\")\n    print(f\"‚úÖ numpy: {np.__version__}\")\n    print(f'‚úÖ wandb: {wandb.__version__}')\n    print()\n    print(\"‚úÖ All training dependencies ready!\")\n    print()\n    \nexcept ImportError as e:\n    print(f\"‚ùå Import failed: {e}\")\n    print()\n    print(\"Please restart runtime and try again.\")\n    print()\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Weights & Biases Setup (Optional)\n",
    "\n",
    "**What is W&B?** Weights & Biases tracks your experiments so you never lose training data.\n",
    "\n",
    "**Benefits:**\n",
    "- üìà Automatic logging of loss, metrics, and hyperparameters\n",
    "- üíæ Persistent storage (survives Colab disconnects)\n",
    "- üîç Compare multiple training runs side-by-side\n",
    "- üåê Access dashboard from anywhere: [wandb.ai](https://wandb.ai)\n",
    "\n",
    "**Setup options:**\n",
    "1. **Recommended:** Use Colab Secrets (secure, reusable)\n",
    "   - Go to üîë (key icon) in left sidebar ‚Üí Add Secret\n",
    "   - Name: `WANDB_API_KEY`\n",
    "   - Value: Get from [wandb.ai/authorize](https://wandb.ai/authorize)\n",
    "\n",
    "2. **Quick:** Run the cell below and paste API key when prompted\n",
    "\n",
    "3. **Skip:** Cell will auto-enable offline mode (logs saved locally)\n",
    "\n",
    "**Free tier:** Unlimited runs, 100GB storage. [Create free account](https://wandb.ai/signup)\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö†Ô∏è Security:** NEVER hardcode API keys in notebooks. Always use Colab Secrets or environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# W&B AUTHENTICATION (OPTIONAL)\n",
    "# ==============================================================================\n",
    "\n",
    "#@title üîê **W&B Login** (optional - skip to use offline mode)\n",
    "#@markdown Run this cell to connect to Weights & Biases for experiment tracking.\n",
    "\n",
    "import os\n",
    "\n",
    "# Variable to track W&B status\n",
    "wandb_enabled = False\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    \n",
    "    # Attempt 1: Try Colab Secrets (most secure)\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        wandb_api_key = userdata.get('WANDB_API_KEY')\n",
    "        wandb.login(key=wandb_api_key)\n",
    "        wandb_enabled = True\n",
    "        print(\"‚úÖ W&B authenticated via Colab Secrets\")\n",
    "        print(f\"‚úÖ Logged in as: {wandb.api.viewer()['entity']}\")\n",
    "        print()\n",
    "        print(\"üéØ Experiments will be tracked at: https://wandb.ai\")\n",
    "    except Exception as e:\n",
    "        # Attempt 2: Try interactive login\n",
    "        try:\n",
    "            print(\"‚ö†Ô∏è  Colab Secrets not configured, trying interactive login...\")\n",
    "            print(\"üìù Get your API key from: https://wandb.ai/authorize\")\n",
    "            print()\n",
    "            wandb.login()\n",
    "            wandb_enabled = True\n",
    "            print(\"‚úÖ W&B authenticated via interactive login\")\n",
    "        except Exception as e2:\n",
    "            # Fallback: Offline mode\n",
    "            print(\"‚ö†Ô∏è  W&B authentication skipped\")\n",
    "            print(\"üì¥ Running in offline mode (logs saved locally to .wandb/)\")\n",
    "            print()\n",
    "            print(\"To enable tracking:\")\n",
    "            print(\"  1. Create free account: https://wandb.ai/signup\")\n",
    "            print(\"  2. Add WANDB_API_KEY to Colab Secrets\")\n",
    "            print(\"  3. Re-run this cell\")\n",
    "            print()\n",
    "            os.environ['WANDB_MODE'] = 'offline'\n",
    "            wandb_enabled = False\n",
    "\n",
    "except ImportError:\n",
    "    print(\"‚ùå wandb not installed - please run the dependencies cell first\")\n",
    "    wandb_enabled = False\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "if wandb_enabled:\n",
    "    print(\"‚úÖ W&B READY - Experiments will be tracked online\")\n",
    "else:\n",
    "    print(\"üì¥ W&B OFFLINE MODE - Logs saved locally only\")\n",
    "print(\"=\" * 70)\n",
    "print()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• STEP 3: Download Utils Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DOWNLOAD UTILS PACKAGE\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üì¶ Downloading test utilities package...\")\n",
    "\n",
    "# Remove old utils directory if exists\n",
    "!rm -rf utils/\n",
    "\n",
    "# Download complete utils package from GitHub\n",
    "!git clone --depth 1 --branch main https://github.com/matt-hans/transformer-builder-colab-templates.git temp_repo 2>/dev/null\n",
    "\n",
    "# Copy utils directory\n",
    "!cp -r temp_repo/utils ./\n",
    "\n",
    "# Cleanup\n",
    "!rm -rf temp_repo\n",
    "\n",
    "# Verify package structure\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if './' not in sys.path:\n",
    "    sys.path.insert(0, './')\n",
    "\n",
    "try:\n",
    "    import utils\n",
    "    print(f\"‚úÖ Utils package loaded (version {utils.__version__})\")\n",
    "    \n",
    "    # Test importing training functions\n",
    "    from utils import (\n",
    "        test_fine_tuning,\n",
    "        test_hyperparameter_search,\n",
    "        test_benchmark_comparison\n",
    "    )\n",
    "    print(\"‚úÖ Training test functions importable\")\n",
    "    print()\n",
    "    print(\"‚úÖ Utils package ready!\")\n",
    "    print()\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import utils package: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ STEP 4: Load Model from Gist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOAD CUSTOM MODEL FROM GIST\n",
    "# ==============================================================================\n",
    "\n",
    "import os, re, json, urllib.request, urllib.error\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL LOADING\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "if 'GIST_ID' not in globals() or not GIST_ID:\n",
    "    raise ValueError(\"Gist ID required - please run Cell 1 first\")\n",
    "\n",
    "gist_id = GIST_ID\n",
    "model_name = \"Model\"\n",
    "\n",
    "print(f\"üì• Loading model from GitHub Gist: {gist_id}\")\n",
    "print()\n",
    "\n",
    "def _fetch_gist(gid: str) -> dict:\n",
    "    url = f\"https://api.github.com/gists/{gid}\"\n",
    "    req = urllib.request.Request(url, headers={\n",
    "        \"Accept\": \"application/vnd.github+json\",\n",
    "        \"User-Agent\": \"transformer-builder-training\"\n",
    "    })\n",
    "    try:\n",
    "        with urllib.request.urlopen(req, timeout=20) as resp:\n",
    "            return json.loads(resp.read().decode(\"utf-8\"))\n",
    "    except urllib.error.HTTPError as e:\n",
    "        detail = f\"HTTP {e.code}\"\n",
    "        if e.code == 404:\n",
    "            detail += \" - Gist not found\"\n",
    "        raise RuntimeError(f\"GitHub API error: {detail}\") from e\n",
    "\n",
    "def _write(path: str, text: str):\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "try:\n",
    "    gist_data = _fetch_gist(gist_id)\n",
    "    files = gist_data.get(\"files\") or {}\n",
    "    \n",
    "    if \"model.py\" not in files or \"config.json\" not in files:\n",
    "        raise RuntimeError(\"Gist missing model.py or config.json\")\n",
    "    \n",
    "    model_code = files[\"model.py\"].get(\"content\", \"\")\n",
    "    config_json = files[\"config.json\"].get(\"content\", \"\")\n",
    "    \n",
    "    if not model_code or not config_json:\n",
    "        raise RuntimeError(\"Empty content in model files\")\n",
    "    \n",
    "    _write(\"custom_transformer.py\", model_code)\n",
    "    _write(\"config.json\", config_json)\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"‚úÖ Model code: {len(model_code):,} bytes\")\n",
    "    print(f\"‚úÖ Config: {len(config_json):,} bytes\")\n",
    "    print()\n",
    "    \n",
    "    # Parse model name\n",
    "    try:\n",
    "        config_dict = json.loads(config_json)\n",
    "        if 'model_name' in config_dict:\n",
    "            model_name = config_dict['model_name']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load model: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ MODEL LOADING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "params = {\"name\": model_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß STEP 5: Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch",
    "import json",
    "from utils.model_helpers import find_model_class, instantiate_model, create_model_config, count_parameters",
    "",
    "# Import the custom model",
    "exec(open('custom_transformer.py').read())",
    "",
    "# Load config",
    "with open('config.json') as f:",
    "    config_dict = json.load(f)",
    "",
    "# Find the model class",
    "model_class = find_model_class(globals(), model_name=params['name'])",
    "",
    "if model_class is None:",
    "    raise RuntimeError(f\"Could not find model class '{params['name']}'\")",
    "",
    "try:",
    "    # Instantiate model",
    "    model = instantiate_model(model_class, config_dict)",
    "    ",
    "    # Count parameters",
    "    param_counts = count_parameters(model)",
    "    ",
    "    print(f\"‚úÖ Model instantiated: {model_class.__name__}\")",
    "    print(f\"‚úÖ Total parameters: {param_counts['total']:,}\")",
    "    print(f\"‚úÖ Trainable parameters: {param_counts['trainable']:,}\")",
    "    ",
    "    # Move to GPU if available",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
    "    model = model.to(device)",
    "    print(f\"‚úÖ Device: {device}\")",
    "    print()",
    "    ",
    "except Exception as e:",
    "    print(f\"‚ùå Failed to instantiate model: {e}\")",
    "    import traceback",
    "    traceback.print_exc()",
    "    raise",
    "",
    "# Create config object",
    "config = create_model_config(config_dict)",
    "print(f\"‚úÖ Config prepared (vocab_size={config.vocab_size}, max_seq_len={config.max_seq_len})\")",
    "print(\"‚úÖ Ready for training tests!\")",
    "print()",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé≤ Reproducibility: Random Seed Management\n\nSetting a random seed ensures that your training runs are reproducible.\nWith the same seed, you'll get identical results every time.\n\nRecommendations:\n\n- Use a fixed seed (e.g., 42) for experiments you want to reproduce\n- Use different seeds for multiple runs to verify robustness\n- Enable deterministic mode only when exact reproduction is critical (it's slower)\n\nDeterministic Mode:\n\n- Fully reproducible results (bit-exact) ‚Äî ~20% slower training\n- Use for: Final experiments, debugging, publishing results\n\nFast Mode (default):\n\n- Faster training with cuDNN optimizations (minor non-determinism)\n- Use for: Hyperparameter search, initial experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Seed management ‚Äî set before any model init or data loading\nfrom utils.training.seed_manager import set_random_seed\n\n# Configure reproducibility\nrandom_seed = 42  #@param {type:\"integer\"}\ndeterministic = False  #@param {type:\"boolean\"}\n\n# Apply seeds across Python, NumPy, PyTorch (CPU/GPU)\nset_random_seed(random_seed, deterministic=deterministic)\nprint(f'üé≤ Using random_seed={random_seed}, deterministic={deterministic}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üöÄ Training Tests\n",
    "\n",
    "Now we'll run the Tier 3 training utilities on your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Tier 3 training utilitiesfrom utils.test_functions import (    test_fine_tuning,    test_hyperparameter_search,    test_benchmark_comparison)from utils.wandb_helpers import build_wandb_config, detect_model_type, print_wandb_summary# ==============================================================================# Initialize W&B Tracking# ==============================================================================if 'wandb_enabled' in globals() and wandb_enabled:    from datetime import datetime    import wandb        # Define hyperparameters (will be used in training tests)    hyperparameters = {        'learning_rate': 5e-5,        'batch_size': 2,        'epochs': 3,        'warmup_ratio': 0.1,        'weight_decay': 0.01,        'max_grad_norm': 1.0,        'use_amp': True,        'grad_accum_steps': 1,\n        'random_seed': random_seed,\n        'deterministic': deterministic    }        # Build W&B config using helper    wandb_config = build_wandb_config(model, config, hyperparameters)        # Detect model type for run name and tags    model_type = detect_model_type(model)    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')        # Initialize W&B run    run = wandb.init(        project=\"transformer-builder-training\",        name=f\"{model_type}_{timestamp}\",        tags=[model_type, \"v1\", \"tier3\"],        config=wandb_config    )        # Print summary    print_wandb_summary(run, model, hyperparameters)else:    print(\"üì¥ W&B tracking disabled (offline mode or not authenticated)\")    print()print(\"=\" * 80)print(\"TIER 3: TRAINING & PRODUCTION UTILITIES\")print(\"=\" * 80)print()# Test 1: Fine-Tuningprint(\"Test 1/3: Fine-Tuning Demo\")print(\"-\" * 80)print(\"Running 3 epochs of fine-tuning with synthetic data...\")try:    fine_tune_results = test_fine_tuning(        model,         config,         num_epochs=3,        batch_size=2,        learning_rate=5e-5    )    print(f\"\\nFinal Loss: {fine_tune_results['final_loss']:.4f}\")    print(f\"Best Loss: {fine_tune_results['best_loss']:.4f}\")    print(\"‚úÖ Fine-tuning complete\")except Exception as e:    print(f\"‚ö†Ô∏è Fine-tuning failed: {e}\")    import traceback    traceback.print_exc()print()# Test 2: Hyperparameter Search (OPTIONAL)print(\"Test 2/3: Hyperparameter Optimization\")print(\"-\" * 80)print(\"‚ö†Ô∏è Skipping hyperparameter search (compute-intensive)\")print(\"To enable: uncomment the code block below\")print()# Uncomment to run:# try:#     hp_results = test_hyperparameter_search(#         model,#         config,#         n_trials=5,#         epochs_per_trial=2#     )#     print(\"\\nBest Parameters:\")#     for param, value in hp_results['best_params'].items():#         print(f\"  {param}: {value}\")#     print(\"‚úÖ Hyperparameter search complete\")# except Exception as e:#     print(f\"‚ö†Ô∏è Hyperparameter search failed: {e}\")# Test 3: Benchmark Comparisonprint(\"Test 3/3: Benchmark Against Baseline\")print(\"-\" * 80)print(\"Comparing against distilgpt2 baseline...\")try:    benchmark_results = test_benchmark_comparison(        model,        config,        baseline_model=\"distilgpt2\",        n_samples=10    )    if benchmark_results is not None:        display(benchmark_results)    print(\"‚úÖ Benchmark comparison complete\")except Exception as e:    print(f\"‚ö†Ô∏è Benchmark comparison failed: {e}\")    import traceback    traceback.print_exc()print()print(\"=\" * 80)print(\"‚úÖ TRAINING UTILITIES COMPLETE\")print(\"=\" * 80)print()print(\"üéâ All training tests complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìà Perplexity (PPL) Guide\n\nPerplexity is exp(loss) and indicates how uncertain the model is about the next token.\n\n- Lower is better (PPL=1 is perfect).\n- PPL‚âàe (2.72) corresponds to loss‚âà1.0.\n- We clip loss at 20 for stability (exp(20)‚âà4.85e8).\n\nBaselines (approx., WikiText-103):\n\n- GPT-2 small: ~26\n- GPT-2 medium: ~19 | GPT-2 large: ~17\n\nInterpretation: \"Validation PPL=26\" ‚Üí model is ~26√ó uncertain about the next token.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
