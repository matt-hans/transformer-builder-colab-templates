{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start: Train a GPT-2 Style Model in 10 Minutes\n",
    "\n",
    "This notebook demonstrates the simplest possible workflow:\n",
    "1. Load a pre-built transformer model\n",
    "2. Train on WikiText-2 dataset\n",
    "3. Generate text samples\n",
    "\n",
    "**Hardware**: Works on Colab free tier (T4 GPU)\n",
    "\n",
    "**Time**: ~10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install dependencies and download utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch pytorch-lightning transformers datasets tokenizers\n",
    "\n",
    "# Download utils package\n",
    "!wget -q https://github.com/matt-hans/transformer-builder-colab-templates/archive/refs/heads/main.zip\n",
    "!unzip -q main.zip\n",
    "!mv transformer-builder-colab-templates-main/utils .\n",
    "!rm -rf transformer-builder-colab-templates-main main.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "For this example, we'll use a simple transformer model.\n",
    "Replace this with your model from Transformer Builder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Simple transformer for demonstration\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "# Create small GPT-2 config\n",
    "config = GPT2Config(\n",
    "    vocab_size=50257,\n",
    "    n_positions=512,\n",
    "    n_embd=512,\n",
    "    n_layer=6,\n",
    "    n_head=8,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "print(f\"Model created: {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "One function does everything:\n",
    "- Load WikiText-2 dataset\n",
    "- Create GPT-2 tokenizer (exact vocab match)\n",
    "- Train for 3 epochs\n",
    "- Save best checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training import train_model\n",
    "\n",
    "results = train_model(\n",
    "    model=model,\n",
    "    dataset='wikitext',\n",
    "    config_name='wikitext-2-raw-v1',\n",
    "    vocab_size=50257,\n",
    "    max_epochs=3,\n",
    "    batch_size=16,\n",
    "    learning_rate=1e-4\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Training complete!\")\n",
    "print(f\"Best checkpoint: {results['best_model_path']}\")\n",
    "print(f\"Final metrics: {results['final_metrics']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "\n",
    "Test the trained model with text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Get trained model\n",
    "trained_model = results['model'].model  # Extract from adapter\n",
    "trained_model.eval()\n",
    "\n",
    "# Generate text\n",
    "prompt = \"The transformer architecture\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = trained_model.generate(\n",
    "        input_ids,\n",
    "        max_length=100,\n",
    "        num_return_sequences=3,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Generated Samples\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for i, sample in enumerate(output, 1):\n",
    "    text = tokenizer.decode(sample, skip_special_tokens=True)\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've:\n",
    "- ✅ Trained a transformer model\n",
    "- ✅ Saved checkpoints automatically\n",
    "- ✅ Generated text samples\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Customize**: Adjust hyperparameters (epochs, batch size, learning rate)\n",
    "- **Use Your Model**: Replace the demo model with your Transformer Builder model\n",
    "- **Export**: See `04_model_export.ipynb` for ONNX/TorchScript export\n",
    "- **Advanced**: Check `03_large_scale_training.ipynb` for multi-GPU training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
