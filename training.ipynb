{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e697c2b",
   "metadata": {},
   "source": [
    "# üöÄ Transformer Training & Fine-Tuning Notebook\n",
    "\n",
    "**Professional ML Training Environment** for transformer models exported from [Transformer Builder](https://transformer-builder.com).\n",
    "\n",
    "## Quick Start Modes\n",
    "\n",
    "| Mode | Epochs | Time | Use Case |\n",
    "|------|--------|------|----------|\n",
    "| **‚ö° Fast** | 3 | ~5 min | Quick validation |\n",
    "| **‚öñÔ∏è Balanced** | 10 | ~15 min | Development |\n",
    "| **üíé Quality** | 20 | ~45 min | Production |\n",
    "\n",
    "## Features\n",
    "- ‚úÖ 5 Data Sources (HuggingFace, Drive, Upload, Local, Synthetic)\n",
    "- ‚úÖ Live Training Visualization\n",
    "- ‚úÖ Google Drive Checkpoints\n",
    "- ‚úÖ W&B + Local SQLite Tracking\n",
    "- ‚úÖ Hyperparameter Search\n",
    "- ‚úÖ Export & Comparison Tools\n",
    "\n",
    "**üìå Tip**: Run all cells in order for best results. Adjust hyperparameters in Section 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef71373",
   "metadata": {},
   "source": [
    "## üìã Table of Contents\n",
    "\n",
    "1. [Section 0: Quick Start](#section-0) ‚Üê You are here\n",
    "2. [Section 1: Setup & Drive Workspace](#section-1) (2 min)\n",
    "3. [Section 2: Model Loading](#section-2) (Load custom or example model)\n",
    "4. [Section 3: Data Loading](#section-3) (5 sources)\n",
    "5. [Section 4: Training Configuration](#section-4) (Hyperparameters)\n",
    "6. [Section 5: W&B Tracking Setup](#section-5) (Optional)\n",
    "7. [Section 6: Training Loop](#section-6) (Main training)\n",
    "8. [Section 7: Analysis & Visualization](#section-7) (Dashboards)\n",
    "9. [Section 8: Export & Results](#section-8) (Download checkpoints)\n",
    "10. [Section 9: Advanced Features](#section-9) (Hyperparameter search)\n",
    "\n",
    "‚è±Ô∏è **Total Time**: ~20-60 minutes depending on mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410215b4",
   "metadata": {},
   "source": [
    "## üì¶ Requirements\n",
    "\n",
    "This notebook requires:\n",
    "- Python >= 3.10\n",
    "- PyTorch (pre-installed in Colab)\n",
    "- Transformer Builder utilities (auto-downloaded)\n",
    "\n",
    "**GPU Recommended** but not required. Training will auto-detect and use GPU if available.\n",
    "\n",
    "---\n",
    "<a id=\"section-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c2f932",
   "metadata": {},
   "outputs": [],
   "source": "# Install training dependencies\n!pip install -q -r https://raw.githubusercontent.com/matt-hans/transformer-builder-colab-templates/main/requirements-training.txt\n\nprint(\"‚úÖ Dependencies installed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676a4b27",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nprint(\"üì• Downloading training utilities...\")\n\n# Remove old utils directory if exists\n!rm -rf utils/\n\n# Download complete utils package from GitHub\n!git clone --depth 1 --branch main https://github.com/matt-hans/transformer-builder-colab-templates.git temp_repo 2>/dev/null\n\n# Copy utils directory\n!cp -r temp_repo/utils ./\n\n# Cleanup\n!rm -rf temp_repo\n\n# Verify package structure\nutils_path = os.path.join(os.getcwd(), 'utils')\nif os.path.exists(utils_path):\n    print(f\"‚úÖ Utils package downloaded\")\n    \n    # Verify training subdirectory\n    training_path = os.path.join(utils_path, 'training')\n    if os.path.exists(training_path):\n        n_files = len([f for f in os.listdir(training_path) if f.endswith('.py')])\n        print(f\"‚úÖ Training utilities: {n_files} modules found\")\n    \n    # Verify tier3 utilities\n    tier3_path = os.path.join(utils_path, 'tier3_training_utilities.py')\n    if os.path.exists(tier3_path):\n        print(f\"‚úÖ Tier 3 training utilities ready\")\nelse:\n    print(\"‚ùå Failed to download utils package\")\n    raise RuntimeError(\"Could not download training utilities\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create workspace folders\n",
    "workspace_root = '/content/drive/MyDrive/TransformerTraining'\n",
    "os.makedirs(f'{workspace_root}/checkpoints', exist_ok=True)\n",
    "os.makedirs(f'{workspace_root}/configs', exist_ok=True)\n",
    "os.makedirs(f'{workspace_root}/results', exist_ok=True)\n",
    "os.makedirs(f'{workspace_root}/datasets', exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Workspace created at: {workspace_root}\")\n",
    "print(f\"   üìÅ checkpoints/ - Saved model weights\")\n",
    "print(f\"   üìÅ configs/ - Training configurations\")\n",
    "print(f\"   üìÅ results/ - Metrics, plots, dashboards\")\n",
    "print(f\"   üìÅ datasets/ - Cached datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c65122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training.experiment_db import ExperimentDB\n",
    "\n",
    "# Initialize local SQLite tracking (backup to W&B)\n",
    "db = ExperimentDB(f'{workspace_root}/experiments.db')\n",
    "\n",
    "print(\"‚úÖ Experiment database initialized\")\n",
    "print(f\"   Database: {workspace_root}/experiments.db\")\n",
    "print(f\"   Recent runs:\")\n",
    "recent_runs = db.list_runs(limit=5)\n",
    "if recent_runs:\n",
    "    print(recent_runs)\n",
    "else:\n",
    "    print(\"   (No previous runs found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-2\"></a>\n",
    "# üì¶ Section 2: Model Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your transformer model from Transformer Builder or use the example model.\n",
    "\n",
    "**Options:**\n",
    "- **Custom Model**: Provide Gist ID from Transformer Builder (auto-detected from URL)\n",
    "- **Example Model**: GPT-2 style architecture for testing\n",
    "\n",
    "**You will see:**\n",
    "1. Model code preview\n",
    "2. Architecture summary (layers, parameters, size)\n",
    "3. GPU compatibility check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üîó Model Source Configuration { display-mode: \"form\" }\n",
    "\n",
    "# Step 1: Try to extract from URL hash using JavaScript\n",
    "from google.colab import output\n",
    "import os\n",
    "import json\n",
    "\n",
    "# JavaScript to extract gist_id and model_name from URL hash\n",
    "js_code = \"\"\"\n",
    "(function() {\n",
    "    let gist_id = '';\n",
    "    let model_name = '';\n",
    "\n",
    "    try {\n",
    "        // Try to read URL hash from parent window (Colab embedding)\n",
    "        const hash = window.parent.location.hash || window.location.hash || '';\n",
    "\n",
    "        if (hash) {\n",
    "            // Parse hash parameters (e.g., #gist_id=abc123&name=MyModel)\n",
    "            const params = new URLSearchParams(hash.substring(1));\n",
    "            gist_id = params.get('gist_id') || '';\n",
    "            model_name = params.get('name') || '';\n",
    "\n",
    "            console.log('Extracted from URL hash:', {gist_id, model_name});\n",
    "        }\n",
    "    } catch (e) {\n",
    "        console.log('Could not access URL hash:', e);\n",
    "    }\n",
    "\n",
    "    // Return as JSON string\n",
    "    return JSON.stringify({gist_id: gist_id, model_name: model_name});\n",
    "})();\n",
    "\"\"\"\n",
    "\n",
    "# Execute JavaScript and get returned values\n",
    "try:\n",
    "    url_params_json = output.eval_js(js_code)\n",
    "    url_params = json.loads(url_params_json)\n",
    "    gist_id_from_url = url_params.get('gist_id', '')\n",
    "    model_name_from_url = url_params.get('model_name', '')\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not extract from URL hash: {e}\")\n",
    "    gist_id_from_url = ''\n",
    "    model_name_from_url = ''\n",
    "\n",
    "# Step 2: Manual input forms (as fallback)\n",
    "gist_id_manual = \"\"  #@param {type:\"string\"}\n",
    "model_name_manual = \"CustomTransformer\"  #@param {type:\"string\"}\n",
    "\n",
    "# Step 3: Environment variables (lowest priority)\n",
    "gist_id_env = os.getenv('GIST_ID', '')\n",
    "model_name_env = os.getenv('MODEL_NAME', '')\n",
    "\n",
    "# Step 4: Determine final values (URL > Manual > Env)\n",
    "gist_id = gist_id_from_url or gist_id_manual or gist_id_env\n",
    "model_name = model_name_from_url or model_name_manual or model_name_env or 'CustomTransformer'\n",
    "\n",
    "# Display source\n",
    "print(\"=\"*60)\n",
    "if gist_id:\n",
    "    source = \"URL hash\" if gist_id_from_url else (\"Manual input\" if gist_id_manual else \"Environment variable\")\n",
    "    print(f\"‚úÖ Model Source: {source}\")\n",
    "    print(f\"   Gist ID: {gist_id}\")\n",
    "    print(f\"   Model Name: {model_name}\")\n",
    "    print(f\"\\n   Loading custom model from Transformer Builder...\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No Gist ID provided\")\n",
    "    print(\"   Options to provide Gist ID:\")\n",
    "    print(\"   1. Open via Transformer Builder link (auto-detects from URL)\")\n",
    "    print(\"   2. Enter Gist ID in the form above\")\n",
    "    print(\"   3. Set GIST_ID environment variable\")\n",
    "    print(\"\\n   Proceeding with example model for demonstration...\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üì¶ Load Model from Gist { display-mode: \"form\" }\n",
    "\n",
    "import urllib.request\n",
    "import json\n",
    "import sys\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL LOADING\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# ==============================================================================\n",
    "# VERIFY GIST ID WAS PROVIDED\n",
    "# ==============================================================================\n",
    "\n",
    "if 'gist_id' not in globals() or not gist_id:\n",
    "    print(\"‚ùå ERROR: No Gist ID found!\")\n",
    "    print()\n",
    "    print(\"==\" * 35)\n",
    "    print(\"üîô GO BACK TO PREVIOUS CELL\")\n",
    "    print(\"==\" * 35)\n",
    "    print()\n",
    "    print(\"You must run the Model Source Configuration cell first.\")\n",
    "    print()\n",
    "    raise ValueError(\"Gist ID required - run previous cell first\")\n",
    "\n",
    "print(f\"üì• Loading model from GitHub Gist: {gist_id}\")\n",
    "print()\n",
    "\n",
    "# ==============================================================================\n",
    "# FETCH GIST AND LOAD MODEL FILES - GitHub API Approach\n",
    "# ==============================================================================\n",
    "\n",
    "def _fetch_gist(gid: str) -> dict:\n",
    "    \"\"\"Fetch Gist data from GitHub API.\"\"\"\n",
    "    url = f\"https://api.github.com/gists/{gid}\"\n",
    "    req = urllib.request.Request(url, headers={\n",
    "        \"Accept\": \"application/vnd.github+json\",\n",
    "        \"User-Agent\": \"transformer-builder-colab\"\n",
    "    })\n",
    "    try:\n",
    "        with urllib.request.urlopen(req, timeout=20) as resp:\n",
    "            return json.loads(resp.read().decode(\"utf-8\"))\n",
    "    except urllib.error.HTTPError as e:\n",
    "        detail = f\"HTTP {e.code}\"\n",
    "        try:\n",
    "            body = e.read().decode(\"utf-8\")\n",
    "            if \"rate limit\" in body.lower():\n",
    "                detail += \" - GitHub API rate limit (try again in an hour)\"\n",
    "            elif e.code == 404:\n",
    "                detail += \" - Gist not found (check your Gist ID)\"\n",
    "        except:\n",
    "            pass\n",
    "        raise RuntimeError(f\"GitHub API error: {detail}\") from e\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Network error: {e}\") from e\n",
    "\n",
    "def _write(path: str, text: str):\n",
    "    \"\"\"Write text to file.\"\"\"\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "# Fetch Gist\n",
    "try:\n",
    "    gist_data = _fetch_gist(gist_id)\n",
    "    files = gist_data.get(\"files\") or {}\n",
    "\n",
    "    # Check for required files\n",
    "    if \"model.py\" not in files:\n",
    "        raise RuntimeError(\"Gist is missing 'model.py' - please re-export from Transformer Builder\")\n",
    "    if \"config.json\" not in files:\n",
    "        raise RuntimeError(\"Gist is missing 'config.json' - please re-export from Transformer Builder\")\n",
    "\n",
    "    model_code = files[\"model.py\"].get(\"content\", \"\")\n",
    "    config_json = files[\"config.json\"].get(\"content\", \"\")\n",
    "\n",
    "    if not model_code or not config_json:\n",
    "        raise RuntimeError(\"Empty content in model.py or config.json\")\n",
    "\n",
    "    # Write to files\n",
    "    _write(\"model.py\", model_code)\n",
    "    _write(\"config.json\", config_json)\n",
    "\n",
    "    print(f\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"‚úÖ Gist URL: {gist_data.get('html_url', 'N/A')}\")\n",
    "    print(f\"‚úÖ Model code: {len(model_code):,} bytes\")\n",
    "    print(f\"‚úÖ Config: {len(config_json):,} bytes\")\n",
    "    print()\n",
    "\n",
    "    # Parse model name from config if available\n",
    "    try:\n",
    "        model_config = json.loads(config_json)\n",
    "        if 'model_name' in model_config:\n",
    "            model_name = model_config['model_name']\n",
    "            print(f\"‚úÖ Model name: {model_name}\")\n",
    "        else:\n",
    "            model_name = 'CustomTransformer'\n",
    "            print(f\"‚ÑπÔ∏è  Using default name: {model_name}\")\n",
    "        print()\n",
    "    except:\n",
    "        model_name = 'CustomTransformer'\n",
    "        print(f\"‚ö†Ô∏è  Could not parse config, using default name: {model_name}\")\n",
    "\n",
    "    # Store for next cell\n",
    "    gist_loaded = True\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load model from Gist!\")\n",
    "    print()\n",
    "    print(f\"Error: {e}\")\n",
    "    print()\n",
    "    print(\"=\" * 70)\n",
    "    print(\"TROUBLESHOOTING\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(\"Common issues:\")\n",
    "    print(\"  1. Check your Gist ID is correct (go back to previous cell)\")\n",
    "    print(\"  2. Ensure you exported from Transformer Builder successfully\")\n",
    "    print(\"  3. Check you're not hitting GitHub rate limit (60 requests/hour)\")\n",
    "    print(\"  4. Try re-exporting from Transformer Builder\")\n",
    "    print()\n",
    "    print(\"If the problem persists:\")\n",
    "    print(f\"  ‚Ä¢ Gist URL: https://gist.github.com/{gist_id}\")\n",
    "    print(\"  ‚Ä¢ Verify the Gist contains model.py and config.json\")\n",
    "    print()\n",
    "\n",
    "    # Fallback to example model\n",
    "    print(\"‚ö†Ô∏è  Falling back to example model for demonstration...\")\n",
    "    gist_loaded = False\n",
    "    model_name = 'ExampleTransformer'\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ MODEL LOADING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Model will be instantiated in the next cell.\")\n",
    "print()\n",
    "\n",
    "# Display downloaded model code preview\n",
    "if gist_loaded:\n",
    "    print(\"\\nüìÑ Model Code Preview:\")\n",
    "    print(\"=\" * 60)\n",
    "    with open('model.py', 'r') as f:\n",
    "        model_lines = f.read().split('\\n')\n",
    "        # Show first 20 lines\n",
    "        for i, line in enumerate(model_lines[:20], 1):\n",
    "            print(f\"{i:3d} | {line}\")\n",
    "        if len(model_lines) > 20:\n",
    "            print(f\"... ({len(model_lines) - 20} more lines)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Model: {model_name}\")\n",
    "if gist_loaded:\n",
    "    print(f\"   Config: {json.dumps(model_config, indent=2)}\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title üöÄ Initialize Model { display-mode: \"form\" }\n\nimport torch\nimport torch.nn as nn\nimport inspect\nfrom types import SimpleNamespace\n\n# Detect device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"üñ•Ô∏è  Device: {device}\")\n\nif torch.cuda.is_available():\n    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\n# Create model instance\nif gist_loaded:\n    # Custom model from Transformer Builder\n    # Import the model from downloaded file\n    try:\n        sys.path.insert(0, '.')\n\n        # Import all classes from model.py\n        import importlib.util\n        spec = importlib.util.spec_from_file_location(\"custom_model\", \"model.py\")\n        custom_model_module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(custom_model_module)\n\n        # Find the model class\n        model_class = None\n        for name, obj in vars(custom_model_module).items():\n            if isinstance(obj, type) and issubclass(obj, nn.Module) and obj is not nn.Module:\n                if name == model_name:\n                    model_class = obj\n                    break\n        \n        if model_class is None:\n            # Fallback: find any nn.Module subclass\n            for name, obj in vars(custom_model_module).items():\n                if isinstance(obj, type) and issubclass(obj, nn.Module) and obj is not nn.Module:\n                    model_class = obj\n                    print(f\"‚ö†Ô∏è Using {name} (expected {model_name})\")\n                    break\n        \n        if model_class:\n            # Check constructor signature (KEY FIX from template.ipynb)\n            sig = inspect.signature(model_class.__init__)\n            params_list = [p for p in sig.parameters.values() if p.name != 'self']\n            \n            if len(params_list) == 0:\n                # Parameterless constructor (Transformer Builder models)\n                print(\"‚ÑπÔ∏è  Model has parameterless constructor (Transformer Builder export)\")\n                model = model_class()\n            else:\n                # Parameterized constructor (traditional models)\n                print(f\"‚ÑπÔ∏è  Model accepts {len(params_list)} parameter(s)\")\n                model = model_class(**model_config)\n            \n            print(f\"‚úÖ Custom model instantiated: {model.__class__.__name__}\")\n        else:\n            raise Exception(\"No model class found in model.py\")\n\n    except Exception as e:\n        print(f\"‚ùå Failed to instantiate custom model: {e}\")\n        print(\"   Falling back to example model...\")\n        gist_loaded = False\n\nif not gist_loaded:\n    # Example model (fallback)\n    print(\"üì¶ Loading example model (GPT-2 architecture)...\")\n\n    class ExampleTransformer(nn.Module):\n        \"\"\"Example GPT-2 style transformer for demonstration.\"\"\"\n\n        def __init__(self, vocab_size=50257, d_model=768, n_layers=12, n_heads=12, max_seq_len=1024):\n            super().__init__()\n            self.vocab_size = vocab_size\n            self.d_model = d_model\n            self.n_layers = n_layers\n            self.n_heads = n_heads\n            self.max_seq_len = max_seq_len\n\n            self.embedding = nn.Embedding(vocab_size, d_model)\n            self.position_embedding = nn.Embedding(max_seq_len, d_model)\n\n            # Simple transformer layers\n            self.layers = nn.ModuleList([\n                nn.TransformerEncoderLayer(\n                    d_model,\n                    n_heads,\n                    dim_feedforward=d_model*4,\n                    batch_first=True,\n                    dropout=0.1\n                )\n                for _ in range(n_layers)\n            ])\n\n            self.ln_f = nn.LayerNorm(d_model)\n            self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n\n        def forward(self, input_ids):\n            batch_size, seq_len = input_ids.shape\n\n            # Embeddings\n            token_emb = self.embedding(input_ids)\n            pos_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n            pos_emb = self.position_embedding(pos_ids)\n\n            x = token_emb + pos_emb\n\n            # Transformer layers\n            for layer in self.layers:\n                x = layer(x)\n\n            x = self.ln_f(x)\n            logits = self.lm_head(x)\n\n            return logits\n\n    # Create example model\n    model = ExampleTransformer()\n    model_config = {\n        'vocab_size': 50257,\n        'd_model': 768,\n        'n_layers': 12,\n        'n_heads': 12,\n        'max_seq_len': 1024\n    }\n\n    print(f\"‚úÖ Example model definition loaded\")\n\n# Move to device\nmodel = model.to(device)\n\n# Model summary\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"\\n‚úÖ Model initialized on {device}\")\nprint(f\"   Total parameters: {total_params:,}\")\nprint(f\"   Trainable parameters: {trainable_params:,}\")\nprint(f\"   Model size: {total_params * 4 / 1e6:.1f} MB (fp32)\")\n\n# Create config object for training utilities\nconfig_obj = SimpleNamespace(**model_config)\nif not hasattr(config_obj, 'vocab_size'):\n    config_obj.vocab_size = model_config.get('vocab_size', 50257)\nif not hasattr(config_obj, 'max_seq_len'):\n    config_obj.max_seq_len = model_config.get('max_seq_len', 1024)\n\nprint(f\"\\nüéØ Ready for training!\")\nprint(f\"\\n‚ÑπÔ∏è  Note: Update Section 4 training config before starting training loop.\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "5fc17228",
   "metadata": {},
   "source": [
    "<a id=\"section-3\"></a>\n",
    "# üìä Section 3: Data Loading\n",
    "\n",
    "Choose your data source (run ONE of the following cells):\n",
    "- **Option 1**: HuggingFace Datasets (recommended)\n",
    "- **Option 2**: Google Drive Upload\n",
    "- **Option 3**: File Upload (small datasets)\n",
    "- **Option 4**: Local Files (from previous sessions)\n",
    "- **Option 5**: Synthetic Data (testing only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3e6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# CONFIGURATION: Edit dataset name\n",
    "dataset_name = \"wikitext\"  #@param {type:\"string\"}\n",
    "config_name = \"wikitext-2-raw-v1\"  #@param {type:\"string\"}\n",
    "max_samples = 1000  #@param {type:\"integer\"}\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(dataset_name, config_name)\n",
    "train_data = dataset['train'].select(range(min(max_samples, len(dataset['train']))))\n",
    "val_data = dataset['validation'].select(range(min(100, len(dataset['validation']))))\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(train_data)} training samples, {len(val_data)} validation samples\")\n",
    "print(f\"   Example: {train_data[0]}\")\n",
    "\n",
    "data_source = \"huggingface\"\n",
    "dataset_info = {'name': dataset_name, 'config': config_name, 'train_size': len(train_data), 'val_size': len(val_data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e417890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "drive_data_path = \"/content/drive/MyDrive/TransformerTraining/datasets/my_data.txt\"  #@param {type:\"string\"}\n",
    "\n",
    "if os.path.exists(drive_data_path):\n",
    "    with open(drive_data_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    split_idx = int(0.9 * len(lines))\n",
    "    train_data = [line.strip() for line in lines[:split_idx]]\n",
    "    val_data = [line.strip() for line in lines[split_idx:]]\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(train_data)} training samples, {len(val_data)} validation samples\")\n",
    "    data_source = \"google_drive\"\n",
    "    dataset_info = {'path': drive_data_path, 'train_size': len(train_data), 'val_size': len(val_data)}\n",
    "else:\n",
    "    print(f\"‚ùå File not found: {drive_data_path}\")\n",
    "    print(\"   Please upload your data to Google Drive first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366269e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "# Upload file\n",
    "uploaded = files.upload()\n",
    "\n",
    "if uploaded:\n",
    "    filename = list(uploaded.keys())[0]\n",
    "    content = uploaded[filename].decode('utf-8')\n",
    "    lines = content.split('\\n')\n",
    "\n",
    "    split_idx = int(0.9 * len(lines))\n",
    "    train_data = [line.strip() for line in lines[:split_idx]]\n",
    "    val_data = [line.strip() for line in lines[split_idx:]]\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(train_data)} training samples, {len(val_data)} validation samples\")\n",
    "    data_source = \"file_upload\"\n",
    "    dataset_info = {'filename': filename, 'train_size': len(train_data), 'val_size': len(val_data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a4882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "cache_path = f'{workspace_root}/datasets/cached_data.pkl'\n",
    "\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    train_data = data['train']\n",
    "    val_data = data['val']\n",
    "\n",
    "    print(f\"‚úÖ Loaded cached data: {len(train_data)} train, {len(val_data)} val\")\n",
    "    data_source = \"cached\"\n",
    "    dataset_info = {'path': cache_path, 'train_size': len(train_data), 'val_size': len(val_data)}\n",
    "else:\n",
    "    print(f\"‚ùå No cached data found at {cache_path}\")\n",
    "    print(\"   Run one of the other data loading options first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7865100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Generate synthetic data for testing\n",
    "vocab_size = 50257  # GPT-2 vocab\n",
    "seq_len = 32\n",
    "n_samples = 100\n",
    "\n",
    "train_data = [torch.randint(0, vocab_size, (seq_len,)) for _ in range(n_samples)]\n",
    "val_data = [torch.randint(0, vocab_size, (seq_len,)) for _ in range(20)]\n",
    "\n",
    "print(f\"‚úÖ Generated {len(train_data)} synthetic training samples\")\n",
    "print(f\"   ‚ö†Ô∏è Warning: Synthetic data is for testing only\")\n",
    "data_source = \"synthetic\"\n",
    "dataset_info = {'vocab_size': vocab_size, 'seq_len': seq_len, 'train_size': len(train_data), 'val_size': len(val_data)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56295914",
   "metadata": {},
   "source": [
    "<a id=\"section-4\"></a>\n",
    "# ‚öôÔ∏è Section 4: Training Configuration\n",
    "\n",
    "Configure hyperparameters using Colab forms below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269a022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training.training_config import TrainingConfig\n",
    "\n",
    "# HYPERPARAMETERS (edit via forms)\n",
    "learning_rate = 5e-5  #@param {type:\"number\"}\n",
    "batch_size = 4  #@param {type:\"integer\"}\n",
    "epochs = 10  #@param {type:\"integer\"}\n",
    "warmup_ratio = 0.1  #@param {type:\"number\"}\n",
    "weight_decay = 0.01  #@param {type:\"number\"}\n",
    "gradient_clip_norm = 1.0  #@param {type:\"number\"}\n",
    "\n",
    "# TRAINING FEATURES\n",
    "use_amp = True  #@param {type:\"boolean\"}\n",
    "gradient_accumulation_steps = 1  #@param {type:\"integer\"}\n",
    "deterministic = False  #@param {type:\"boolean\"}\n",
    "\n",
    "# EXPERIMENT\n",
    "run_name = \"training-run\"  #@param {type:\"string\"}\n",
    "random_seed = 42  #@param {type:\"integer\"}\n",
    "\n",
    "# Create config\n",
    "config = TrainingConfig(\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    weight_decay=weight_decay,\n",
    "    max_grad_norm=gradient_clip_norm,\n",
    "    use_amp=use_amp,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    deterministic=deterministic,\n",
    "    random_seed=random_seed,\n",
    "    run_name=run_name\n",
    ")\n",
    "\n",
    "# Validate\n",
    "config.validate()\n",
    "\n",
    "# Save to Drive\n",
    "config_path = config.save(f'{workspace_root}/configs/')\n",
    "print(f\"‚úÖ Config saved: {config_path}\")\n",
    "print(f\"\\n{config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b568df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display configuration summary\n",
    "print(\"=\" * 60)\n",
    "print(\" \" * 15 + \"TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Run Name:':<25} {config.run_name}\")\n",
    "print(f\"{'Learning Rate:':<25} {config.learning_rate}\")\n",
    "print(f\"{'Batch Size (effective):':<25} {config.batch_size * config.gradient_accumulation_steps}\")\n",
    "print(f\"{'Epochs:':<25} {config.epochs}\")\n",
    "print(f\"{'Warmup Ratio:':<25} {config.warmup_ratio}\")\n",
    "print(f\"{'Gradient Clipping:':<25} {config.max_grad_norm}\")\n",
    "print(f\"{'AMP Enabled:':<25} {config.use_amp}\")\n",
    "print(f\"{'Deterministic:':<25} {config.deterministic}\")\n",
    "print(f\"{'Random Seed:':<25} {config.random_seed}\")\n",
    "print(f\"{'Data Source:':<25} {data_source}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5e4445",
   "metadata": {},
   "source": [
    "### Training Mode Selection\n",
    "\n",
    "Based on your `epochs` setting:\n",
    "- **epochs <= 5**: ‚ö° Fast Mode (~5 min)\n",
    "- **epochs <= 15**: ‚öñÔ∏è Balanced Mode (~15 min)\n",
    "- **epochs > 15**: üíé Quality Mode (45+ min)\n",
    "\n",
    "Proceed to training in Section 5 ‚¨áÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d46ead6",
   "metadata": {},
   "source": [
    "<a id=\"section-5\"></a>\n",
    "# üî¨ Section 5: W&B Tracking Setup (Optional)\n",
    "\n",
    "Enable Weights & Biases for cloud-based experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae42ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from getpass import getpass\n",
    "\n",
    "use_wandb = True  #@param {type:\"boolean\"}\n",
    "wandb_project = \"transformer-training\"  #@param {type:\"string\"}\n",
    "wandb_entity = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "if use_wandb:\n",
    "    # Login to W&B\n",
    "    wandb_key = getpass(\"Enter W&B API key (or leave blank to skip): \")\n",
    "    if wandb_key:\n",
    "        wandb.login(key=wandb_key)\n",
    "\n",
    "        # Initialize run\n",
    "        wandb.init(\n",
    "            project=wandb_project,\n",
    "            entity=wandb_entity if wandb_entity else None,\n",
    "            name=config.run_name,\n",
    "            config=config.to_dict(),\n",
    "            tags=[data_source, f\"epochs_{epochs}\"]\n",
    "        )\n",
    "        print(f\"‚úÖ W&B initialized: {wandb.run.url}\")\n",
    "    else:\n",
    "        use_wandb = False\n",
    "        print(\"‚ö†Ô∏è W&B skipped - training will use local tracking only\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è W&B disabled - using local SQLite tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ce57e5",
   "metadata": {},
   "source": [
    "<a id=\"section-6\"></a>\n",
    "# üèãÔ∏è Section 6: Training Loop\n",
    "\n",
    "Main training loop with live visualization and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c98ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training.metrics_tracker import MetricsTracker\n",
    "from utils.training.live_plotting import LivePlotter\n",
    "from utils.training.seed_manager import set_random_seed\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set random seed\n",
    "set_random_seed(config.random_seed, config.deterministic)\n",
    "\n",
    "# Initialize metrics tracker\n",
    "tracker = MetricsTracker(use_wandb=use_wandb)\n",
    "\n",
    "# Initialize live plotter\n",
    "plotter = LivePlotter(update_interval=1)\n",
    "\n",
    "# Create DataLoader (simplified - adapt to your data format)\n",
    "if data_source == \"synthetic\":\n",
    "    train_dataset = TensorDataset(torch.stack(train_data))\n",
    "    val_dataset = TensorDataset(torch.stack(val_data))\n",
    "else:\n",
    "    # For HuggingFace datasets or text data, you'll need proper tokenization\n",
    "    print(\"‚ö†Ô∏è Using synthetic data - implement proper tokenization for real datasets\")\n",
    "    train_dataset = TensorDataset(torch.stack([torch.randint(0, 50257, (32,)) for _ in range(100)]))\n",
    "    val_dataset = TensorDataset(torch.stack([torch.randint(0, 50257, (32,)) for _ in range(20)]))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "# Learning rate scheduler (warmup + cosine decay)\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=config.learning_rate,\n",
    "    epochs=config.epochs,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=config.warmup_ratio\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training initialized\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import time\n",
    "\n",
    "# Initialize gradient scaler for AMP\n",
    "scaler = GradScaler(enabled=config.use_amp)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(config.epochs):\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_idx, (input_ids,) in enumerate(train_loader):\n",
    "        input_ids = input_ids.to(device)\n",
    "\n",
    "        # Forward pass with AMP\n",
    "        with autocast(enabled=config.use_amp):\n",
    "            # Shift for language modeling: predict next token\n",
    "            logits = model(input_ids[:, :-1])\n",
    "            targets = input_ids[:, 1:]\n",
    "\n",
    "            # Compute loss\n",
    "            loss = F.cross_entropy(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                targets.reshape(-1)\n",
    "            )\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        if config.max_grad_norm is not None:\n",
    "            scaler.unscale_(optimizer)\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "        else:\n",
    "            grad_norm = 0.0\n",
    "\n",
    "        # Optimizer step\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Log batch metrics\n",
    "        global_step = epoch * len(train_loader) + batch_idx\n",
    "        tracker.log_scalar('train/batch_loss', loss.item(), step=global_step)\n",
    "        tracker.log_scalar('train/learning_rate', scheduler.get_last_lr()[0], step=global_step)\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{config.epochs} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, in val_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "\n",
    "            with autocast(enabled=config.use_amp):\n",
    "                logits = model(input_ids[:, :-1])\n",
    "                targets = input_ids[:, 1:]\n",
    "                loss = F.cross_entropy(\n",
    "                    logits.reshape(-1, logits.size(-1)),\n",
    "                    targets.reshape(-1)\n",
    "                )\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Compute epoch metrics\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "\n",
    "    # Log epoch metrics\n",
    "    tracker.log_epoch(\n",
    "        epoch=epoch,\n",
    "        train_metrics={'loss': avg_train_loss},\n",
    "        val_metrics={'loss': avg_val_loss, 'perplexity': torch.exp(torch.tensor(avg_val_loss)).item()},\n",
    "        learning_rate=scheduler.get_last_lr()[0],\n",
    "        gradient_norm=grad_norm if isinstance(grad_norm, float) else grad_norm.item(),\n",
    "        epoch_duration=epoch_time\n",
    "    )\n",
    "\n",
    "    # Update live plot\n",
    "    plotter.update(tracker.get_summary())\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % 5 == 0 or epoch == config.epochs - 1:\n",
    "        checkpoint_path = f\"{workspace_root}/checkpoints/{config.run_name}_epoch{epoch+1}.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'config': config.to_dict()\n",
    "        }, checkpoint_path)\n",
    "        print(f\"üíæ Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{config.epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Time: {epoch_time:.1f}s\")\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")\n",
    "\n",
    "# Save experiment to database\n",
    "db.save_run(\n",
    "    run_name=config.run_name,\n",
    "    config=config.to_dict(),\n",
    "    metrics=tracker.get_summary().to_dict('records')[-1],\n",
    "    data_source=data_source\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd41698",
   "metadata": {},
   "source": [
    "<a id=\"section-7\"></a>\n",
    "# üìà Section 7: Analysis & Visualization\n",
    "\n",
    "Analyze training results with comprehensive dashboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a6e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training.dashboard import TrainingDashboard\n",
    "\n",
    "# Create comprehensive 6-panel dashboard\n",
    "metrics_df = tracker.get_summary()\n",
    "dashboard = TrainingDashboard(figsize=(18, 12))\n",
    "\n",
    "fig = dashboard.plot(\n",
    "    metrics_df,\n",
    "    config=config,\n",
    "    title=f\"Training Dashboard: {config.run_name}\"\n",
    ")\n",
    "\n",
    "# Save to Drive\n",
    "dashboard_path = f'{workspace_root}/results/{config.run_name}_dashboard.png'\n",
    "dashboard.save(dashboard_path, dpi=150)\n",
    "print(f\"‚úÖ Dashboard saved to Drive: {dashboard_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d96e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best epoch based on validation loss\n",
    "best_epoch_idx = metrics_df['val/loss'].idxmin()\n",
    "best_epoch = metrics_df.loc[best_epoch_idx]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\" \" * 20 + \"BEST EPOCH ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Best Epoch:':<25} {int(best_epoch['epoch']) + 1}\")\n",
    "print(f\"{'Validation Loss:':<25} {best_epoch['val/loss']:.4f}\")\n",
    "print(f\"{'Validation Perplexity:':<25} {best_epoch['val/perplexity']:.2f}\")\n",
    "print(f\"{'Training Loss:':<25} {best_epoch['train/loss']:.4f}\")\n",
    "print(f\"{'Learning Rate:':<25} {best_epoch['train/learning_rate']:.2e}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load best checkpoint\n",
    "best_checkpoint_path = f\"{workspace_root}/checkpoints/{config.run_name}_epoch{int(best_epoch['epoch']) + 1}.pt\"\n",
    "if os.path.exists(best_checkpoint_path):\n",
    "    print(f\"\\nüíæ Best checkpoint: {best_checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Best checkpoint not found (may not have been saved)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metrics table\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.4f}')\n",
    "\n",
    "display_cols = ['epoch', 'train/loss', 'val/loss', 'val/perplexity', 'train/learning_rate']\n",
    "available_cols = [col for col in display_cols if col in metrics_df.columns]\n",
    "\n",
    "print(\"\\nTraining Metrics Summary:\")\n",
    "print(metrics_df[available_cols].to_string(index=False))\n",
    "\n",
    "# Export to CSV\n",
    "csv_path = f'{workspace_root}/results/{config.run_name}_metrics.csv'\n",
    "metrics_df.to_csv(csv_path, index=False)\n",
    "print(f\"\\n‚úÖ Metrics exported to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe5404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"=\" * 60)\n",
    "    print(\" \" * 20 + \"GPU METRICS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    gpu_cols = [col for col in metrics_df.columns if col.startswith('gpu/')]\n",
    "    if gpu_cols:\n",
    "        print(metrics_df[['epoch'] + gpu_cols].tail(5).to_string(index=False))\n",
    "\n",
    "        # Plot GPU utilization\n",
    "        import matplotlib.pyplot as plt\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "        if 'gpu/memory_allocated_mb' in metrics_df.columns:\n",
    "            ax1.plot(metrics_df['epoch'], metrics_df['gpu/memory_allocated_mb'])\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('GPU Memory (MB)')\n",
    "            ax1.set_title('GPU Memory Usage')\n",
    "            ax1.grid(True)\n",
    "\n",
    "        if 'gpu/utilization_percent' in metrics_df.columns:\n",
    "            ax2.plot(metrics_df['epoch'], metrics_df['gpu/utilization_percent'])\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('GPU Utilization (%)')\n",
    "            ax2.set_title('GPU Utilization')\n",
    "            ax2.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{workspace_root}/results/{config.run_name}_gpu_metrics.png', dpi=100)\n",
    "        plt.show()\n",
    "        print(f\"\\n‚úÖ GPU metrics saved\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No GPU metrics collected during training\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Training was performed on CPU (no GPU metrics available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6fdf23",
   "metadata": {},
   "source": [
    "<a id=\"section-8\"></a>\n",
    "# üíæ Section 8: Export & Results\n",
    "\n",
    "Download checkpoints, configs, and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a039396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\" \" * 20 + \"EXPORT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìÅ Workspace: {workspace_root}\")\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"   - Dashboard: {config.run_name}_dashboard.png\")\n",
    "print(f\"   - Metrics CSV: {config.run_name}_metrics.csv\")\n",
    "print(f\"   - Config: {os.path.basename(config_path)}\")\n",
    "print(f\"\\nüíæ Checkpoints:\")\n",
    "\n",
    "checkpoint_dir = f\"{workspace_root}/checkpoints\"\n",
    "checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(config.run_name)]\n",
    "for ckpt in sorted(checkpoints):\n",
    "    ckpt_path = os.path.join(checkpoint_dir, ckpt)\n",
    "    size_mb = os.path.getsize(ckpt_path) / (1024 * 1024)\n",
    "    print(f\"   - {ckpt} ({size_mb:.1f} MB)\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1656431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results to local machine\n",
    "download_results = False  #@param {type:\"boolean\"}\n",
    "\n",
    "if download_results:\n",
    "    print(\"Downloading files...\")\n",
    "\n",
    "    # Download dashboard\n",
    "    dashboard_file = f'{workspace_root}/results/{config.run_name}_dashboard.png'\n",
    "    if os.path.exists(dashboard_file):\n",
    "        files.download(dashboard_file)\n",
    "\n",
    "    # Download metrics CSV\n",
    "    metrics_file = f'{workspace_root}/results/{config.run_name}_metrics.csv'\n",
    "    if os.path.exists(metrics_file):\n",
    "        files.download(metrics_file)\n",
    "\n",
    "    # Download config\n",
    "    if os.path.exists(config_path):\n",
    "        files.download(config_path)\n",
    "\n",
    "    # Download best checkpoint\n",
    "    if os.path.exists(best_checkpoint_path):\n",
    "        files.download(best_checkpoint_path)\n",
    "        print(f\"‚úÖ Downloaded {os.path.basename(best_checkpoint_path)}\")\n",
    "\n",
    "    print(\"‚úÖ Downloads complete\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Downloads skipped. Files are saved in Google Drive.\")\n",
    "    print(f\"   Access them at: {workspace_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f00c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with previous runs\n",
    "all_runs = db.list_runs(limit=10)\n",
    "\n",
    "if len(all_runs) > 1:\n",
    "    print(\"=\" * 60)\n",
    "    print(\" \" * 15 + \"COMPARISON WITH PREVIOUS RUNS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    comparison_data = []\n",
    "    for run in all_runs:\n",
    "        comparison_data.append({\n",
    "            'run_name': run.get('run_name', 'unknown'),\n",
    "            'final_val_loss': run.get('metrics', {}).get('val/loss', float('nan')),\n",
    "            'final_perplexity': run.get('metrics', {}).get('val/perplexity', float('nan')),\n",
    "            'data_source': run.get('data_source', 'unknown'),\n",
    "            'timestamp': run.get('timestamp', 'unknown')\n",
    "        })\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No previous runs to compare (this is your first run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a901c0",
   "metadata": {},
   "source": [
    "<a id=\"section-9\"></a>\n",
    "# üî¨ Section 9: Advanced Features\n",
    "\n",
    "Hyperparameter search, multi-run experiments, and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f3d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tier3_training_utilities import test_hyperparameter_search\n",
    "\n",
    "# Hyperparameter search configuration\n",
    "run_hp_search = False  #@param {type:\"boolean\"}\n",
    "n_trials = 10  #@param {type:\"integer\"}\n",
    "search_timeout = 3600  #@param {type:\"integer\"}\n",
    "\n",
    "if run_hp_search:\n",
    "    print(\"üîç Starting hyperparameter search...\")\n",
    "    print(f\"   Trials: {n_trials}\")\n",
    "    print(f\"   Timeout: {search_timeout}s ({search_timeout/60:.1f} min)\")\n",
    "    print(\"\\n‚ö†Ô∏è This may take a while. Progress will be shown below.\")\n",
    "\n",
    "    # Define search space\n",
    "    search_space = {\n",
    "        'learning_rate': (1e-5, 1e-3),\n",
    "        'batch_size': [4, 8, 16],\n",
    "        'warmup_ratio': (0.0, 0.2),\n",
    "        'weight_decay': (0.0, 0.1)\n",
    "    }\n",
    "\n",
    "    print(f\"\\nSearch space: {search_space}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Hyperparameter search disabled\")\n",
    "    print(\"   Set 'run_hp_search = True' to enable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19ee7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_hp_search:\n",
    "    # Run search\n",
    "    hp_results = test_hyperparameter_search(\n",
    "        model=model,\n",
    "        config=config,\n",
    "        train_data=train_data,\n",
    "        val_data=val_data,\n",
    "        n_trials=n_trials,\n",
    "        timeout=search_timeout,\n",
    "        use_wandb=use_wandb\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\" \" * 15 + \"HYPERPARAMETER SEARCH RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nBest parameters:\")\n",
    "    for param, value in hp_results['best_params'].items():\n",
    "        print(f\"   {param}: {value}\")\n",
    "\n",
    "    print(f\"\\nBest validation loss: {hp_results['best_value']:.4f}\")\n",
    "    print(f\"\\nAll trials:\")\n",
    "    print(hp_results['trials_df'].to_string(index=False))\n",
    "\n",
    "    # Save results\n",
    "    hp_results['trials_df'].to_csv(\n",
    "        f'{workspace_root}/results/{config.run_name}_hp_search.csv',\n",
    "        index=False\n",
    "    )\n",
    "    print(f\"\\n‚úÖ Results saved to: {config.run_name}_hp_search.csv\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Hyperparameter search skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63affe7",
   "metadata": {},
   "source": "## üéâ Training Complete!\n\n### Next Steps\n\n1. **Review Results**: Check the dashboard in Section 6\n2. **Download Files**: Use Section 7 to download checkpoints\n3. **Compare Runs**: See Section 7 for comparison with previous experiments\n4. **Optimize**: Try hyperparameter search in Section 8\n\n### Workspace Structure\n\nAll files are saved in Google Drive:\n```\n/content/drive/MyDrive/TransformerTraining/\n‚îú‚îÄ‚îÄ checkpoints/     # Model weights (.pt files)\n‚îú‚îÄ‚îÄ configs/         # Training configs (.json files)\n‚îú‚îÄ‚îÄ results/         # Dashboards, metrics, plots\n‚îú‚îÄ‚îÄ datasets/        # Cached datasets\n‚îî‚îÄ‚îÄ experiments.db   # SQLite tracking database\n```\n\n### Resources\n\n- [Transformer Builder Documentation](https://transformer-builder.com/docs)\n- [Training Utilities Reference](https://github.com/matt-hans/transformer-builder-colab-templates)\n- [W&B Dashboard](https://wandb.ai) (if enabled)\n\n---\n\n**üí° Tip**: Save this notebook to Google Drive for future use!"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}