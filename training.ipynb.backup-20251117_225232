{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e697c2b",
   "metadata": {},
   "source": [
    "# üöÄ Transformer Training & Fine-Tuning Notebook\n",
    "\n",
    "**Professional ML Training Environment** for transformer models exported from [Transformer Builder](https://transformer-builder.com).\n",
    "\n",
    "## Quick Start Modes\n",
    "\n",
    "| Mode | Epochs | Time | Use Case |\n",
    "|------|--------|------|----------|\n",
    "| **‚ö° Fast** | 3 | ~5 min | Quick validation |\n",
    "| **‚öñÔ∏è Balanced** | 10 | ~15 min | Development |\n",
    "| **üíé Quality** | 20 | ~45 min | Production |\n",
    "\n",
    "## Features\n",
    "- ‚úÖ 5 Data Sources (HuggingFace, Drive, Upload, Local, Synthetic)\n",
    "- ‚úÖ Live Training Visualization\n",
    "- ‚úÖ Google Drive Checkpoints\n",
    "- ‚úÖ W&B + Local SQLite Tracking\n",
    "- ‚úÖ Hyperparameter Search\n",
    "- ‚úÖ Export & Comparison Tools\n",
    "\n",
    "**üìå Tip**: Run all cells in order for best results. Adjust hyperparameters in Section 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef71373",
   "metadata": {},
   "source": [
    "## üìã Table of Contents\n",
    "\n",
    "1. [Section 0: Quick Start](#section-0) ‚Üê You are here\n",
    "2. [Section 1: Setup & Drive Workspace](#section-1) (2 min)\n",
    "3. [Section 2: Data Loading](#section-2) (5 sources)\n",
    "4. [Section 3: Training Configuration](#section-3) (Hyperparameters)\n",
    "5. [Section 4: W&B Tracking Setup](#section-4) (Optional)\n",
    "6. [Section 5: Training Loop](#section-5) (Main training)\n",
    "7. [Section 6: Analysis & Visualization](#section-6) (Dashboards)\n",
    "8. [Section 7: Export & Results](#section-7) (Download checkpoints)\n",
    "9. [Section 8: Advanced Features](#section-8) (Hyperparameter search)\n",
    "\n",
    "‚è±Ô∏è **Total Time**: ~20-60 minutes depending on mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410215b4",
   "metadata": {},
   "source": [
    "## üì¶ Requirements\n",
    "\n",
    "This notebook requires:\n",
    "- Python >= 3.10\n",
    "- PyTorch (pre-installed in Colab)\n",
    "- Transformer Builder utilities (auto-downloaded)\n",
    "\n",
    "**GPU Recommended** but not required. Training will auto-detect and use GPU if available.\n",
    "\n",
    "---\n",
    "<a id=\"section-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c2f932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install training dependencies\n",
    "!pip install -q -r https://raw.githubusercontent.com/transformer-builder/colab-templates/main/requirements-training.txt\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676a4b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Download training utilities\n",
    "utils_files = [\n",
    "    'tier3_training_utilities.py',\n",
    "    'training/training_config.py',\n",
    "    'training/metrics_tracker.py',\n",
    "    'training/seed_manager.py',\n",
    "    'training/live_plotting.py',\n",
    "    'training/experiment_db.py',\n",
    "    'training/dashboard.py'\n",
    "]\n",
    "\n",
    "os.makedirs('utils/training', exist_ok=True)\n",
    "\n",
    "base_url = 'https://raw.githubusercontent.com/transformer-builder/colab-templates/main/utils/'\n",
    "for file in utils_files:\n",
    "    url = f'{base_url}{file}'\n",
    "    dest = f'utils/{file}'\n",
    "    urllib.request.urlretrieve(url, dest)\n",
    "\n",
    "print(f\"‚úÖ Downloaded {len(utils_files)} utility files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create workspace folders\n",
    "workspace_root = '/content/drive/MyDrive/TransformerTraining'\n",
    "os.makedirs(f'{workspace_root}/checkpoints', exist_ok=True)\n",
    "os.makedirs(f'{workspace_root}/configs', exist_ok=True)\n",
    "os.makedirs(f'{workspace_root}/results', exist_ok=True)\n",
    "os.makedirs(f'{workspace_root}/datasets', exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Workspace created at: {workspace_root}\")\n",
    "print(f\"   üìÅ checkpoints/ - Saved model weights\")\n",
    "print(f\"   üìÅ configs/ - Training configurations\")\n",
    "print(f\"   üìÅ results/ - Metrics, plots, dashboards\")\n",
    "print(f\"   üìÅ datasets/ - Cached datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c65122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training.experiment_db import ExperimentDB\n",
    "\n",
    "# Initialize local SQLite tracking (backup to W&B)\n",
    "db = ExperimentDB(f'{workspace_root}/experiments.db')\n",
    "\n",
    "print(\"‚úÖ Experiment database initialized\")\n",
    "print(f\"   Database: {workspace_root}/experiments.db\")\n",
    "print(f\"   Recent runs:\")\n",
    "recent_runs = db.list_runs(limit=5)\n",
    "if recent_runs:\n",
    "    print(recent_runs)\n",
    "else:\n",
    "    print(\"   (No previous runs found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc17228",
   "metadata": {},
   "source": [
    "<a id=\"section-2\"></a>\n",
    "# üìä Section 2: Data Loading\n",
    "\n",
    "Choose your data source (run ONE of the following cells):\n",
    "- **Option 1**: HuggingFace Datasets (recommended)\n",
    "- **Option 2**: Google Drive Upload\n",
    "- **Option 3**: File Upload (small datasets)\n",
    "- **Option 4**: Local Files (from previous sessions)\n",
    "- **Option 5**: Synthetic Data (testing only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3e6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# CONFIGURATION: Edit dataset name\n",
    "dataset_name = \"wikitext\"  #@param {type:\"string\"}\n",
    "config_name = \"wikitext-2-raw-v1\"  #@param {type:\"string\"}\n",
    "max_samples = 1000  #@param {type:\"integer\"}\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(dataset_name, config_name)\n",
    "train_data = dataset['train'].select(range(min(max_samples, len(dataset['train']))))\n",
    "val_data = dataset['validation'].select(range(min(100, len(dataset['validation']))))\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(train_data)} training samples, {len(val_data)} validation samples\")\n",
    "print(f\"   Example: {train_data[0]}\")\n",
    "\n",
    "data_source = \"huggingface\"\n",
    "dataset_info = {'name': dataset_name, 'config': config_name, 'train_size': len(train_data), 'val_size': len(val_data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e417890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "drive_data_path = \"/content/drive/MyDrive/TransformerTraining/datasets/my_data.txt\"  #@param {type:\"string\"}\n",
    "\n",
    "if os.path.exists(drive_data_path):\n",
    "    with open(drive_data_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    split_idx = int(0.9 * len(lines))\n",
    "    train_data = [line.strip() for line in lines[:split_idx]]\n",
    "    val_data = [line.strip() for line in lines[split_idx:]]\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(train_data)} training samples, {len(val_data)} validation samples\")\n",
    "    data_source = \"google_drive\"\n",
    "    dataset_info = {'path': drive_data_path, 'train_size': len(train_data), 'val_size': len(val_data)}\n",
    "else:\n",
    "    print(f\"‚ùå File not found: {drive_data_path}\")\n",
    "    print(\"   Please upload your data to Google Drive first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366269e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "# Upload file\n",
    "uploaded = files.upload()\n",
    "\n",
    "if uploaded:\n",
    "    filename = list(uploaded.keys())[0]\n",
    "    content = uploaded[filename].decode('utf-8')\n",
    "    lines = content.split('\\n')\n",
    "\n",
    "    split_idx = int(0.9 * len(lines))\n",
    "    train_data = [line.strip() for line in lines[:split_idx]]\n",
    "    val_data = [line.strip() for line in lines[split_idx:]]\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(train_data)} training samples, {len(val_data)} validation samples\")\n",
    "    data_source = \"file_upload\"\n",
    "    dataset_info = {'filename': filename, 'train_size': len(train_data), 'val_size': len(val_data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a4882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "cache_path = f'{workspace_root}/datasets/cached_data.pkl'\n",
    "\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    train_data = data['train']\n",
    "    val_data = data['val']\n",
    "\n",
    "    print(f\"‚úÖ Loaded cached data: {len(train_data)} train, {len(val_data)} val\")\n",
    "    data_source = \"cached\"\n",
    "    dataset_info = {'path': cache_path, 'train_size': len(train_data), 'val_size': len(val_data)}\n",
    "else:\n",
    "    print(f\"‚ùå No cached data found at {cache_path}\")\n",
    "    print(\"   Run one of the other data loading options first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7865100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Generate synthetic data for testing\n",
    "vocab_size = 50257  # GPT-2 vocab\n",
    "seq_len = 32\n",
    "n_samples = 100\n",
    "\n",
    "train_data = [torch.randint(0, vocab_size, (seq_len,)) for _ in range(n_samples)]\n",
    "val_data = [torch.randint(0, vocab_size, (seq_len,)) for _ in range(20)]\n",
    "\n",
    "print(f\"‚úÖ Generated {len(train_data)} synthetic training samples\")\n",
    "print(f\"   ‚ö†Ô∏è Warning: Synthetic data is for testing only\")\n",
    "data_source = \"synthetic\"\n",
    "dataset_info = {'vocab_size': vocab_size, 'seq_len': seq_len, 'train_size': len(train_data), 'val_size': len(val_data)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56295914",
   "metadata": {},
   "source": [
    "<a id=\"section-3\"></a>\n",
    "# ‚öôÔ∏è Section 3: Training Configuration\n",
    "\n",
    "Configure hyperparameters using Colab forms below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269a022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training.training_config import TrainingConfig\n",
    "\n",
    "# HYPERPARAMETERS (edit via forms)\n",
    "learning_rate = 5e-5  #@param {type:\"number\"}\n",
    "batch_size = 4  #@param {type:\"integer\"}\n",
    "epochs = 10  #@param {type:\"integer\"}\n",
    "warmup_ratio = 0.1  #@param {type:\"number\"}\n",
    "weight_decay = 0.01  #@param {type:\"number\"}\n",
    "gradient_clip_norm = 1.0  #@param {type:\"number\"}\n",
    "\n",
    "# TRAINING FEATURES\n",
    "use_amp = True  #@param {type:\"boolean\"}\n",
    "gradient_accumulation_steps = 1  #@param {type:\"integer\"}\n",
    "deterministic = False  #@param {type:\"boolean\"}\n",
    "\n",
    "# EXPERIMENT\n",
    "run_name = \"training-run\"  #@param {type:\"string\"}\n",
    "random_seed = 42  #@param {type:\"integer\"}\n",
    "\n",
    "# Create config\n",
    "config = TrainingConfig(\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    weight_decay=weight_decay,\n",
    "    max_grad_norm=gradient_clip_norm,\n",
    "    use_amp=use_amp,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    deterministic=deterministic,\n",
    "    random_seed=random_seed,\n",
    "    run_name=run_name\n",
    ")\n",
    "\n",
    "# Validate\n",
    "config.validate()\n",
    "\n",
    "# Save to Drive\n",
    "config_path = config.save(f'{workspace_root}/configs/')\n",
    "print(f\"‚úÖ Config saved: {config_path}\")\n",
    "print(f\"\\n{config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b568df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display configuration summary\n",
    "print(\"=\" * 60)\n",
    "print(\" \" * 15 + \"TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Run Name:':<25} {config.run_name}\")\n",
    "print(f\"{'Learning Rate:':<25} {config.learning_rate}\")\n",
    "print(f\"{'Batch Size (effective):':<25} {config.batch_size * config.gradient_accumulation_steps}\")\n",
    "print(f\"{'Epochs:':<25} {config.epochs}\")\n",
    "print(f\"{'Warmup Ratio:':<25} {config.warmup_ratio}\")\n",
    "print(f\"{'Gradient Clipping:':<25} {config.max_grad_norm}\")\n",
    "print(f\"{'AMP Enabled:':<25} {config.use_amp}\")\n",
    "print(f\"{'Deterministic:':<25} {config.deterministic}\")\n",
    "print(f\"{'Random Seed:':<25} {config.random_seed}\")\n",
    "print(f\"{'Data Source:':<25} {data_source}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5e4445",
   "metadata": {},
   "source": [
    "### Training Mode Selection\n",
    "\n",
    "Based on your `epochs` setting:\n",
    "- **epochs <= 5**: ‚ö° Fast Mode (~5 min)\n",
    "- **epochs <= 15**: ‚öñÔ∏è Balanced Mode (~15 min)\n",
    "- **epochs > 15**: üíé Quality Mode (45+ min)\n",
    "\n",
    "Proceed to training in Section 5 ‚¨áÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d46ead6",
   "metadata": {},
   "source": [
    "<a id=\"section-4\"></a>\n",
    "# üî¨ Section 4: W&B Tracking Setup (Optional)\n",
    "\n",
    "Enable Weights & Biases for cloud-based experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae42ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from getpass import getpass\n",
    "\n",
    "use_wandb = True  #@param {type:\"boolean\"}\n",
    "wandb_project = \"transformer-training\"  #@param {type:\"string\"}\n",
    "wandb_entity = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "if use_wandb:\n",
    "    # Login to W&B\n",
    "    wandb_key = getpass(\"Enter W&B API key (or leave blank to skip): \")\n",
    "    if wandb_key:\n",
    "        wandb.login(key=wandb_key)\n",
    "\n",
    "        # Initialize run\n",
    "        wandb.init(\n",
    "            project=wandb_project,\n",
    "            entity=wandb_entity if wandb_entity else None,\n",
    "            name=config.run_name,\n",
    "            config=config.to_dict(),\n",
    "            tags=[data_source, f\"epochs_{epochs}\"]\n",
    "        )\n",
    "        print(f\"‚úÖ W&B initialized: {wandb.run.url}\")\n",
    "    else:\n",
    "        use_wandb = False\n",
    "        print(\"‚ö†Ô∏è W&B skipped - training will use local tracking only\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è W&B disabled - using local SQLite tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ce57e5",
   "metadata": {},
   "source": [
    "<a id=\"section-5\"></a>\n",
    "# üèãÔ∏è Section 5: Training Loop\n",
    "\n",
    "Main training loop with live visualization and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üîó Model Source Configuration { display-mode: \"form\" }\n",
    "\n",
    "# Step 1: Try to extract from URL hash using JavaScript\n",
    "from google.colab import output\n",
    "import os\n",
    "import json\n",
    "\n",
    "# JavaScript to extract gist_id and model_name from URL hash\n",
    "js_code = \"\"\"\n",
    "(function() {\n",
    "    let gist_id = '';\n",
    "    let model_name = '';\n",
    "\n",
    "    try {\n",
    "        // Try to read URL hash from parent window (Colab embedding)\n",
    "        const hash = window.parent.location.hash || window.location.hash || '';\n",
    "\n",
    "        if (hash) {\n",
    "            // Parse hash parameters (e.g., #gist_id=abc123&name=MyModel)\n",
    "            const params = new URLSearchParams(hash.substring(1));\n",
    "            gist_id = params.get('gist_id') || '';\n",
    "            model_name = params.get('name') || '';\n",
    "\n",
    "            console.log('Extracted from URL hash:', {gist_id, model_name});\n",
    "        }\n",
    "    } catch (e) {\n",
    "        console.log('Could not access URL hash:', e);\n",
    "    }\n",
    "\n",
    "    // Return as JSON string\n",
    "    return JSON.stringify({gist_id: gist_id, model_name: model_name});\n",
    "})();\n",
    "\"\"\"\n",
    "\n",
    "# Execute JavaScript and get returned values\n",
    "try:\n",
    "    url_params_json = output.eval_js(js_code)\n",
    "    url_params = json.loads(url_params_json)\n",
    "    gist_id_from_url = url_params.get('gist_id', '')\n",
    "    model_name_from_url = url_params.get('model_name', '')\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not extract from URL hash: {e}\")\n",
    "    gist_id_from_url = ''\n",
    "    model_name_from_url = ''\n",
    "\n",
    "# Step 2: Manual input forms (as fallback)\n",
    "gist_id_manual = \"\"  #@param {type:\"string\"}\n",
    "model_name_manual = \"CustomTransformer\"  #@param {type:\"string\"}\n",
    "\n",
    "# Step 3: Environment variables (lowest priority)\n",
    "gist_id_env = os.getenv('GIST_ID', '')\n",
    "model_name_env = os.getenv('MODEL_NAME', '')\n",
    "\n",
    "# Step 4: Determine final values (URL > Manual > Env)\n",
    "gist_id = gist_id_from_url or gist_id_manual or gist_id_env\n",
    "model_name = model_name_from_url or model_name_manual or model_name_env or 'CustomTransformer'\n",
    "\n",
    "# Display source\n",
    "print(\"=\"*60)\n",
    "if gist_id:\n",
    "    source = \"URL hash\" if gist_id_from_url else (\"Manual input\" if gist_id_manual else \"Environment variable\")\n",
    "    print(f\"‚úÖ Model Source: {source}\")\n",
    "    print(f\"   Gist ID: {gist_id}\")\n",
    "    print(f\"   Model Name: {model_name}\")\n",
    "    print(f\"\\n   Loading custom model from Transformer Builder...\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No Gist ID provided\")\n",
    "    print(\"   Options to provide Gist ID:\")\n",
    "    print(\"   1. Open via Transformer Builder link (auto-detects from URL)\")\n",
    "    print(\"   2. Enter Gist ID in the form above\")\n",
    "    print(\"   3. Set GIST_ID environment variable\")\n",
    "    print(\"\\n   Proceeding with example model for demonstration...\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üì¶ Load Model from Gist { display-mode: \"form\" }",
    "",
    "import urllib.request",
    "import json",
    "import sys",
    "import tempfile",
    "import shutil",
    "",
    "model = None  # Will be initialized later",
    "model_config = {}",
    "",
    "if gist_id:",
    "    # Load custom model from Gist",
    "    try:",
    "        # Fetch model.py from Gist (try multiple URL patterns)",
    "        gist_urls = [",
    "            f\"https://gist.githubusercontent.com/{gist_id}/raw/model.py\",",
    "            f\"https://gist.github.com/{gist_id}/raw/model.py\"",
    "        ]",
    "",
    "        config_urls = [",
    "            f\"https://gist.githubusercontent.com/{gist_id}/raw/config.json\",",
    "            f\"https://gist.github.com/{gist_id}/raw/config.json\"",
    "        ]",
    "",
    "        print(f\"üì• Downloading model from Gist {gist_id}...\")",
    "",
    "        # Try to download model.py",
    "        model_downloaded = False",
    "        for url in gist_urls:",
    "            try:",
    "                urllib.request.urlretrieve(url, 'model.py')",
    "                print(f\"   ‚úÖ model.py downloaded from {url}\")",
    "                model_downloaded = True",
    "                break",
    "            except Exception as e:",
    "                continue",
    "",
    "        if not model_downloaded:",
    "            raise Exception(f\"Could not download model.py from Gist {gist_id}\")",
    "",
    "        # Try to download config.json",
    "        config_downloaded = False",
    "        for url in config_urls:",
    "            try:",
    "                urllib.request.urlretrieve(url, 'config.json')",
    "                print(f\"   ‚úÖ config.json downloaded from {url}\")",
    "                config_downloaded = True",
    "",
    "                # Load config",
    "                with open('config.json', 'r') as f:",
    "                    model_config = json.load(f)",
    "                break",
    "            except Exception as e:",
    "                continue",
    "",
    "        if not config_downloaded:",
    "            print(f\"   ‚ö†Ô∏è  config.json not found, using defaults\")",
    "            model_config = {",
    "                'vocab_size': 50257,",
    "                'd_model': 768,",
    "                'n_heads': 12,",
    "                'n_layers': 12,",
    "                'max_seq_len': 1024",
    "            }",
    "",
    "        # Display downloaded model code",
    "        print(f\"\\nüìÑ Model Code Preview:\")",
    "        print(\"=\" * 60)",
    "        with open('model.py', 'r') as f:",
    "            model_code = f.read()",
    "            # Show first 20 lines",
    "            lines = model_code.split('\\n')[:20]",
    "            for i, line in enumerate(lines, 1):",
    "                print(f\"{i:3d} | {line}\")",
    "            if len(model_code.split('\\n')) > 20:",
    "                print(f\"... ({len(model_code.split('\\n')) - 20} more lines)\")",
    "        print(\"=\" * 60)",
    "",
    "        print(f\"\\n‚úÖ Model '{model_name}' loaded successfully from Gist\")",
    "        gist_loaded = True",
    "",
    "    except Exception as e:",
    "        print(f\"\\n‚ùå Failed to load model from Gist: {e}\")",
    "        print(\"   Falling back to example model...\")",
    "        gist_id = None  # Trigger fallback",
    "        gist_loaded = False",
    "",
    "if not gist_id:",
    "    # Fallback: Load example GPT-2 style model",
    "    print(\"üì¶ Loading example model (GPT-2 architecture)...\")",
    "",
    "    import torch",
    "    import torch.nn as nn",
    "",
    "    class ExampleTransformer(nn.Module):",
    "        \"\"\"Example GPT-2 style transformer for demonstration.\"\"\"",
    "",
    "        def __init__(self, vocab_size=50257, d_model=768, n_layers=12, n_heads=12, max_seq_len=1024):",
    "            super().__init__()",
    "            self.vocab_size = vocab_size",
    "            self.d_model = d_model",
    "            self.n_layers = n_layers",
    "            self.n_heads = n_heads",
    "            self.max_seq_len = max_seq_len",
    "",
    "            self.embedding = nn.Embedding(vocab_size, d_model)",
    "            self.position_embedding = nn.Embedding(max_seq_len, d_model)",
    "",
    "            # Simple transformer layers",
    "            self.layers = nn.ModuleList([",
    "                nn.TransformerEncoderLayer(",
    "                    d_model,",
    "                    n_heads,",
    "                    dim_feedforward=d_model*4,",
    "                    batch_first=True,",
    "                    dropout=0.1",
    "                )",
    "                for _ in range(n_layers)",
    "            ])",
    "",
    "            self.ln_f = nn.LayerNorm(d_model)",
    "            self.lm_head = nn.Linear(d_model, vocab_size, bias=False)",
    "",
    "        def forward(self, input_ids):",
    "            batch_size, seq_len = input_ids.shape",
    "",
    "            # Embeddings",
    "            token_emb = self.embedding(input_ids)",
    "            pos_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)",
    "            pos_emb = self.position_embedding(pos_ids)",
    "",
    "            x = token_emb + pos_emb",
    "",
    "            # Transformer layers",
    "            for layer in self.layers:",
    "                x = layer(x)",
    "",
    "            x = self.ln_f(x)",
    "            logits = self.lm_head(x)",
    "",
    "            return logits",
    "",
    "    # Create example model",
    "    model_name = \"ExampleTransformer\"",
    "    model_config = {",
    "        'vocab_size': 50257,",
    "        'd_model': 768,",
    "        'n_layers': 12,",
    "        'n_heads': 12,",
    "        'max_seq_len': 1024",
    "    }",
    "",
    "    print(f\"‚úÖ Example model definition loaded\")",
    "    gist_loaded = False",
    "",
    "print(f\"\\nüìä Model: {model_name}\")",
    "print(f\"   Config: {json.dumps(model_config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üöÄ Initialize Model { display-mode: \"form\" }",
    "",
    "import torch",
    "from types import SimpleNamespace",
    "",
    "# Detect device",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
    "print(f\"üñ•Ô∏è  Device: {device}\")",
    "",
    "if torch.cuda.is_available():",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")",
    "",
    "# Create model instance",
    "if gist_loaded:",
    "    # Custom model from Transformer Builder",
    "    # Import the model from downloaded file",
    "    try:",
    "        sys.path.insert(0, '.')",
    "",
    "        # Import all classes from model.py",
    "        import importlib.util",
    "        spec = importlib.util.spec_from_file_location(\"custom_model\", \"model.py\")",
    "        custom_model_module = importlib.util.module_from_spec(spec)",
    "        spec.loader.exec_module(custom_model_module)",
    "",
    "        # Try common constructor patterns",
    "        try:",
    "            # Pattern 1: Direct class instantiation",
    "            model_class = getattr(custom_model_module, model_name)",
    "            model = model_class(**model_config)",
    "        except (AttributeError, TypeError):",
    "            try:",
    "                # Pattern 2: Main model class",
    "                model_class = getattr(custom_model_module, f'{model_name}Model')",
    "                model = model_class(**model_config)",
    "            except (AttributeError, TypeError):",
    "                try:",
    "                    # Pattern 3: Config-based instantiation",
    "                    config_obj = SimpleNamespace(**model_config)",
    "                    model_class = getattr(custom_model_module, model_name)",
    "                    model = model_class(config_obj)",
    "                except Exception as e:",
    "                    raise Exception(f\"Could not instantiate model: {e}\")",
    "",
    "        print(f\"‚úÖ Custom model instantiated: {model.__class__.__name__}\")",
    "",
    "    except Exception as e:",
    "        print(f\"‚ùå Failed to instantiate custom model: {e}\")",
    "        print(\"   Falling back to example model...\")",
    "        model = ExampleTransformer(**model_config)",
    "else:",
    "    # Example model",
    "    model = ExampleTransformer(**model_config)",
    "",
    "# Move to device",
    "model = model.to(device)",
    "",
    "# Model summary",
    "total_params = sum(p.numel() for p in model.parameters())",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)",
    "",
    "print(f\"\\n‚úÖ Model initialized on {device}\")",
    "print(f\"   Total parameters: {total_params:,}\")",
    "print(f\"   Trainable parameters: {trainable_params:,}\")",
    "print(f\"   Model size: {total_params * 4 / 1e6:.1f} MB (fp32)\")",
    "",
    "# Create config object for training utilities",
    "config_obj = SimpleNamespace(**model_config)",
    "if not hasattr(config_obj, 'vocab_size'):",
    "    config_obj.vocab_size = model_config.get('vocab_size', 50257)",
    "if not hasattr(config_obj, 'max_seq_len'):",
    "    config_obj.max_seq_len = model_config.get('max_seq_len', 1024)",
    "",
    "print(f\"\\nüéØ Ready for training!\")",
    "print(f\"\\n‚ÑπÔ∏è  Note: Update Section 3 training config if needed before starting training loop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c98ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training.metrics_tracker import MetricsTracker\n",
    "from utils.training.live_plotting import LivePlotter\n",
    "from utils.training.seed_manager import set_random_seed\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set random seed\n",
    "set_random_seed(config.random_seed, config.deterministic)\n",
    "\n",
    "# Initialize metrics tracker\n",
    "tracker = MetricsTracker(use_wandb=use_wandb)\n",
    "\n",
    "# Initialize live plotter\n",
    "plotter = LivePlotter(update_interval=1)\n",
    "\n",
    "# Create DataLoader (simplified - adapt to your data format)\n",
    "if data_source == \"synthetic\":\n",
    "    train_dataset = TensorDataset(torch.stack(train_data))\n",
    "    val_dataset = TensorDataset(torch.stack(val_data))\n",
    "else:\n",
    "    # For HuggingFace datasets or text data, you'll need proper tokenization\n",
    "    print(\"‚ö†Ô∏è Using synthetic data - implement proper tokenization for real datasets\")\n",
    "    train_dataset = TensorDataset(torch.stack([torch.randint(0, 50257, (32,)) for _ in range(100)]))\n",
    "    val_dataset = TensorDataset(torch.stack([torch.randint(0, 50257, (32,)) for _ in range(20)]))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "# Learning rate scheduler (warmup + cosine decay)\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=config.learning_rate,\n",
    "    epochs=config.epochs,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=config.warmup_ratio\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training initialized\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import time\n",
    "\n",
    "# Initialize gradient scaler for AMP\n",
    "scaler = GradScaler(enabled=config.use_amp)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(config.epochs):\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_idx, (input_ids,) in enumerate(train_loader):\n",
    "        input_ids = input_ids.to(device)\n",
    "\n",
    "        # Forward pass with AMP\n",
    "        with autocast(enabled=config.use_amp):\n",
    "            # Shift for language modeling: predict next token\n",
    "            logits = model(input_ids[:, :-1])\n",
    "            targets = input_ids[:, 1:]\n",
    "\n",
    "            # Compute loss\n",
    "            loss = F.cross_entropy(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                targets.reshape(-1)\n",
    "            )\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        if config.max_grad_norm is not None:\n",
    "            scaler.unscale_(optimizer)\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "        else:\n",
    "            grad_norm = 0.0\n",
    "\n",
    "        # Optimizer step\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Log batch metrics\n",
    "        global_step = epoch * len(train_loader) + batch_idx\n",
    "        tracker.log_scalar('train/batch_loss', loss.item(), step=global_step)\n",
    "        tracker.log_scalar('train/learning_rate', scheduler.get_last_lr()[0], step=global_step)\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{config.epochs} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, in val_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "\n",
    "            with autocast(enabled=config.use_amp):\n",
    "                logits = model(input_ids[:, :-1])\n",
    "                targets = input_ids[:, 1:]\n",
    "                loss = F.cross_entropy(\n",
    "                    logits.reshape(-1, logits.size(-1)),\n",
    "                    targets.reshape(-1)\n",
    "                )\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Compute epoch metrics\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "\n",
    "    # Log epoch metrics\n",
    "    tracker.log_epoch(\n",
    "        epoch=epoch,\n",
    "        train_metrics={'loss': avg_train_loss},\n",
    "        val_metrics={'loss': avg_val_loss, 'perplexity': torch.exp(torch.tensor(avg_val_loss)).item()},\n",
    "        learning_rate=scheduler.get_last_lr()[0],\n",
    "        gradient_norm=grad_norm if isinstance(grad_norm, float) else grad_norm.item(),\n",
    "        epoch_duration=epoch_time\n",
    "    )\n",
    "\n",
    "    # Update live plot\n",
    "    plotter.update(tracker.get_summary())\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % 5 == 0 or epoch == config.epochs - 1:\n",
    "        checkpoint_path = f\"{workspace_root}/checkpoints/{config.run_name}_epoch{epoch+1}.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'config': config.to_dict()\n",
    "        }, checkpoint_path)\n",
    "        print(f\"üíæ Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{config.epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Time: {epoch_time:.1f}s\")\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")\n",
    "\n",
    "# Save experiment to database\n",
    "db.save_run(\n",
    "    run_name=config.run_name,\n",
    "    config=config.to_dict(),\n",
    "    metrics=tracker.get_summary().to_dict('records')[-1],\n",
    "    data_source=data_source\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd41698",
   "metadata": {},
   "source": [
    "<a id=\"section-6\"></a>\n",
    "# üìà Section 6: Analysis & Visualization\n",
    "\n",
    "Analyze training results with comprehensive dashboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a6e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training.dashboard import TrainingDashboard\n",
    "\n",
    "# Create comprehensive 6-panel dashboard\n",
    "metrics_df = tracker.get_summary()\n",
    "dashboard = TrainingDashboard(figsize=(18, 12))\n",
    "\n",
    "fig = dashboard.plot(\n",
    "    metrics_df,\n",
    "    config=config,\n",
    "    title=f\"Training Dashboard: {config.run_name}\"\n",
    ")\n",
    "\n",
    "# Save to Drive\n",
    "dashboard_path = f'{workspace_root}/results/{config.run_name}_dashboard.png'\n",
    "dashboard.save(dashboard_path, dpi=150)\n",
    "print(f\"‚úÖ Dashboard saved to Drive: {dashboard_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d96e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best epoch based on validation loss\n",
    "best_epoch_idx = metrics_df['val/loss'].idxmin()\n",
    "best_epoch = metrics_df.loc[best_epoch_idx]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\" \" * 20 + \"BEST EPOCH ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Best Epoch:':<25} {int(best_epoch['epoch']) + 1}\")\n",
    "print(f\"{'Validation Loss:':<25} {best_epoch['val/loss']:.4f}\")\n",
    "print(f\"{'Validation Perplexity:':<25} {best_epoch['val/perplexity']:.2f}\")\n",
    "print(f\"{'Training Loss:':<25} {best_epoch['train/loss']:.4f}\")\n",
    "print(f\"{'Learning Rate:':<25} {best_epoch['train/learning_rate']:.2e}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load best checkpoint\n",
    "best_checkpoint_path = f\"{workspace_root}/checkpoints/{config.run_name}_epoch{int(best_epoch['epoch']) + 1}.pt\"\n",
    "if os.path.exists(best_checkpoint_path):\n",
    "    print(f\"\\nüíæ Best checkpoint: {best_checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Best checkpoint not found (may not have been saved)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metrics table\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.4f}')\n",
    "\n",
    "display_cols = ['epoch', 'train/loss', 'val/loss', 'val/perplexity', 'train/learning_rate']\n",
    "available_cols = [col for col in display_cols if col in metrics_df.columns]\n",
    "\n",
    "print(\"\\nTraining Metrics Summary:\")\n",
    "print(metrics_df[available_cols].to_string(index=False))\n",
    "\n",
    "# Export to CSV\n",
    "csv_path = f'{workspace_root}/results/{config.run_name}_metrics.csv'\n",
    "metrics_df.to_csv(csv_path, index=False)\n",
    "print(f\"\\n‚úÖ Metrics exported to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe5404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"=\" * 60)\n",
    "    print(\" \" * 20 + \"GPU METRICS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    gpu_cols = [col for col in metrics_df.columns if col.startswith('gpu/')]\n",
    "    if gpu_cols:\n",
    "        print(metrics_df[['epoch'] + gpu_cols].tail(5).to_string(index=False))\n",
    "\n",
    "        # Plot GPU utilization\n",
    "        import matplotlib.pyplot as plt\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "        if 'gpu/memory_allocated_mb' in metrics_df.columns:\n",
    "            ax1.plot(metrics_df['epoch'], metrics_df['gpu/memory_allocated_mb'])\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('GPU Memory (MB)')\n",
    "            ax1.set_title('GPU Memory Usage')\n",
    "            ax1.grid(True)\n",
    "\n",
    "        if 'gpu/utilization_percent' in metrics_df.columns:\n",
    "            ax2.plot(metrics_df['epoch'], metrics_df['gpu/utilization_percent'])\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('GPU Utilization (%)')\n",
    "            ax2.set_title('GPU Utilization')\n",
    "            ax2.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{workspace_root}/results/{config.run_name}_gpu_metrics.png', dpi=100)\n",
    "        plt.show()\n",
    "        print(f\"\\n‚úÖ GPU metrics saved\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No GPU metrics collected during training\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Training was performed on CPU (no GPU metrics available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6fdf23",
   "metadata": {},
   "source": [
    "<a id=\"section-7\"></a>\n",
    "# üíæ Section 7: Export & Results\n",
    "\n",
    "Download checkpoints, configs, and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a039396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\" \" * 20 + \"EXPORT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìÅ Workspace: {workspace_root}\")\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"   - Dashboard: {config.run_name}_dashboard.png\")\n",
    "print(f\"   - Metrics CSV: {config.run_name}_metrics.csv\")\n",
    "print(f\"   - Config: {os.path.basename(config_path)}\")\n",
    "print(f\"\\nüíæ Checkpoints:\")\n",
    "\n",
    "checkpoint_dir = f\"{workspace_root}/checkpoints\"\n",
    "checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(config.run_name)]\n",
    "for ckpt in sorted(checkpoints):\n",
    "    ckpt_path = os.path.join(checkpoint_dir, ckpt)\n",
    "    size_mb = os.path.getsize(ckpt_path) / (1024 * 1024)\n",
    "    print(f\"   - {ckpt} ({size_mb:.1f} MB)\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1656431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results to local machine\n",
    "download_results = False  #@param {type:\"boolean\"}\n",
    "\n",
    "if download_results:\n",
    "    print(\"Downloading files...\")\n",
    "\n",
    "    # Download dashboard\n",
    "    dashboard_file = f'{workspace_root}/results/{config.run_name}_dashboard.png'\n",
    "    if os.path.exists(dashboard_file):\n",
    "        files.download(dashboard_file)\n",
    "\n",
    "    # Download metrics CSV\n",
    "    metrics_file = f'{workspace_root}/results/{config.run_name}_metrics.csv'\n",
    "    if os.path.exists(metrics_file):\n",
    "        files.download(metrics_file)\n",
    "\n",
    "    # Download config\n",
    "    if os.path.exists(config_path):\n",
    "        files.download(config_path)\n",
    "\n",
    "    # Download best checkpoint\n",
    "    if os.path.exists(best_checkpoint_path):\n",
    "        files.download(best_checkpoint_path)\n",
    "        print(f\"‚úÖ Downloaded {os.path.basename(best_checkpoint_path)}\")\n",
    "\n",
    "    print(\"‚úÖ Downloads complete\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Downloads skipped. Files are saved in Google Drive.\")\n",
    "    print(f\"   Access them at: {workspace_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f00c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with previous runs\n",
    "all_runs = db.list_runs(limit=10)\n",
    "\n",
    "if len(all_runs) > 1:\n",
    "    print(\"=\" * 60)\n",
    "    print(\" \" * 15 + \"COMPARISON WITH PREVIOUS RUNS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    comparison_data = []\n",
    "    for run in all_runs:\n",
    "        comparison_data.append({\n",
    "            'run_name': run.get('run_name', 'unknown'),\n",
    "            'final_val_loss': run.get('metrics', {}).get('val/loss', float('nan')),\n",
    "            'final_perplexity': run.get('metrics', {}).get('val/perplexity', float('nan')),\n",
    "            'data_source': run.get('data_source', 'unknown'),\n",
    "            'timestamp': run.get('timestamp', 'unknown')\n",
    "        })\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No previous runs to compare (this is your first run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a901c0",
   "metadata": {},
   "source": [
    "<a id=\"section-8\"></a>\n",
    "# üî¨ Section 8: Advanced Features\n",
    "\n",
    "Hyperparameter search, multi-run experiments, and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f3d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tier3_training_utilities import test_hyperparameter_search\n",
    "\n",
    "# Hyperparameter search configuration\n",
    "run_hp_search = False  #@param {type:\"boolean\"}\n",
    "n_trials = 10  #@param {type:\"integer\"}\n",
    "search_timeout = 3600  #@param {type:\"integer\"}\n",
    "\n",
    "if run_hp_search:\n",
    "    print(\"üîç Starting hyperparameter search...\")\n",
    "    print(f\"   Trials: {n_trials}\")\n",
    "    print(f\"   Timeout: {search_timeout}s ({search_timeout/60:.1f} min)\")\n",
    "    print(\"\\n‚ö†Ô∏è This may take a while. Progress will be shown below.\")\n",
    "\n",
    "    # Define search space\n",
    "    search_space = {\n",
    "        'learning_rate': (1e-5, 1e-3),\n",
    "        'batch_size': [4, 8, 16],\n",
    "        'warmup_ratio': (0.0, 0.2),\n",
    "        'weight_decay': (0.0, 0.1)\n",
    "    }\n",
    "\n",
    "    print(f\"\\nSearch space: {search_space}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Hyperparameter search disabled\")\n",
    "    print(\"   Set 'run_hp_search = True' to enable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19ee7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_hp_search:\n",
    "    # Run search\n",
    "    hp_results = test_hyperparameter_search(\n",
    "        model=model,\n",
    "        config=config,\n",
    "        train_data=train_data,\n",
    "        val_data=val_data,\n",
    "        n_trials=n_trials,\n",
    "        timeout=search_timeout,\n",
    "        use_wandb=use_wandb\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\" \" * 15 + \"HYPERPARAMETER SEARCH RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nBest parameters:\")\n",
    "    for param, value in hp_results['best_params'].items():\n",
    "        print(f\"   {param}: {value}\")\n",
    "\n",
    "    print(f\"\\nBest validation loss: {hp_results['best_value']:.4f}\")\n",
    "    print(f\"\\nAll trials:\")\n",
    "    print(hp_results['trials_df'].to_string(index=False))\n",
    "\n",
    "    # Save results\n",
    "    hp_results['trials_df'].to_csv(\n",
    "        f'{workspace_root}/results/{config.run_name}_hp_search.csv',\n",
    "        index=False\n",
    "    )\n",
    "    print(f\"\\n‚úÖ Results saved to: {config.run_name}_hp_search.csv\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Hyperparameter search skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63affe7",
   "metadata": {},
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Review Results**: Check the dashboard in Section 6\n",
    "2. **Download Files**: Use Section 7 to download checkpoints\n",
    "3. **Compare Runs**: See Section 7 for comparison with previous experiments\n",
    "4. **Optimize**: Try hyperparameter search in Section 8\n",
    "\n",
    "### Workspace Structure\n",
    "\n",
    "All files are saved in Google Drive:\n",
    "```\n",
    "/content/drive/MyDrive/TransformerTraining/\n",
    "‚îú‚îÄ‚îÄ checkpoints/     # Model weights (.pt files)\n",
    "‚îú‚îÄ‚îÄ configs/         # Training configs (.json files)\n",
    "‚îú‚îÄ‚îÄ results/         # Dashboards, metrics, plots\n",
    "‚îú‚îÄ‚îÄ datasets/        # Cached datasets\n",
    "‚îî‚îÄ‚îÄ experiments.db   # SQLite tracking database\n",
    "```\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Transformer Builder Documentation](https://transformer-builder.com/docs)\n",
    "- [Training Utilities Reference](https://github.com/transformer-builder/colab-templates)\n",
    "- [W&B Dashboard](https://wandb.ai) (if enabled)\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Tip**: Save this notebook to Google Drive for future use!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}