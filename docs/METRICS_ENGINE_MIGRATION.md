# MetricsEngine Migration Guide

**Version**: 3.7.0
**Phase**: P1-2 (Metrics Tracking with Drift Detection)
**Author**: MLOps Agent 6

## Overview

MetricsEngine is a comprehensive refactoring of the original MetricsTracker with enhanced capabilities for drift detection, confidence tracking, and performance alerts. This guide helps you migrate from MetricsTracker to MetricsEngine.

## What's New

### Core Enhancements

1. **Drift Detection**: Automatic dataset distribution drift detection using JS divergence
2. **Confidence Tracking**: Top-1, top-5 confidence and entropy logging for model calibration
3. **Performance Alerts**: Configurable thresholds for loss spikes, accuracy drops, gradient explosions
4. **ExperimentDB Integration**: Native support for local SQLite experiment tracking
5. **Type Safety**: Full mypy compliance with strict type checking

### Preserved Features

All existing MetricsTracker features are preserved:
- W&B integration with gradient accumulation awareness
- Epoch and step-level metrics logging
- GPU metrics tracking (memory, utilization)
- Perplexity computation
- Best epoch tracking

## Migration Steps

### 1. Basic Migration (Drop-in Replacement)

**Before (MetricsTracker):**
```python
from utils.training.metrics_tracker import MetricsTracker

tracker = MetricsTracker(use_wandb=True, gradient_accumulation_steps=4)

tracker.log_epoch(
    epoch=epoch,
    train_metrics={'loss': train_loss, 'accuracy': train_acc},
    val_metrics={'loss': val_loss, 'accuracy': val_acc},
    learning_rate=lr,
    gradient_norm=grad_norm,
    epoch_duration=epoch_time
)

df = tracker.get_summary()
best_epoch = tracker.get_best_epoch('val/loss', 'min')
```

**After (MetricsEngine):**
```python
from utils.training.engine.metrics import MetricsEngine

engine = MetricsEngine(use_wandb=True, gradient_accumulation_steps=4)

engine.log_epoch(
    epoch=epoch,
    train_metrics={'loss': train_loss, 'accuracy': train_acc},
    val_metrics={'loss': val_loss, 'accuracy': val_acc},
    learning_rate=lr,
    gradient_norm=grad_norm,
    epoch_duration=epoch_time
)

df = engine.get_summary()
best_epoch = engine.get_best_epoch('val/loss', 'min')
```

**Changes**: Only import path changed. All method signatures are backward compatible.

### 2. Enable Drift Detection

Add drift detection to your training loop:

```python
from utils.training.engine.metrics import MetricsEngine
from utils.training.drift_metrics import compute_dataset_profile
from utils.training.task_spec import TaskSpec

# Initialize engine with drift thresholds
engine = MetricsEngine(
    use_wandb=True,
    gradient_accumulation_steps=4,
    drift_threshold_warning=0.1,   # JS divergence > 0.1 triggers warning
    drift_threshold_critical=0.2   # JS divergence > 0.2 triggers critical alert
)

# Profile reference dataset (one-time at training start)
task_spec = TaskSpec(
    name='lm_task',
    task_type='lm',
    model_family='decoder_only',
    input_fields=['input_ids'],
    target_field='labels',
    loss_type='cross_entropy',
    metrics=['loss', 'accuracy'],
    modality='text'
)
ref_profile = compute_dataset_profile(train_dataset, task_spec, sample_size=1000)

# In training loop, profile current batch and check drift
for epoch in range(n_epochs):
    # ... training code ...

    # Profile current epoch's data
    curr_profile = compute_dataset_profile(current_batch_data, task_spec, sample_size=500)

    # Log with drift detection
    drift_metrics = engine.log_epoch(
        epoch=epoch,
        train_metrics=train_metrics,
        val_metrics=val_metrics,
        learning_rate=lr,
        gradient_norm=grad_norm,
        epoch_duration=epoch_time,
        reference_profile=ref_profile,
        current_profile=curr_profile
    )

    if drift_metrics and drift_metrics.status != 'healthy':
        print(f"‚ö†Ô∏è Drift detected: {drift_metrics.status}")
        print(f"   JS divergence: {drift_metrics.js_divergence:.3f}")
        print(f"   Affected features: {drift_metrics.affected_features}")
```

### 3. Enable Performance Alerts

Configure alerts for performance degradation:

```python
from utils.training.engine.metrics import MetricsEngine, AlertConfig

def slack_alert(alert_type: str, message: str, metrics: dict):
    """Custom alert callback for Slack notifications."""
    # Send to Slack webhook
    import requests
    requests.post(SLACK_WEBHOOK_URL, json={'text': message})

# Configure alert thresholds
alert_config = AlertConfig(
    val_loss_spike_threshold=0.2,      # 20% loss increase triggers alert
    accuracy_drop_threshold=0.05,      # 5% accuracy drop triggers alert
    gradient_explosion_threshold=10.0   # Gradient norm > 10.0 triggers alert
)

engine = MetricsEngine(
    use_wandb=True,
    alert_config=alert_config,
    alert_callbacks=[slack_alert]  # Custom callback in addition to console logging
)

# Alerts are automatically checked after each log_epoch call
engine.log_epoch(...)

# Check for alerts programmatically
if engine.has_alerts():
    alerts = engine.get_alerts()
    for alert in alerts:
        print(f"üö® {alert['type']}: {alert['message']}")
```

### 4. Enable Confidence Tracking

Track prediction confidence during validation:

```python
from utils.training.engine.metrics import MetricsEngine

engine = MetricsEngine(use_wandb=True)

# During validation loop
for batch_idx, batch in enumerate(val_loader):
    logits = model(batch['input_ids'])
    labels = batch['labels']

    # Log confidence metrics
    confidence = engine.log_confidence(
        logits=logits,
        labels=labels,
        step=batch_idx,
        ignore_index=-100  # Exclude padding tokens
    )

    # Analyze confidence
    print(f"Top-1 confidence: {confidence.top1_confidence:.3f}")
    print(f"Top-5 confidence: {confidence.top5_confidence:.3f}")
    print(f"Entropy: {confidence.entropy:.3f}")
```

### 5. Integrate with ExperimentDB

Use MetricsEngine with local SQLite tracking:

```python
from utils.training.engine.metrics import MetricsEngine
from utils.training.experiment_db import ExperimentDB

# Initialize ExperimentDB
db = ExperimentDB('experiments.db')
run_id = db.log_run('experiment-v1', config={'lr': 5e-5, 'bs': 8})

# Initialize MetricsEngine with DB integration
engine = MetricsEngine(
    use_wandb=True,
    gradient_accumulation_steps=4,
    experiment_db=db,
    run_id=run_id
)

# Metrics are automatically logged to both W&B and SQLite
engine.log_epoch(...)

# Query metrics from SQLite
metrics_df = db.get_metrics(run_id, 'val/loss')
print(metrics_df[['epoch', 'value']])

# Compare multiple runs
comparison = db.compare_runs([run_id, previous_run_id])
print(comparison[['run_name', 'final_val_loss', 'best_epoch']])
```

## API Compatibility Matrix

| Feature | MetricsTracker | MetricsEngine | Notes |
|---------|---------------|---------------|-------|
| `log_epoch()` | ‚úÖ | ‚úÖ | Backward compatible |
| `log_scalar()` | ‚úÖ | ‚úÖ | Backward compatible |
| `get_summary()` | ‚úÖ | ‚úÖ | Backward compatible |
| `get_step_metrics()` | ‚úÖ | ‚úÖ | Backward compatible |
| `get_best_epoch()` | ‚úÖ | ‚úÖ | Backward compatible |
| `compute_perplexity()` | ‚úÖ | ‚úÖ (private) | Now `_compute_perplexity()` |
| `compute_accuracy()` | ‚úÖ | ‚ùå | Removed (use loss strategies) |
| **Drift detection** | ‚ùå | ‚úÖ | New in MetricsEngine |
| **Confidence tracking** | ‚ùå | ‚úÖ | New in MetricsEngine |
| **Performance alerts** | ‚ùå | ‚úÖ | New in MetricsEngine |
| **ExperimentDB integration** | ‚ùå | ‚úÖ | New in MetricsEngine |

## Breaking Changes

### 1. `compute_accuracy()` Removed

**Reason**: Accuracy computation is now handled by task-aware loss strategies.

**Migration**:
```python
# Before (MetricsTracker)
accuracy = tracker.compute_accuracy(logits, labels, ignore_index=-100)

# After (MetricsEngine)
from utils.training.engine.loss import get_loss_strategy

loss_strategy = get_loss_strategy('lm', config, task_spec)
loss, metrics = loss_strategy.compute_loss(model_output, batch)
accuracy = metrics.get('accuracy', 0.0)
```

### 2. `compute_perplexity()` is Private

**Reason**: Perplexity is now computed automatically in `log_epoch()`.

**Migration**:
```python
# Before (MetricsTracker)
ppl = tracker.compute_perplexity(loss)

# After (MetricsEngine)
# Perplexity is automatically logged as 'train/perplexity' and 'val/perplexity'
engine.log_epoch(...)
df = engine.get_summary()
ppl = df.loc[epoch, 'val/perplexity']
```

## Performance Considerations

### Overhead

MetricsEngine adds minimal overhead:
- **Baseline metrics logging**: <1% overhead (same as MetricsTracker)
- **With drift detection**: <2% overhead (one-time JS divergence calculation per epoch)
- **With confidence tracking**: <1% overhead (computed during validation only)

### Memory Usage

- **Metrics history**: ~1KB per epoch (same as MetricsTracker)
- **Drift history**: ~2KB per drift check
- **Step metrics**: ~100 bytes per logged step

### Recommendations

1. **Drift detection**: Sample 500-1000 examples per profile (balances accuracy vs speed)
2. **Confidence tracking**: Log every N batches during validation (e.g., every 10 batches)
3. **Alert callbacks**: Keep callbacks lightweight to avoid training slowdown

## Testing Your Migration

Run this checklist to verify successful migration:

```python
# 1. Basic metrics logging
engine = MetricsEngine(use_wandb=False)
engine.log_epoch(
    epoch=0,
    train_metrics={'loss': 0.5, 'accuracy': 0.8},
    val_metrics={'loss': 0.4, 'accuracy': 0.85},
    learning_rate=1e-4,
    gradient_norm=0.5,
    epoch_duration=100
)
df = engine.get_summary()
assert len(df) == 1
print("‚úÖ Basic metrics logging works")

# 2. Drift detection
from utils.training.drift_metrics import compute_dataset_profile
ref_profile = compute_dataset_profile(train_dataset, task_spec)
drift = engine.check_drift(ref_profile, ref_profile)
assert drift.status == 'healthy'
print("‚úÖ Drift detection works")

# 3. Confidence tracking
import torch
logits = torch.randn(4, 10, 100)
labels = torch.randint(0, 100, (4, 10))
confidence = engine.log_confidence(logits, labels, step=0)
assert 0.0 <= confidence.top1_confidence <= 1.0
print("‚úÖ Confidence tracking works")

# 4. Performance alerts
engine2 = MetricsEngine(
    use_wandb=False,
    alert_config=AlertConfig(val_loss_spike_threshold=0.1)
)
engine2.log_epoch(epoch=0, train_metrics={'loss': 0.5, 'accuracy': 0.8},
                  val_metrics={'loss': 0.4, 'accuracy': 0.85},
                  learning_rate=1e-4, gradient_norm=0.5, epoch_duration=100)
engine2.log_epoch(epoch=1, train_metrics={'loss': 0.5, 'accuracy': 0.8},
                  val_metrics={'loss': 0.5, 'accuracy': 0.85},  # 25% spike
                  learning_rate=1e-4, gradient_norm=0.5, epoch_duration=100)
assert engine2.has_alerts()
print("‚úÖ Performance alerts work")
```

## Troubleshooting

### Issue: "No drift detected but I expected it"

**Solution**: Check drift thresholds and sample sizes
```python
# Increase sensitivity
engine = MetricsEngine(
    drift_threshold_warning=0.05,   # More sensitive (default: 0.1)
    drift_threshold_critical=0.1    # More sensitive (default: 0.2)
)

# Increase sample size for more accurate drift estimation
ref_profile = compute_dataset_profile(dataset, task_spec, sample_size=2000)
```

### Issue: "Too many alerts being triggered"

**Solution**: Relax alert thresholds
```python
alert_config = AlertConfig(
    val_loss_spike_threshold=0.3,    # Less sensitive (default: 0.2)
    accuracy_drop_threshold=0.1       # Less sensitive (default: 0.05)
)
```

### Issue: "W&B histogram logging fails"

**Solution**: W&B histogram requires list, not numpy array
```python
# This is handled automatically in MetricsEngine
# If you encounter issues, ensure wandb is installed: pip install wandb
```

## Support

For issues or questions:
1. Check test examples: `tests/training/engine/test_metrics.py`
2. Review docstrings: `utils/training/engine/metrics.py`
3. Consult CLAUDE.md for usage patterns

## Changelog

### v3.7.0 (2025-11-20) - Phase P1-2
- ‚ú® Added drift detection with JS divergence
- ‚ú® Added confidence tracking (top-1, top-5, entropy)
- ‚ú® Added performance alerts with configurable thresholds
- ‚ú® Added ExperimentDB integration
- ‚úÖ Full mypy compliance
- ‚úÖ 24 comprehensive unit tests (100% pass rate)
- üîÑ Preserved all MetricsTracker functionality
